{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Dataset\n",
    "import torch.utils.data as utils\n",
    "\n",
    "#Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For paths\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#imread and resize\n",
    "from skimage import io, transform\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Timestamp\n",
    "import datetime\n",
    "\n",
    "#PyTorch Models\n",
    "path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"models\")\n",
    "sys.path.append(path)\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"kinect_leap_dataset\", \"acquisitions\")\n",
    "p_id = [\"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"P9\", \"P10\", \"P11\", \"P12\", \"P13\", \"P14\"]\n",
    "g_id = [\"G1\", \"G2\", \"G3\", \"G4\", \"G5\", \"G6\", \"G7\", \"G8\", \"G9\", \"G10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(os.path.join(root_path, \"P1\", \"G1\"))\n",
    "files = glob.glob(os.path.join(root_path, \"P1\", \"G1\", \"*rgb.png\"))\n",
    "print(files)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for p in p_id:\n",
    "    for g in g_id:\n",
    "        path = os.path.join(root_path, p, g)\n",
    "        image_names = glob.glob(os.path.join(path, \"*rgb.png\"))\n",
    "        for img_path in image_names:\n",
    "            img = io.imread(img_path)\n",
    "            \n",
    "            #img = transform.rescale(img, 1.0 / 20.0)\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            #48x64x3 -> 3x48x64\n",
    "            img = np.moveaxis(img, [0, 1, 2], [1, 2, 0])\n",
    "            \n",
    "            dataset.append(img)\n",
    "            \n",
    "            #label 10 will be 0\n",
    "            tmp = np.zeros(10)\n",
    "            tmp[int(g[-1])] = 1\n",
    "            labels.append(tmp)\n",
    "            #labels.append(int(g[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.array(dataset).astype(float)\n",
    "labels = np.array(labels).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test_val, y_train, y_test_val = train_test_split(dataset, labels, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.from_numpy(dataset).float()\n",
    "labels = torch.from_numpy(labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_dataset = utils.TensorDataset(dataset, labels) # create your dataset\n",
    "my_dataloader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dataset = utils.TensorDataset(X_train, y_train) # create your dataset\n",
    "train_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader\n",
    "\n",
    "my_dataset = utils.TensorDataset(X_val, y_val) # create your dataset\n",
    "val_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader\n",
    "\n",
    "my_dataset = utils.TensorDataset(X_test, y_test) # create your dataset\n",
    "test_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ResNet152()\n",
    "#Target has to be LongSensor for CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=5e-4)\n",
    "\n",
    "#optimizer = optim.SGD(resnet.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    net = ResNet152()\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "num_epochs = 200\n",
    "iter_per_epoch = len(train_loader)\n",
    "#224 lines -> 112 val output for log_nth=10000\n",
    "#1120000 iter\n",
    "log_nth = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):\n",
    "        #inputs, targets = Variable(inputs.float()), Variable(targets.float())\n",
    "        #for CrossEntropyLoss\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #loss = criterion(outputs, targets)\n",
    "        #for CrossEntropyLoss\n",
    "        loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_history.append(loss.data.cpu().numpy())\n",
    "        if log_nth and i % log_nth == 0:\n",
    "            last_log_nth_losses = train_loss_history[-log_nth:]\n",
    "            train_loss = np.mean(last_log_nth_losses)\n",
    "            print('[Iteration %d/%d] TRAIN loss: %.3f' % \\\n",
    "                        (i + epoch * iter_per_epoch,\n",
    "                         iter_per_epoch * num_epochs,\n",
    "                         train_loss))\n",
    "            \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "\n",
    "    train_acc = np.mean((preds == target_indices).data.cpu().numpy())\n",
    "    train_acc_history.append(train_acc)\n",
    "    if log_nth:\n",
    "        print('[Epoch %d/%d] TRAIN acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                            num_epochs,\n",
    "                                                            train_acc,\n",
    "                                                            loss))\n",
    "        '''_, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Only allow images/pixels with label >= 0 e.g. for segmentation\n",
    "        targets_mask = labels >= 0\n",
    "        train_acc = np.mean((preds == targets)[targets_mask].data.cpu().numpy())\n",
    "        train_acc_history.append(train_acc)\n",
    "        if log_nth:\n",
    "            print('[Epoch %d/%d] TRAIN acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                                   num_epochs,\n",
    "                                                                   train_acc,\n",
    "                                                                   train_loss))'''\n",
    "        \n",
    "    # VALIDATION\n",
    "    val_losses = []\n",
    "    val_scores = []\n",
    "    model.eval()\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        \n",
    "        #loss = criterion(outputs, targets)\n",
    "        \n",
    "        #for CrossEntropyLoss\n",
    "        loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "        \n",
    "        val_losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        _, target_indices = torch.max(targets, 1)\n",
    "\n",
    "        scores = np.mean((preds == target_indices).data.cpu().numpy())\n",
    "        val_scores.append(scores)\n",
    "\n",
    "    model.train()\n",
    "    val_acc, val_loss = np.mean(val_scores), np.mean(val_losses)\n",
    "    val_acc_history.append(val_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    if log_nth:\n",
    "        print('[Epoch %d/%d] VAL   acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                            num_epochs,\n",
    "                                                            val_acc,\n",
    "                                                            val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models\n",
    "currentDT = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"saved_models\", \"resnet_rgb_\" + str(num_epochs) + \"_\" + currentDT + \".model\")\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "#Loading\n",
    "#model = VGG('VGG16')\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = ResNet152()\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "\n",
    "#path = \"C:/Users/Bilal/git/gesture_recog_leap/saved_models/resnet_rgb_100_2018-07-04_06-59-19.model\"\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    scores.extend((preds == target_indices).data.cpu().numpy())\n",
    "    \n",
    "print('Test set accuracy: %f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, '-')\n",
    "#plt.plot(val_loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_history, '-o')\n",
    "plt.plot(val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    scores.extend((preds == target_indices).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    \n",
    "    numpy_inputs = inputs.data.cpu().numpy()\n",
    "    numpy_outputs = outputs.data.cpu().numpy()\n",
    "    numpy_targets = targets.data.cpu().numpy()\n",
    "    \n",
    "    img = numpy_inputs[0]\n",
    "    img = img[0, :, :]\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Predicted: \"+str(preds[0].item()) + \" Target: \"+str(target_indices[0].item()))\n",
    "    plt.show()\n",
    "    #currentDT = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    #path = \"images/img_\" + currentDT + \".png\"\n",
    "    #plt.imsave(path, img.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
