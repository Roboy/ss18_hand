{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Dataset\n",
    "import torch.utils.data as utils\n",
    "\n",
    "#Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For paths\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#imread and resize\n",
    "from skimage import io, transform\n",
    "\n",
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Timestamp\n",
    "import datetime\n",
    "\n",
    "#PyTorch Models\n",
    "path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"models\")\n",
    "sys.path.append(path)\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"kinect_leap_dataset\", \"acquisitions\")\n",
    "p_id = [\"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"P9\", \"P10\", \"P11\", \"P12\", \"P13\", \"P14\"]\n",
    "g_id = [\"G1\", \"G2\", \"G3\", \"G4\", \"G5\", \"G6\", \"G7\", \"G8\", \"G9\", \"G10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bilal\\git\\gesture_recog_leap\\kinect_leap_dataset\\acquisitions\\P1\\G1\n",
      "['C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\10_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\1_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\2_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\3_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\4_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\5_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\6_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\7_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\8_rgb.png', 'C:\\\\Users\\\\Bilal\\\\git\\\\gesture_recog_leap\\\\kinect_leap_dataset\\\\acquisitions\\\\P1\\\\G1\\\\9_rgb.png']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(os.path.join(root_path, \"P1\", \"G1\"))\n",
    "files = glob.glob(os.path.join(root_path, \"P1\", \"G1\", \"*rgb.png\"))\n",
    "print(files)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for p in p_id:\n",
    "    for g in g_id:\n",
    "        path = os.path.join(root_path, p, g)\n",
    "        image_names = glob.glob(os.path.join(path, \"*rgb.png\"))\n",
    "        for img_path in image_names:\n",
    "            img = io.imread(img_path)\n",
    "            \n",
    "            #img = transform.rescale(img, 1.0 / 20.0)\n",
    "            img = transform.resize(img, (32, 32))\n",
    "            #48x64x3 -> 3x48x64\n",
    "            img = np.moveaxis(img, [0, 1, 2], [1, 2, 0])\n",
    "            \n",
    "            dataset.append(img)\n",
    "            \n",
    "            #label 10 will be 0\n",
    "            tmp = np.zeros(10)\n",
    "            tmp[int(g[-1])] = 1\n",
    "            labels.append(tmp)\n",
    "            #labels.append(int(g[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.array(dataset).astype(float)\n",
    "labels = np.array(labels).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test_val, y_train, y_test_val = train_test_split(dataset, labels, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.from_numpy(dataset).float()\n",
    "labels = torch.from_numpy(labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_dataset = utils.TensorDataset(dataset, labels) # create your dataset\n",
    "my_dataloader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dataset = utils.TensorDataset(X_train, y_train) # create your dataset\n",
    "train_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader\n",
    "\n",
    "my_dataset = utils.TensorDataset(X_val, y_val) # create your dataset\n",
    "val_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader\n",
    "\n",
    "my_dataset = utils.TensorDataset(X_test, y_test) # create your dataset\n",
    "test_loader = utils.DataLoader(my_dataset, batch_size=10, shuffle=True, num_workers=4) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ResNet152()\n",
    "#Target has to be LongSensor for CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-8, weight_decay=5e-4)\n",
    "\n",
    "#optimizer = optim.SGD(resnet.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    net = ResNet152()\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 10/22400] TRAIN loss: 2.328\n",
      "[Iteration 20/22400] TRAIN loss: 2.324\n",
      "[Iteration 30/22400] TRAIN loss: 2.328\n",
      "[Iteration 40/22400] TRAIN loss: 2.352\n",
      "[Iteration 50/22400] TRAIN loss: 2.319\n",
      "[Iteration 60/22400] TRAIN loss: 2.306\n",
      "[Iteration 70/22400] TRAIN loss: 2.317\n",
      "[Iteration 80/22400] TRAIN loss: 2.320\n",
      "[Iteration 90/22400] TRAIN loss: 2.326\n",
      "[Iteration 100/22400] TRAIN loss: 2.327\n",
      "[Iteration 110/22400] TRAIN loss: 2.333\n",
      "[Epoch 1/200] TRAIN acc/loss: 0.100/2.312\n",
      "[Epoch 1/200] VAL   acc/loss: 0.086/2.316\n",
      "[Iteration 122/22400] TRAIN loss: 2.291\n",
      "[Iteration 132/22400] TRAIN loss: 2.292\n",
      "[Iteration 142/22400] TRAIN loss: 2.309\n",
      "[Iteration 152/22400] TRAIN loss: 2.309\n",
      "[Iteration 162/22400] TRAIN loss: 2.324\n",
      "[Iteration 172/22400] TRAIN loss: 2.309\n",
      "[Iteration 182/22400] TRAIN loss: 2.331\n",
      "[Iteration 192/22400] TRAIN loss: 2.323\n",
      "[Iteration 202/22400] TRAIN loss: 2.303\n",
      "[Iteration 212/22400] TRAIN loss: 2.327\n",
      "[Iteration 222/22400] TRAIN loss: 2.312\n",
      "[Epoch 2/200] TRAIN acc/loss: 0.000/2.321\n",
      "[Epoch 2/200] VAL   acc/loss: 0.121/2.332\n",
      "[Iteration 234/22400] TRAIN loss: 2.309\n",
      "[Iteration 244/22400] TRAIN loss: 2.323\n",
      "[Iteration 254/22400] TRAIN loss: 2.308\n",
      "[Iteration 264/22400] TRAIN loss: 2.273\n",
      "[Iteration 274/22400] TRAIN loss: 2.308\n",
      "[Iteration 284/22400] TRAIN loss: 2.287\n",
      "[Iteration 294/22400] TRAIN loss: 2.310\n",
      "[Iteration 304/22400] TRAIN loss: 2.283\n",
      "[Iteration 314/22400] TRAIN loss: 2.326\n",
      "[Iteration 324/22400] TRAIN loss: 2.341\n",
      "[Iteration 334/22400] TRAIN loss: 2.289\n",
      "[Epoch 3/200] TRAIN acc/loss: 0.000/2.319\n",
      "[Epoch 3/200] VAL   acc/loss: 0.093/2.315\n",
      "[Iteration 346/22400] TRAIN loss: 2.300\n",
      "[Iteration 356/22400] TRAIN loss: 2.269\n",
      "[Iteration 366/22400] TRAIN loss: 2.304\n",
      "[Iteration 376/22400] TRAIN loss: 2.304\n",
      "[Iteration 386/22400] TRAIN loss: 2.274\n",
      "[Iteration 396/22400] TRAIN loss: 2.314\n",
      "[Iteration 406/22400] TRAIN loss: 2.306\n",
      "[Iteration 416/22400] TRAIN loss: 2.309\n",
      "[Iteration 426/22400] TRAIN loss: 2.287\n",
      "[Iteration 436/22400] TRAIN loss: 2.331\n",
      "[Iteration 446/22400] TRAIN loss: 2.293\n",
      "[Epoch 4/200] TRAIN acc/loss: 0.000/2.353\n",
      "[Epoch 4/200] VAL   acc/loss: 0.093/2.321\n",
      "[Iteration 458/22400] TRAIN loss: 2.313\n",
      "[Iteration 468/22400] TRAIN loss: 2.297\n",
      "[Iteration 478/22400] TRAIN loss: 2.305\n",
      "[Iteration 488/22400] TRAIN loss: 2.293\n",
      "[Iteration 498/22400] TRAIN loss: 2.296\n",
      "[Iteration 508/22400] TRAIN loss: 2.301\n",
      "[Iteration 518/22400] TRAIN loss: 2.320\n",
      "[Iteration 528/22400] TRAIN loss: 2.287\n",
      "[Iteration 538/22400] TRAIN loss: 2.304\n",
      "[Iteration 548/22400] TRAIN loss: 2.305\n",
      "[Iteration 558/22400] TRAIN loss: 2.290\n",
      "[Epoch 5/200] TRAIN acc/loss: 0.000/2.266\n",
      "[Epoch 5/200] VAL   acc/loss: 0.107/2.345\n",
      "[Iteration 570/22400] TRAIN loss: 2.296\n",
      "[Iteration 580/22400] TRAIN loss: 2.286\n",
      "[Iteration 590/22400] TRAIN loss: 2.279\n",
      "[Iteration 600/22400] TRAIN loss: 2.304\n",
      "[Iteration 610/22400] TRAIN loss: 2.297\n",
      "[Iteration 620/22400] TRAIN loss: 2.297\n",
      "[Iteration 630/22400] TRAIN loss: 2.289\n",
      "[Iteration 640/22400] TRAIN loss: 2.321\n",
      "[Iteration 650/22400] TRAIN loss: 2.303\n",
      "[Iteration 660/22400] TRAIN loss: 2.306\n",
      "[Iteration 670/22400] TRAIN loss: 2.290\n",
      "[Epoch 6/200] TRAIN acc/loss: 0.200/2.256\n",
      "[Epoch 6/200] VAL   acc/loss: 0.093/2.317\n",
      "[Iteration 682/22400] TRAIN loss: 2.281\n",
      "[Iteration 692/22400] TRAIN loss: 2.282\n",
      "[Iteration 702/22400] TRAIN loss: 2.321\n",
      "[Iteration 712/22400] TRAIN loss: 2.298\n",
      "[Iteration 722/22400] TRAIN loss: 2.289\n",
      "[Iteration 732/22400] TRAIN loss: 2.298\n",
      "[Iteration 742/22400] TRAIN loss: 2.297\n",
      "[Iteration 752/22400] TRAIN loss: 2.271\n",
      "[Iteration 762/22400] TRAIN loss: 2.295\n",
      "[Iteration 772/22400] TRAIN loss: 2.319\n",
      "[Iteration 782/22400] TRAIN loss: 2.312\n",
      "[Epoch 7/200] TRAIN acc/loss: 0.200/2.271\n",
      "[Epoch 7/200] VAL   acc/loss: 0.114/2.283\n",
      "[Iteration 794/22400] TRAIN loss: 2.273\n",
      "[Iteration 804/22400] TRAIN loss: 2.291\n",
      "[Iteration 814/22400] TRAIN loss: 2.291\n",
      "[Iteration 824/22400] TRAIN loss: 2.272\n",
      "[Iteration 834/22400] TRAIN loss: 2.288\n",
      "[Iteration 844/22400] TRAIN loss: 2.280\n",
      "[Iteration 854/22400] TRAIN loss: 2.301\n",
      "[Iteration 864/22400] TRAIN loss: 2.280\n",
      "[Iteration 874/22400] TRAIN loss: 2.261\n",
      "[Iteration 884/22400] TRAIN loss: 2.328\n",
      "[Iteration 894/22400] TRAIN loss: 2.292\n",
      "[Epoch 8/200] TRAIN acc/loss: 0.200/2.314\n",
      "[Epoch 8/200] VAL   acc/loss: 0.093/2.289\n",
      "[Iteration 906/22400] TRAIN loss: 2.295\n",
      "[Iteration 916/22400] TRAIN loss: 2.295\n",
      "[Iteration 926/22400] TRAIN loss: 2.291\n",
      "[Iteration 936/22400] TRAIN loss: 2.299\n",
      "[Iteration 946/22400] TRAIN loss: 2.276\n",
      "[Iteration 956/22400] TRAIN loss: 2.259\n",
      "[Iteration 966/22400] TRAIN loss: 2.281\n",
      "[Iteration 976/22400] TRAIN loss: 2.276\n",
      "[Iteration 986/22400] TRAIN loss: 2.307\n",
      "[Iteration 996/22400] TRAIN loss: 2.267\n",
      "[Iteration 1006/22400] TRAIN loss: 2.301\n",
      "[Epoch 9/200] TRAIN acc/loss: 0.200/2.321\n",
      "[Epoch 9/200] VAL   acc/loss: 0.121/2.302\n",
      "[Iteration 1018/22400] TRAIN loss: 2.298\n",
      "[Iteration 1028/22400] TRAIN loss: 2.274\n",
      "[Iteration 1038/22400] TRAIN loss: 2.287\n",
      "[Iteration 1048/22400] TRAIN loss: 2.257\n",
      "[Iteration 1058/22400] TRAIN loss: 2.280\n",
      "[Iteration 1068/22400] TRAIN loss: 2.275\n",
      "[Iteration 1078/22400] TRAIN loss: 2.306\n",
      "[Iteration 1088/22400] TRAIN loss: 2.293\n",
      "[Iteration 1098/22400] TRAIN loss: 2.269\n",
      "[Iteration 1108/22400] TRAIN loss: 2.275\n",
      "[Iteration 1118/22400] TRAIN loss: 2.273\n",
      "[Epoch 10/200] TRAIN acc/loss: 0.200/2.237\n",
      "[Epoch 10/200] VAL   acc/loss: 0.121/2.305\n",
      "[Iteration 1130/22400] TRAIN loss: 2.276\n",
      "[Iteration 1140/22400] TRAIN loss: 2.261\n",
      "[Iteration 1150/22400] TRAIN loss: 2.256\n",
      "[Iteration 1160/22400] TRAIN loss: 2.279\n",
      "[Iteration 1170/22400] TRAIN loss: 2.266\n",
      "[Iteration 1180/22400] TRAIN loss: 2.311\n",
      "[Iteration 1190/22400] TRAIN loss: 2.349\n",
      "[Iteration 1200/22400] TRAIN loss: 2.307\n",
      "[Iteration 1210/22400] TRAIN loss: 2.287\n",
      "[Iteration 1220/22400] TRAIN loss: 2.272\n",
      "[Iteration 1230/22400] TRAIN loss: 2.241\n",
      "[Epoch 11/200] TRAIN acc/loss: 0.200/2.282\n",
      "[Epoch 11/200] VAL   acc/loss: 0.150/2.285\n",
      "[Iteration 1242/22400] TRAIN loss: 2.251\n",
      "[Iteration 1252/22400] TRAIN loss: 2.256\n",
      "[Iteration 1262/22400] TRAIN loss: 2.273\n",
      "[Iteration 1272/22400] TRAIN loss: 2.277\n",
      "[Iteration 1282/22400] TRAIN loss: 2.266\n",
      "[Iteration 1292/22400] TRAIN loss: 2.271\n",
      "[Iteration 1302/22400] TRAIN loss: 2.293\n",
      "[Iteration 1312/22400] TRAIN loss: 2.267\n",
      "[Iteration 1322/22400] TRAIN loss: 2.263\n",
      "[Iteration 1332/22400] TRAIN loss: 2.276\n",
      "[Iteration 1342/22400] TRAIN loss: 2.275\n",
      "[Epoch 12/200] TRAIN acc/loss: 0.200/2.242\n",
      "[Epoch 12/200] VAL   acc/loss: 0.136/2.286\n",
      "[Iteration 1354/22400] TRAIN loss: 2.261\n",
      "[Iteration 1364/22400] TRAIN loss: 2.262\n",
      "[Iteration 1374/22400] TRAIN loss: 2.260\n",
      "[Iteration 1384/22400] TRAIN loss: 2.303\n",
      "[Iteration 1394/22400] TRAIN loss: 2.252\n",
      "[Iteration 1404/22400] TRAIN loss: 2.264\n",
      "[Iteration 1414/22400] TRAIN loss: 2.291\n",
      "[Iteration 1424/22400] TRAIN loss: 2.292\n",
      "[Iteration 1434/22400] TRAIN loss: 2.254\n",
      "[Iteration 1444/22400] TRAIN loss: 2.261\n",
      "[Iteration 1454/22400] TRAIN loss: 2.284\n",
      "[Epoch 13/200] TRAIN acc/loss: 0.300/2.174\n",
      "[Epoch 13/200] VAL   acc/loss: 0.179/2.253\n",
      "[Iteration 1466/22400] TRAIN loss: 2.264\n",
      "[Iteration 1476/22400] TRAIN loss: 2.236\n",
      "[Iteration 1486/22400] TRAIN loss: 2.259\n",
      "[Iteration 1496/22400] TRAIN loss: 2.296\n",
      "[Iteration 1506/22400] TRAIN loss: 2.262\n",
      "[Iteration 1516/22400] TRAIN loss: 2.269\n",
      "[Iteration 1526/22400] TRAIN loss: 2.245\n",
      "[Iteration 1536/22400] TRAIN loss: 2.287\n",
      "[Iteration 1546/22400] TRAIN loss: 2.255\n",
      "[Iteration 1556/22400] TRAIN loss: 2.272\n",
      "[Iteration 1566/22400] TRAIN loss: 2.250\n",
      "[Epoch 14/200] TRAIN acc/loss: 0.200/2.184\n",
      "[Epoch 14/200] VAL   acc/loss: 0.157/2.255\n",
      "[Iteration 1578/22400] TRAIN loss: 2.286\n",
      "[Iteration 1588/22400] TRAIN loss: 2.264\n",
      "[Iteration 1598/22400] TRAIN loss: 2.251\n",
      "[Iteration 1608/22400] TRAIN loss: 2.256\n",
      "[Iteration 1618/22400] TRAIN loss: 2.265\n",
      "[Iteration 1628/22400] TRAIN loss: 2.258\n",
      "[Iteration 1638/22400] TRAIN loss: 2.296\n",
      "[Iteration 1648/22400] TRAIN loss: 2.231\n",
      "[Iteration 1658/22400] TRAIN loss: 2.264\n",
      "[Iteration 1668/22400] TRAIN loss: 2.288\n",
      "[Iteration 1678/22400] TRAIN loss: 2.274\n",
      "[Epoch 15/200] TRAIN acc/loss: 0.200/2.256\n",
      "[Epoch 15/200] VAL   acc/loss: 0.186/2.240\n",
      "[Iteration 1690/22400] TRAIN loss: 2.237\n",
      "[Iteration 1700/22400] TRAIN loss: 2.225\n",
      "[Iteration 1710/22400] TRAIN loss: 2.238\n",
      "[Iteration 1720/22400] TRAIN loss: 2.226\n",
      "[Iteration 1730/22400] TRAIN loss: 2.289\n",
      "[Iteration 1740/22400] TRAIN loss: 2.270\n",
      "[Iteration 1750/22400] TRAIN loss: 2.219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1760/22400] TRAIN loss: 2.242\n",
      "[Iteration 1770/22400] TRAIN loss: 2.229\n",
      "[Iteration 1780/22400] TRAIN loss: 2.238\n",
      "[Iteration 1790/22400] TRAIN loss: 2.241\n",
      "[Epoch 16/200] TRAIN acc/loss: 0.100/2.233\n",
      "[Epoch 16/200] VAL   acc/loss: 0.193/2.195\n",
      "[Iteration 1802/22400] TRAIN loss: 2.244\n",
      "[Iteration 1812/22400] TRAIN loss: 2.234\n",
      "[Iteration 1822/22400] TRAIN loss: 2.251\n",
      "[Iteration 1832/22400] TRAIN loss: 2.239\n",
      "[Iteration 1842/22400] TRAIN loss: 2.235\n",
      "[Iteration 1852/22400] TRAIN loss: 2.230\n",
      "[Iteration 1862/22400] TRAIN loss: 2.204\n",
      "[Iteration 1872/22400] TRAIN loss: 2.214\n",
      "[Iteration 1882/22400] TRAIN loss: 2.209\n",
      "[Iteration 1892/22400] TRAIN loss: 2.238\n",
      "[Iteration 1902/22400] TRAIN loss: 2.252\n",
      "[Epoch 17/200] TRAIN acc/loss: 0.400/2.142\n",
      "[Epoch 17/200] VAL   acc/loss: 0.207/2.206\n",
      "[Iteration 1914/22400] TRAIN loss: 2.249\n",
      "[Iteration 1924/22400] TRAIN loss: 2.238\n",
      "[Iteration 1934/22400] TRAIN loss: 2.224\n",
      "[Iteration 1944/22400] TRAIN loss: 2.203\n",
      "[Iteration 1954/22400] TRAIN loss: 2.254\n",
      "[Iteration 1964/22400] TRAIN loss: 2.223\n",
      "[Iteration 1974/22400] TRAIN loss: 2.166\n",
      "[Iteration 1984/22400] TRAIN loss: 2.219\n",
      "[Iteration 1994/22400] TRAIN loss: 2.230\n",
      "[Iteration 2004/22400] TRAIN loss: 2.173\n",
      "[Iteration 2014/22400] TRAIN loss: 2.142\n",
      "[Epoch 18/200] TRAIN acc/loss: 0.200/2.278\n",
      "[Epoch 18/200] VAL   acc/loss: 0.214/2.184\n",
      "[Iteration 2026/22400] TRAIN loss: 2.158\n",
      "[Iteration 2036/22400] TRAIN loss: 2.189\n",
      "[Iteration 2046/22400] TRAIN loss: 2.205\n",
      "[Iteration 2056/22400] TRAIN loss: 2.198\n",
      "[Iteration 2066/22400] TRAIN loss: 2.228\n",
      "[Iteration 2076/22400] TRAIN loss: 2.196\n",
      "[Iteration 2086/22400] TRAIN loss: 2.206\n",
      "[Iteration 2096/22400] TRAIN loss: 2.214\n",
      "[Iteration 2106/22400] TRAIN loss: 2.175\n",
      "[Iteration 2116/22400] TRAIN loss: 2.176\n",
      "[Iteration 2126/22400] TRAIN loss: 2.165\n",
      "[Epoch 19/200] TRAIN acc/loss: 0.300/2.061\n",
      "[Epoch 19/200] VAL   acc/loss: 0.157/2.208\n",
      "[Iteration 2138/22400] TRAIN loss: 2.272\n",
      "[Iteration 2148/22400] TRAIN loss: 2.164\n",
      "[Iteration 2158/22400] TRAIN loss: 2.144\n",
      "[Iteration 2168/22400] TRAIN loss: 2.139\n",
      "[Iteration 2178/22400] TRAIN loss: 2.178\n",
      "[Iteration 2188/22400] TRAIN loss: 2.141\n",
      "[Iteration 2198/22400] TRAIN loss: 2.192\n",
      "[Iteration 2208/22400] TRAIN loss: 2.105\n",
      "[Iteration 2218/22400] TRAIN loss: 2.169\n",
      "[Iteration 2228/22400] TRAIN loss: 2.135\n",
      "[Iteration 2238/22400] TRAIN loss: 2.114\n",
      "[Epoch 20/200] TRAIN acc/loss: 0.400/1.979\n",
      "[Epoch 20/200] VAL   acc/loss: 0.157/2.344\n",
      "[Iteration 2250/22400] TRAIN loss: 2.166\n",
      "[Iteration 2260/22400] TRAIN loss: 2.131\n",
      "[Iteration 2270/22400] TRAIN loss: 2.106\n",
      "[Iteration 2280/22400] TRAIN loss: 2.102\n",
      "[Iteration 2290/22400] TRAIN loss: 2.236\n",
      "[Iteration 2300/22400] TRAIN loss: 2.161\n",
      "[Iteration 2310/22400] TRAIN loss: 2.159\n",
      "[Iteration 2320/22400] TRAIN loss: 2.190\n",
      "[Iteration 2330/22400] TRAIN loss: 2.174\n",
      "[Iteration 2340/22400] TRAIN loss: 2.196\n",
      "[Iteration 2350/22400] TRAIN loss: 2.176\n",
      "[Epoch 21/200] TRAIN acc/loss: 0.100/2.245\n",
      "[Epoch 21/200] VAL   acc/loss: 0.143/2.382\n",
      "[Iteration 2362/22400] TRAIN loss: 2.123\n",
      "[Iteration 2372/22400] TRAIN loss: 2.142\n",
      "[Iteration 2382/22400] TRAIN loss: 2.104\n",
      "[Iteration 2392/22400] TRAIN loss: 2.111\n",
      "[Iteration 2402/22400] TRAIN loss: 2.120\n",
      "[Iteration 2412/22400] TRAIN loss: 2.132\n",
      "[Iteration 2422/22400] TRAIN loss: 2.158\n",
      "[Iteration 2432/22400] TRAIN loss: 2.169\n",
      "[Iteration 2442/22400] TRAIN loss: 2.127\n",
      "[Iteration 2452/22400] TRAIN loss: 2.157\n",
      "[Iteration 2462/22400] TRAIN loss: 2.141\n",
      "[Epoch 22/200] TRAIN acc/loss: 0.000/2.239\n",
      "[Epoch 22/200] VAL   acc/loss: 0.186/2.218\n",
      "[Iteration 2474/22400] TRAIN loss: 2.101\n",
      "[Iteration 2484/22400] TRAIN loss: 1.987\n",
      "[Iteration 2494/22400] TRAIN loss: 2.132\n",
      "[Iteration 2504/22400] TRAIN loss: 2.149\n",
      "[Iteration 2514/22400] TRAIN loss: 2.192\n",
      "[Iteration 2524/22400] TRAIN loss: 2.104\n",
      "[Iteration 2534/22400] TRAIN loss: 2.020\n",
      "[Iteration 2544/22400] TRAIN loss: 2.080\n",
      "[Iteration 2554/22400] TRAIN loss: 2.098\n",
      "[Iteration 2564/22400] TRAIN loss: 2.104\n",
      "[Iteration 2574/22400] TRAIN loss: 2.165\n",
      "[Epoch 23/200] TRAIN acc/loss: 0.500/1.880\n",
      "[Epoch 23/200] VAL   acc/loss: 0.207/2.230\n",
      "[Iteration 2586/22400] TRAIN loss: 1.960\n",
      "[Iteration 2596/22400] TRAIN loss: 2.115\n",
      "[Iteration 2606/22400] TRAIN loss: 2.065\n",
      "[Iteration 2616/22400] TRAIN loss: 2.077\n",
      "[Iteration 2626/22400] TRAIN loss: 2.182\n",
      "[Iteration 2636/22400] TRAIN loss: 2.081\n",
      "[Iteration 2646/22400] TRAIN loss: 2.032\n",
      "[Iteration 2656/22400] TRAIN loss: 2.063\n",
      "[Iteration 2666/22400] TRAIN loss: 2.204\n",
      "[Iteration 2676/22400] TRAIN loss: 1.990\n",
      "[Iteration 2686/22400] TRAIN loss: 2.140\n",
      "[Epoch 24/200] TRAIN acc/loss: 0.200/2.113\n",
      "[Epoch 24/200] VAL   acc/loss: 0.207/2.141\n",
      "[Iteration 2698/22400] TRAIN loss: 2.029\n",
      "[Iteration 2708/22400] TRAIN loss: 2.043\n",
      "[Iteration 2718/22400] TRAIN loss: 2.029\n",
      "[Iteration 2728/22400] TRAIN loss: 2.124\n",
      "[Iteration 2738/22400] TRAIN loss: 2.050\n",
      "[Iteration 2748/22400] TRAIN loss: 2.095\n",
      "[Iteration 2758/22400] TRAIN loss: 2.027\n",
      "[Iteration 2768/22400] TRAIN loss: 2.051\n",
      "[Iteration 2778/22400] TRAIN loss: 2.069\n",
      "[Iteration 2788/22400] TRAIN loss: 1.979\n",
      "[Iteration 2798/22400] TRAIN loss: 1.987\n",
      "[Epoch 25/200] TRAIN acc/loss: 0.400/1.896\n",
      "[Epoch 25/200] VAL   acc/loss: 0.179/2.136\n",
      "[Iteration 2810/22400] TRAIN loss: 2.010\n",
      "[Iteration 2820/22400] TRAIN loss: 2.045\n",
      "[Iteration 2830/22400] TRAIN loss: 1.965\n",
      "[Iteration 2840/22400] TRAIN loss: 1.956\n",
      "[Iteration 2850/22400] TRAIN loss: 2.074\n",
      "[Iteration 2860/22400] TRAIN loss: 2.023\n",
      "[Iteration 2870/22400] TRAIN loss: 1.986\n",
      "[Iteration 2880/22400] TRAIN loss: 2.024\n",
      "[Iteration 2890/22400] TRAIN loss: 2.060\n",
      "[Iteration 2900/22400] TRAIN loss: 2.140\n",
      "[Iteration 2910/22400] TRAIN loss: 1.899\n",
      "[Epoch 26/200] TRAIN acc/loss: 0.000/2.259\n",
      "[Epoch 26/200] VAL   acc/loss: 0.221/2.062\n",
      "[Iteration 2922/22400] TRAIN loss: 1.865\n",
      "[Iteration 2932/22400] TRAIN loss: 1.956\n",
      "[Iteration 2942/22400] TRAIN loss: 2.032\n",
      "[Iteration 2952/22400] TRAIN loss: 1.951\n",
      "[Iteration 2962/22400] TRAIN loss: 2.106\n",
      "[Iteration 2972/22400] TRAIN loss: 1.982\n",
      "[Iteration 2982/22400] TRAIN loss: 2.029\n",
      "[Iteration 2992/22400] TRAIN loss: 1.965\n",
      "[Iteration 3002/22400] TRAIN loss: 2.010\n",
      "[Iteration 3012/22400] TRAIN loss: 1.999\n",
      "[Iteration 3022/22400] TRAIN loss: 1.913\n",
      "[Epoch 27/200] TRAIN acc/loss: 0.300/2.001\n",
      "[Epoch 27/200] VAL   acc/loss: 0.286/2.022\n",
      "[Iteration 3034/22400] TRAIN loss: 2.005\n",
      "[Iteration 3044/22400] TRAIN loss: 1.908\n",
      "[Iteration 3054/22400] TRAIN loss: 1.923\n",
      "[Iteration 3064/22400] TRAIN loss: 1.947\n",
      "[Iteration 3074/22400] TRAIN loss: 1.831\n",
      "[Iteration 3084/22400] TRAIN loss: 1.968\n",
      "[Iteration 3094/22400] TRAIN loss: 1.985\n",
      "[Iteration 3104/22400] TRAIN loss: 2.009\n",
      "[Iteration 3114/22400] TRAIN loss: 1.945\n",
      "[Iteration 3124/22400] TRAIN loss: 1.894\n",
      "[Iteration 3134/22400] TRAIN loss: 1.989\n",
      "[Epoch 28/200] TRAIN acc/loss: 0.500/1.912\n",
      "[Epoch 28/200] VAL   acc/loss: 0.236/2.064\n",
      "[Iteration 3146/22400] TRAIN loss: 2.035\n",
      "[Iteration 3156/22400] TRAIN loss: 1.901\n",
      "[Iteration 3166/22400] TRAIN loss: 1.932\n",
      "[Iteration 3176/22400] TRAIN loss: 1.911\n",
      "[Iteration 3186/22400] TRAIN loss: 1.941\n",
      "[Iteration 3196/22400] TRAIN loss: 1.847\n",
      "[Iteration 3206/22400] TRAIN loss: 1.943\n",
      "[Iteration 3216/22400] TRAIN loss: 2.031\n",
      "[Iteration 3226/22400] TRAIN loss: 1.843\n",
      "[Iteration 3236/22400] TRAIN loss: 1.903\n",
      "[Iteration 3246/22400] TRAIN loss: 2.039\n",
      "[Epoch 29/200] TRAIN acc/loss: 0.200/2.102\n",
      "[Epoch 29/200] VAL   acc/loss: 0.221/1.977\n",
      "[Iteration 3258/22400] TRAIN loss: 1.896\n",
      "[Iteration 3268/22400] TRAIN loss: 1.909\n",
      "[Iteration 3278/22400] TRAIN loss: 1.809\n",
      "[Iteration 3288/22400] TRAIN loss: 1.930\n",
      "[Iteration 3298/22400] TRAIN loss: 1.924\n",
      "[Iteration 3308/22400] TRAIN loss: 1.818\n",
      "[Iteration 3318/22400] TRAIN loss: 1.837\n",
      "[Iteration 3328/22400] TRAIN loss: 1.833\n",
      "[Iteration 3338/22400] TRAIN loss: 1.897\n",
      "[Iteration 3348/22400] TRAIN loss: 1.890\n",
      "[Iteration 3358/22400] TRAIN loss: 1.822\n",
      "[Epoch 30/200] TRAIN acc/loss: 0.400/1.871\n",
      "[Epoch 30/200] VAL   acc/loss: 0.193/2.119\n",
      "[Iteration 3370/22400] TRAIN loss: 1.839\n",
      "[Iteration 3380/22400] TRAIN loss: 1.753\n",
      "[Iteration 3390/22400] TRAIN loss: 1.871\n",
      "[Iteration 3400/22400] TRAIN loss: 1.925\n",
      "[Iteration 3410/22400] TRAIN loss: 1.743\n",
      "[Iteration 3420/22400] TRAIN loss: 1.767\n",
      "[Iteration 3430/22400] TRAIN loss: 1.837\n",
      "[Iteration 3440/22400] TRAIN loss: 1.927\n",
      "[Iteration 3450/22400] TRAIN loss: 1.977\n",
      "[Iteration 3460/22400] TRAIN loss: 1.789\n",
      "[Iteration 3470/22400] TRAIN loss: 1.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31/200] TRAIN acc/loss: 0.400/1.640\n",
      "[Epoch 31/200] VAL   acc/loss: 0.250/2.014\n",
      "[Iteration 3482/22400] TRAIN loss: 1.746\n",
      "[Iteration 3492/22400] TRAIN loss: 1.754\n",
      "[Iteration 3502/22400] TRAIN loss: 1.856\n",
      "[Iteration 3512/22400] TRAIN loss: 1.772\n",
      "[Iteration 3522/22400] TRAIN loss: 1.838\n",
      "[Iteration 3532/22400] TRAIN loss: 1.682\n",
      "[Iteration 3542/22400] TRAIN loss: 1.834\n",
      "[Iteration 3552/22400] TRAIN loss: 1.870\n",
      "[Iteration 3562/22400] TRAIN loss: 1.877\n",
      "[Iteration 3572/22400] TRAIN loss: 1.858\n",
      "[Iteration 3582/22400] TRAIN loss: 1.857\n",
      "[Epoch 32/200] TRAIN acc/loss: 0.200/1.817\n",
      "[Epoch 32/200] VAL   acc/loss: 0.257/1.940\n",
      "[Iteration 3594/22400] TRAIN loss: 1.801\n",
      "[Iteration 3604/22400] TRAIN loss: 1.701\n",
      "[Iteration 3614/22400] TRAIN loss: 1.698\n",
      "[Iteration 3624/22400] TRAIN loss: 1.758\n",
      "[Iteration 3634/22400] TRAIN loss: 1.815\n",
      "[Iteration 3644/22400] TRAIN loss: 1.803\n",
      "[Iteration 3654/22400] TRAIN loss: 1.785\n",
      "[Iteration 3664/22400] TRAIN loss: 1.767\n",
      "[Iteration 3674/22400] TRAIN loss: 1.813\n",
      "[Iteration 3684/22400] TRAIN loss: 1.843\n",
      "[Iteration 3694/22400] TRAIN loss: 1.785\n",
      "[Epoch 33/200] TRAIN acc/loss: 0.200/2.198\n",
      "[Epoch 33/200] VAL   acc/loss: 0.314/1.946\n",
      "[Iteration 3706/22400] TRAIN loss: 1.713\n",
      "[Iteration 3716/22400] TRAIN loss: 1.684\n",
      "[Iteration 3726/22400] TRAIN loss: 1.632\n",
      "[Iteration 3736/22400] TRAIN loss: 1.694\n",
      "[Iteration 3746/22400] TRAIN loss: 1.684\n",
      "[Iteration 3756/22400] TRAIN loss: 1.753\n",
      "[Iteration 3766/22400] TRAIN loss: 1.819\n",
      "[Iteration 3776/22400] TRAIN loss: 1.694\n",
      "[Iteration 3786/22400] TRAIN loss: 1.771\n",
      "[Iteration 3796/22400] TRAIN loss: 1.656\n",
      "[Iteration 3806/22400] TRAIN loss: 1.567\n",
      "[Epoch 34/200] TRAIN acc/loss: 0.300/1.846\n",
      "[Epoch 34/200] VAL   acc/loss: 0.314/1.944\n",
      "[Iteration 3818/22400] TRAIN loss: 1.638\n",
      "[Iteration 3828/22400] TRAIN loss: 1.862\n",
      "[Iteration 3838/22400] TRAIN loss: 1.653\n",
      "[Iteration 3848/22400] TRAIN loss: 1.646\n",
      "[Iteration 3858/22400] TRAIN loss: 1.646\n",
      "[Iteration 3868/22400] TRAIN loss: 1.757\n",
      "[Iteration 3878/22400] TRAIN loss: 1.752\n",
      "[Iteration 3888/22400] TRAIN loss: 1.742\n",
      "[Iteration 3898/22400] TRAIN loss: 1.767\n",
      "[Iteration 3908/22400] TRAIN loss: 1.644\n",
      "[Iteration 3918/22400] TRAIN loss: 1.700\n",
      "[Epoch 35/200] TRAIN acc/loss: 0.500/1.617\n",
      "[Epoch 35/200] VAL   acc/loss: 0.329/1.895\n",
      "[Iteration 3930/22400] TRAIN loss: 1.612\n",
      "[Iteration 3940/22400] TRAIN loss: 1.597\n",
      "[Iteration 3950/22400] TRAIN loss: 1.597\n",
      "[Iteration 3960/22400] TRAIN loss: 1.655\n",
      "[Iteration 3970/22400] TRAIN loss: 1.555\n",
      "[Iteration 3980/22400] TRAIN loss: 1.684\n",
      "[Iteration 3990/22400] TRAIN loss: 1.567\n",
      "[Iteration 4000/22400] TRAIN loss: 1.696\n",
      "[Iteration 4010/22400] TRAIN loss: 1.746\n",
      "[Iteration 4020/22400] TRAIN loss: 1.696\n",
      "[Iteration 4030/22400] TRAIN loss: 1.552\n",
      "[Epoch 36/200] TRAIN acc/loss: 0.500/1.277\n",
      "[Epoch 36/200] VAL   acc/loss: 0.321/1.853\n",
      "[Iteration 4042/22400] TRAIN loss: 1.553\n",
      "[Iteration 4052/22400] TRAIN loss: 1.489\n",
      "[Iteration 4062/22400] TRAIN loss: 1.552\n",
      "[Iteration 4072/22400] TRAIN loss: 1.546\n",
      "[Iteration 4082/22400] TRAIN loss: 1.527\n",
      "[Iteration 4092/22400] TRAIN loss: 1.576\n",
      "[Iteration 4102/22400] TRAIN loss: 1.672\n",
      "[Iteration 4112/22400] TRAIN loss: 1.691\n",
      "[Iteration 4122/22400] TRAIN loss: 1.781\n",
      "[Iteration 4132/22400] TRAIN loss: 1.554\n",
      "[Iteration 4142/22400] TRAIN loss: 1.554\n",
      "[Epoch 37/200] TRAIN acc/loss: 0.500/1.322\n",
      "[Epoch 37/200] VAL   acc/loss: 0.293/1.915\n",
      "[Iteration 4154/22400] TRAIN loss: 1.433\n",
      "[Iteration 4164/22400] TRAIN loss: 1.620\n",
      "[Iteration 4174/22400] TRAIN loss: 1.451\n",
      "[Iteration 4184/22400] TRAIN loss: 1.621\n",
      "[Iteration 4194/22400] TRAIN loss: 1.541\n",
      "[Iteration 4204/22400] TRAIN loss: 1.461\n",
      "[Iteration 4214/22400] TRAIN loss: 1.606\n",
      "[Iteration 4224/22400] TRAIN loss: 1.639\n",
      "[Iteration 4234/22400] TRAIN loss: 1.651\n",
      "[Iteration 4244/22400] TRAIN loss: 1.587\n",
      "[Iteration 4254/22400] TRAIN loss: 1.614\n",
      "[Epoch 38/200] TRAIN acc/loss: 0.600/1.492\n",
      "[Epoch 38/200] VAL   acc/loss: 0.336/1.779\n",
      "[Iteration 4266/22400] TRAIN loss: 1.411\n",
      "[Iteration 4276/22400] TRAIN loss: 1.462\n",
      "[Iteration 4286/22400] TRAIN loss: 1.456\n",
      "[Iteration 4296/22400] TRAIN loss: 1.318\n",
      "[Iteration 4306/22400] TRAIN loss: 1.370\n",
      "[Iteration 4316/22400] TRAIN loss: 1.460\n",
      "[Iteration 4326/22400] TRAIN loss: 1.612\n",
      "[Iteration 4336/22400] TRAIN loss: 1.467\n",
      "[Iteration 4346/22400] TRAIN loss: 1.562\n",
      "[Iteration 4356/22400] TRAIN loss: 1.554\n",
      "[Iteration 4366/22400] TRAIN loss: 1.681\n",
      "[Epoch 39/200] TRAIN acc/loss: 0.400/1.494\n",
      "[Epoch 39/200] VAL   acc/loss: 0.414/1.728\n",
      "[Iteration 4378/22400] TRAIN loss: 1.271\n",
      "[Iteration 4388/22400] TRAIN loss: 1.478\n",
      "[Iteration 4398/22400] TRAIN loss: 1.545\n",
      "[Iteration 4408/22400] TRAIN loss: 1.396\n",
      "[Iteration 4418/22400] TRAIN loss: 1.463\n",
      "[Iteration 4428/22400] TRAIN loss: 1.395\n",
      "[Iteration 4438/22400] TRAIN loss: 1.430\n",
      "[Iteration 4448/22400] TRAIN loss: 1.408\n",
      "[Iteration 4458/22400] TRAIN loss: 1.639\n",
      "[Iteration 4468/22400] TRAIN loss: 1.499\n",
      "[Iteration 4478/22400] TRAIN loss: 1.509\n",
      "[Epoch 40/200] TRAIN acc/loss: 0.500/1.237\n",
      "[Epoch 40/200] VAL   acc/loss: 0.336/1.831\n",
      "[Iteration 4490/22400] TRAIN loss: 1.479\n",
      "[Iteration 4500/22400] TRAIN loss: 1.525\n",
      "[Iteration 4510/22400] TRAIN loss: 1.383\n",
      "[Iteration 4520/22400] TRAIN loss: 1.378\n",
      "[Iteration 4530/22400] TRAIN loss: 1.354\n",
      "[Iteration 4540/22400] TRAIN loss: 1.338\n",
      "[Iteration 4550/22400] TRAIN loss: 1.380\n",
      "[Iteration 4560/22400] TRAIN loss: 1.359\n",
      "[Iteration 4570/22400] TRAIN loss: 1.326\n",
      "[Iteration 4580/22400] TRAIN loss: 1.506\n",
      "[Iteration 4590/22400] TRAIN loss: 1.432\n",
      "[Epoch 41/200] TRAIN acc/loss: 0.500/1.487\n",
      "[Epoch 41/200] VAL   acc/loss: 0.407/1.568\n",
      "[Iteration 4602/22400] TRAIN loss: 1.363\n",
      "[Iteration 4612/22400] TRAIN loss: 1.340\n",
      "[Iteration 4622/22400] TRAIN loss: 1.198\n",
      "[Iteration 4632/22400] TRAIN loss: 1.405\n",
      "[Iteration 4642/22400] TRAIN loss: 1.348\n",
      "[Iteration 4652/22400] TRAIN loss: 1.278\n",
      "[Iteration 4662/22400] TRAIN loss: 1.282\n",
      "[Iteration 4672/22400] TRAIN loss: 1.276\n",
      "[Iteration 4682/22400] TRAIN loss: 1.345\n",
      "[Iteration 4692/22400] TRAIN loss: 1.335\n",
      "[Iteration 4702/22400] TRAIN loss: 1.294\n",
      "[Epoch 42/200] TRAIN acc/loss: 0.700/1.040\n",
      "[Epoch 42/200] VAL   acc/loss: 0.407/1.588\n",
      "[Iteration 4714/22400] TRAIN loss: 1.238\n",
      "[Iteration 4724/22400] TRAIN loss: 1.380\n",
      "[Iteration 4734/22400] TRAIN loss: 1.363\n",
      "[Iteration 4744/22400] TRAIN loss: 1.424\n",
      "[Iteration 4754/22400] TRAIN loss: 1.307\n",
      "[Iteration 4764/22400] TRAIN loss: 1.324\n",
      "[Iteration 4774/22400] TRAIN loss: 1.238\n",
      "[Iteration 4784/22400] TRAIN loss: 1.202\n",
      "[Iteration 4794/22400] TRAIN loss: 1.170\n",
      "[Iteration 4804/22400] TRAIN loss: 1.268\n",
      "[Iteration 4814/22400] TRAIN loss: 1.098\n",
      "[Epoch 43/200] TRAIN acc/loss: 0.300/1.810\n",
      "[Epoch 43/200] VAL   acc/loss: 0.421/1.586\n",
      "[Iteration 4826/22400] TRAIN loss: 1.228\n",
      "[Iteration 4836/22400] TRAIN loss: 1.311\n",
      "[Iteration 4846/22400] TRAIN loss: 1.193\n",
      "[Iteration 4856/22400] TRAIN loss: 1.229\n",
      "[Iteration 4866/22400] TRAIN loss: 1.279\n",
      "[Iteration 4876/22400] TRAIN loss: 1.038\n",
      "[Iteration 4886/22400] TRAIN loss: 1.161\n",
      "[Iteration 4896/22400] TRAIN loss: 1.223\n",
      "[Iteration 4906/22400] TRAIN loss: 1.235\n",
      "[Iteration 4916/22400] TRAIN loss: 1.302\n",
      "[Iteration 4926/22400] TRAIN loss: 1.263\n",
      "[Epoch 44/200] TRAIN acc/loss: 0.500/1.282\n",
      "[Epoch 44/200] VAL   acc/loss: 0.421/1.687\n",
      "[Iteration 4938/22400] TRAIN loss: 1.307\n",
      "[Iteration 4948/22400] TRAIN loss: 1.431\n",
      "[Iteration 4958/22400] TRAIN loss: 1.301\n",
      "[Iteration 4968/22400] TRAIN loss: 1.140\n",
      "[Iteration 4978/22400] TRAIN loss: 1.207\n",
      "[Iteration 4988/22400] TRAIN loss: 1.216\n",
      "[Iteration 4998/22400] TRAIN loss: 1.218\n",
      "[Iteration 5008/22400] TRAIN loss: 1.177\n",
      "[Iteration 5018/22400] TRAIN loss: 1.171\n",
      "[Iteration 5028/22400] TRAIN loss: 1.165\n",
      "[Iteration 5038/22400] TRAIN loss: 1.111\n",
      "[Epoch 45/200] TRAIN acc/loss: 0.700/0.816\n",
      "[Epoch 45/200] VAL   acc/loss: 0.393/1.695\n",
      "[Iteration 5050/22400] TRAIN loss: 1.174\n",
      "[Iteration 5060/22400] TRAIN loss: 1.071\n",
      "[Iteration 5070/22400] TRAIN loss: 1.088\n",
      "[Iteration 5080/22400] TRAIN loss: 1.059\n",
      "[Iteration 5090/22400] TRAIN loss: 1.095\n",
      "[Iteration 5100/22400] TRAIN loss: 1.189\n",
      "[Iteration 5110/22400] TRAIN loss: 1.142\n",
      "[Iteration 5120/22400] TRAIN loss: 1.099\n",
      "[Iteration 5130/22400] TRAIN loss: 1.034\n",
      "[Iteration 5140/22400] TRAIN loss: 1.171\n",
      "[Iteration 5150/22400] TRAIN loss: 1.253\n",
      "[Epoch 46/200] TRAIN acc/loss: 0.600/1.271\n",
      "[Epoch 46/200] VAL   acc/loss: 0.421/1.477\n",
      "[Iteration 5162/22400] TRAIN loss: 1.020\n",
      "[Iteration 5172/22400] TRAIN loss: 1.222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 5182/22400] TRAIN loss: 1.131\n",
      "[Iteration 5192/22400] TRAIN loss: 1.147\n",
      "[Iteration 5202/22400] TRAIN loss: 1.140\n",
      "[Iteration 5212/22400] TRAIN loss: 1.083\n",
      "[Iteration 5222/22400] TRAIN loss: 1.170\n",
      "[Iteration 5232/22400] TRAIN loss: 1.202\n",
      "[Iteration 5242/22400] TRAIN loss: 1.114\n",
      "[Iteration 5252/22400] TRAIN loss: 1.231\n",
      "[Iteration 5262/22400] TRAIN loss: 1.335\n",
      "[Epoch 47/200] TRAIN acc/loss: 0.800/0.946\n",
      "[Epoch 47/200] VAL   acc/loss: 0.400/1.705\n",
      "[Iteration 5274/22400] TRAIN loss: 1.081\n",
      "[Iteration 5284/22400] TRAIN loss: 0.996\n",
      "[Iteration 5294/22400] TRAIN loss: 0.928\n",
      "[Iteration 5304/22400] TRAIN loss: 1.106\n",
      "[Iteration 5314/22400] TRAIN loss: 0.967\n",
      "[Iteration 5324/22400] TRAIN loss: 1.042\n",
      "[Iteration 5334/22400] TRAIN loss: 1.169\n",
      "[Iteration 5344/22400] TRAIN loss: 1.121\n",
      "[Iteration 5354/22400] TRAIN loss: 1.066\n",
      "[Iteration 5364/22400] TRAIN loss: 1.076\n",
      "[Iteration 5374/22400] TRAIN loss: 1.098\n",
      "[Epoch 48/200] TRAIN acc/loss: 0.500/1.292\n",
      "[Epoch 48/200] VAL   acc/loss: 0.457/1.501\n",
      "[Iteration 5386/22400] TRAIN loss: 0.992\n",
      "[Iteration 5396/22400] TRAIN loss: 1.122\n",
      "[Iteration 5406/22400] TRAIN loss: 0.949\n",
      "[Iteration 5416/22400] TRAIN loss: 1.119\n",
      "[Iteration 5426/22400] TRAIN loss: 1.148\n",
      "[Iteration 5436/22400] TRAIN loss: 0.980\n",
      "[Iteration 5446/22400] TRAIN loss: 0.967\n",
      "[Iteration 5456/22400] TRAIN loss: 1.177\n",
      "[Iteration 5466/22400] TRAIN loss: 1.223\n",
      "[Iteration 5476/22400] TRAIN loss: 0.928\n",
      "[Iteration 5486/22400] TRAIN loss: 0.969\n",
      "[Epoch 49/200] TRAIN acc/loss: 0.400/1.705\n",
      "[Epoch 49/200] VAL   acc/loss: 0.500/1.367\n",
      "[Iteration 5498/22400] TRAIN loss: 1.011\n",
      "[Iteration 5508/22400] TRAIN loss: 1.217\n",
      "[Iteration 5518/22400] TRAIN loss: 0.985\n",
      "[Iteration 5528/22400] TRAIN loss: 0.978\n",
      "[Iteration 5538/22400] TRAIN loss: 0.993\n",
      "[Iteration 5548/22400] TRAIN loss: 0.981\n",
      "[Iteration 5558/22400] TRAIN loss: 1.007\n",
      "[Iteration 5568/22400] TRAIN loss: 1.073\n",
      "[Iteration 5578/22400] TRAIN loss: 1.040\n",
      "[Iteration 5588/22400] TRAIN loss: 1.179\n",
      "[Iteration 5598/22400] TRAIN loss: 1.108\n",
      "[Epoch 50/200] TRAIN acc/loss: 0.500/1.154\n",
      "[Epoch 50/200] VAL   acc/loss: 0.529/1.411\n",
      "[Iteration 5610/22400] TRAIN loss: 0.975\n",
      "[Iteration 5620/22400] TRAIN loss: 0.934\n",
      "[Iteration 5630/22400] TRAIN loss: 0.986\n",
      "[Iteration 5640/22400] TRAIN loss: 0.963\n",
      "[Iteration 5650/22400] TRAIN loss: 1.139\n",
      "[Iteration 5660/22400] TRAIN loss: 0.943\n",
      "[Iteration 5670/22400] TRAIN loss: 0.901\n",
      "[Iteration 5680/22400] TRAIN loss: 0.905\n",
      "[Iteration 5690/22400] TRAIN loss: 1.050\n",
      "[Iteration 5700/22400] TRAIN loss: 1.100\n",
      "[Iteration 5710/22400] TRAIN loss: 0.985\n",
      "[Epoch 51/200] TRAIN acc/loss: 0.600/1.432\n",
      "[Epoch 51/200] VAL   acc/loss: 0.514/1.499\n",
      "[Iteration 5722/22400] TRAIN loss: 0.859\n",
      "[Iteration 5732/22400] TRAIN loss: 0.890\n",
      "[Iteration 5742/22400] TRAIN loss: 0.756\n",
      "[Iteration 5752/22400] TRAIN loss: 1.067\n",
      "[Iteration 5762/22400] TRAIN loss: 0.828\n",
      "[Iteration 5772/22400] TRAIN loss: 0.909\n",
      "[Iteration 5782/22400] TRAIN loss: 0.824\n",
      "[Iteration 5792/22400] TRAIN loss: 0.786\n",
      "[Iteration 5802/22400] TRAIN loss: 0.689\n",
      "[Iteration 5812/22400] TRAIN loss: 1.016\n",
      "[Iteration 5822/22400] TRAIN loss: 1.047\n",
      "[Epoch 52/200] TRAIN acc/loss: 0.600/1.308\n",
      "[Epoch 52/200] VAL   acc/loss: 0.514/1.435\n",
      "[Iteration 5834/22400] TRAIN loss: 0.812\n",
      "[Iteration 5844/22400] TRAIN loss: 0.858\n",
      "[Iteration 5854/22400] TRAIN loss: 0.846\n",
      "[Iteration 5864/22400] TRAIN loss: 0.900\n",
      "[Iteration 5874/22400] TRAIN loss: 1.042\n",
      "[Iteration 5884/22400] TRAIN loss: 0.932\n",
      "[Iteration 5894/22400] TRAIN loss: 0.856\n",
      "[Iteration 5904/22400] TRAIN loss: 0.819\n",
      "[Iteration 5914/22400] TRAIN loss: 0.835\n",
      "[Iteration 5924/22400] TRAIN loss: 0.892\n",
      "[Iteration 5934/22400] TRAIN loss: 1.066\n",
      "[Epoch 53/200] TRAIN acc/loss: 0.800/0.721\n",
      "[Epoch 53/200] VAL   acc/loss: 0.521/1.296\n",
      "[Iteration 5946/22400] TRAIN loss: 1.039\n",
      "[Iteration 5956/22400] TRAIN loss: 0.833\n",
      "[Iteration 5966/22400] TRAIN loss: 0.911\n",
      "[Iteration 5976/22400] TRAIN loss: 0.756\n",
      "[Iteration 5986/22400] TRAIN loss: 0.931\n",
      "[Iteration 5996/22400] TRAIN loss: 0.784\n",
      "[Iteration 6006/22400] TRAIN loss: 0.889\n",
      "[Iteration 6016/22400] TRAIN loss: 0.959\n",
      "[Iteration 6026/22400] TRAIN loss: 0.996\n",
      "[Iteration 6036/22400] TRAIN loss: 1.000\n",
      "[Iteration 6046/22400] TRAIN loss: 0.878\n",
      "[Epoch 54/200] TRAIN acc/loss: 0.900/0.361\n",
      "[Epoch 54/200] VAL   acc/loss: 0.479/1.592\n",
      "[Iteration 6058/22400] TRAIN loss: 0.983\n",
      "[Iteration 6068/22400] TRAIN loss: 0.765\n",
      "[Iteration 6078/22400] TRAIN loss: 0.831\n",
      "[Iteration 6088/22400] TRAIN loss: 0.792\n",
      "[Iteration 6098/22400] TRAIN loss: 0.769\n",
      "[Iteration 6108/22400] TRAIN loss: 0.791\n",
      "[Iteration 6118/22400] TRAIN loss: 0.737\n",
      "[Iteration 6128/22400] TRAIN loss: 0.899\n",
      "[Iteration 6138/22400] TRAIN loss: 0.775\n",
      "[Iteration 6148/22400] TRAIN loss: 0.931\n",
      "[Iteration 6158/22400] TRAIN loss: 0.892\n",
      "[Epoch 55/200] TRAIN acc/loss: 0.800/0.854\n",
      "[Epoch 55/200] VAL   acc/loss: 0.514/1.428\n",
      "[Iteration 6170/22400] TRAIN loss: 0.687\n",
      "[Iteration 6180/22400] TRAIN loss: 0.975\n",
      "[Iteration 6190/22400] TRAIN loss: 0.682\n",
      "[Iteration 6200/22400] TRAIN loss: 0.823\n",
      "[Iteration 6210/22400] TRAIN loss: 0.870\n",
      "[Iteration 6220/22400] TRAIN loss: 1.026\n",
      "[Iteration 6230/22400] TRAIN loss: 0.788\n",
      "[Iteration 6240/22400] TRAIN loss: 0.809\n",
      "[Iteration 6250/22400] TRAIN loss: 0.677\n",
      "[Iteration 6260/22400] TRAIN loss: 0.704\n",
      "[Iteration 6270/22400] TRAIN loss: 0.834\n",
      "[Epoch 56/200] TRAIN acc/loss: 0.700/0.868\n",
      "[Epoch 56/200] VAL   acc/loss: 0.536/1.359\n",
      "[Iteration 6282/22400] TRAIN loss: 0.748\n",
      "[Iteration 6292/22400] TRAIN loss: 0.586\n",
      "[Iteration 6302/22400] TRAIN loss: 0.690\n",
      "[Iteration 6312/22400] TRAIN loss: 0.807\n",
      "[Iteration 6322/22400] TRAIN loss: 0.762\n",
      "[Iteration 6332/22400] TRAIN loss: 0.805\n",
      "[Iteration 6342/22400] TRAIN loss: 0.861\n",
      "[Iteration 6352/22400] TRAIN loss: 0.661\n",
      "[Iteration 6362/22400] TRAIN loss: 0.880\n",
      "[Iteration 6372/22400] TRAIN loss: 0.890\n",
      "[Iteration 6382/22400] TRAIN loss: 0.681\n",
      "[Epoch 57/200] TRAIN acc/loss: 0.900/0.525\n",
      "[Epoch 57/200] VAL   acc/loss: 0.564/1.236\n",
      "[Iteration 6394/22400] TRAIN loss: 0.725\n",
      "[Iteration 6404/22400] TRAIN loss: 0.757\n",
      "[Iteration 6414/22400] TRAIN loss: 0.750\n",
      "[Iteration 6424/22400] TRAIN loss: 0.684\n",
      "[Iteration 6434/22400] TRAIN loss: 0.742\n",
      "[Iteration 6444/22400] TRAIN loss: 0.757\n",
      "[Iteration 6454/22400] TRAIN loss: 0.711\n",
      "[Iteration 6464/22400] TRAIN loss: 1.071\n",
      "[Iteration 6474/22400] TRAIN loss: 1.009\n",
      "[Iteration 6484/22400] TRAIN loss: 0.860\n",
      "[Iteration 6494/22400] TRAIN loss: 0.824\n",
      "[Epoch 58/200] TRAIN acc/loss: 1.000/0.335\n",
      "[Epoch 58/200] VAL   acc/loss: 0.586/1.236\n",
      "[Iteration 6506/22400] TRAIN loss: 0.507\n",
      "[Iteration 6516/22400] TRAIN loss: 0.790\n",
      "[Iteration 6526/22400] TRAIN loss: 0.845\n",
      "[Iteration 6536/22400] TRAIN loss: 0.888\n",
      "[Iteration 6546/22400] TRAIN loss: 0.673\n",
      "[Iteration 6556/22400] TRAIN loss: 0.703\n",
      "[Iteration 6566/22400] TRAIN loss: 0.651\n",
      "[Iteration 6576/22400] TRAIN loss: 0.536\n",
      "[Iteration 6586/22400] TRAIN loss: 0.819\n",
      "[Iteration 6596/22400] TRAIN loss: 0.634\n",
      "[Iteration 6606/22400] TRAIN loss: 0.707\n",
      "[Epoch 59/200] TRAIN acc/loss: 0.600/0.852\n",
      "[Epoch 59/200] VAL   acc/loss: 0.564/1.258\n",
      "[Iteration 6618/22400] TRAIN loss: 0.659\n",
      "[Iteration 6628/22400] TRAIN loss: 0.638\n",
      "[Iteration 6638/22400] TRAIN loss: 0.791\n",
      "[Iteration 6648/22400] TRAIN loss: 0.652\n",
      "[Iteration 6658/22400] TRAIN loss: 0.655\n",
      "[Iteration 6668/22400] TRAIN loss: 0.726\n",
      "[Iteration 6678/22400] TRAIN loss: 0.826\n",
      "[Iteration 6688/22400] TRAIN loss: 0.472\n",
      "[Iteration 6698/22400] TRAIN loss: 0.857\n",
      "[Iteration 6708/22400] TRAIN loss: 0.756\n",
      "[Iteration 6718/22400] TRAIN loss: 0.597\n",
      "[Epoch 60/200] TRAIN acc/loss: 0.800/0.570\n",
      "[Epoch 60/200] VAL   acc/loss: 0.550/1.281\n",
      "[Iteration 6730/22400] TRAIN loss: 0.776\n",
      "[Iteration 6740/22400] TRAIN loss: 0.879\n",
      "[Iteration 6750/22400] TRAIN loss: 0.752\n",
      "[Iteration 6760/22400] TRAIN loss: 0.630\n",
      "[Iteration 6770/22400] TRAIN loss: 0.642\n",
      "[Iteration 6780/22400] TRAIN loss: 0.693\n",
      "[Iteration 6790/22400] TRAIN loss: 0.819\n",
      "[Iteration 6800/22400] TRAIN loss: 0.727\n",
      "[Iteration 6810/22400] TRAIN loss: 0.574\n",
      "[Iteration 6820/22400] TRAIN loss: 0.815\n",
      "[Iteration 6830/22400] TRAIN loss: 0.611\n",
      "[Epoch 61/200] TRAIN acc/loss: 0.800/0.512\n",
      "[Epoch 61/200] VAL   acc/loss: 0.600/1.246\n",
      "[Iteration 6842/22400] TRAIN loss: 0.699\n",
      "[Iteration 6852/22400] TRAIN loss: 0.491\n",
      "[Iteration 6862/22400] TRAIN loss: 0.619\n",
      "[Iteration 6872/22400] TRAIN loss: 0.690\n",
      "[Iteration 6882/22400] TRAIN loss: 0.684\n",
      "[Iteration 6892/22400] TRAIN loss: 0.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 6902/22400] TRAIN loss: 0.646\n",
      "[Iteration 6912/22400] TRAIN loss: 0.911\n",
      "[Iteration 6922/22400] TRAIN loss: 0.651\n",
      "[Iteration 6932/22400] TRAIN loss: 0.789\n",
      "[Iteration 6942/22400] TRAIN loss: 0.657\n",
      "[Epoch 62/200] TRAIN acc/loss: 0.800/0.534\n",
      "[Epoch 62/200] VAL   acc/loss: 0.543/1.427\n",
      "[Iteration 6954/22400] TRAIN loss: 0.679\n",
      "[Iteration 6964/22400] TRAIN loss: 0.744\n",
      "[Iteration 6974/22400] TRAIN loss: 0.743\n",
      "[Iteration 6984/22400] TRAIN loss: 0.575\n",
      "[Iteration 6994/22400] TRAIN loss: 0.608\n",
      "[Iteration 7004/22400] TRAIN loss: 0.612\n",
      "[Iteration 7014/22400] TRAIN loss: 0.623\n",
      "[Iteration 7024/22400] TRAIN loss: 0.707\n",
      "[Iteration 7034/22400] TRAIN loss: 0.787\n",
      "[Iteration 7044/22400] TRAIN loss: 0.740\n",
      "[Iteration 7054/22400] TRAIN loss: 0.794\n",
      "[Epoch 63/200] TRAIN acc/loss: 0.700/0.929\n",
      "[Epoch 63/200] VAL   acc/loss: 0.586/1.253\n",
      "[Iteration 7066/22400] TRAIN loss: 0.531\n",
      "[Iteration 7076/22400] TRAIN loss: 0.558\n",
      "[Iteration 7086/22400] TRAIN loss: 0.616\n",
      "[Iteration 7096/22400] TRAIN loss: 0.729\n",
      "[Iteration 7106/22400] TRAIN loss: 0.651\n",
      "[Iteration 7116/22400] TRAIN loss: 0.584\n",
      "[Iteration 7126/22400] TRAIN loss: 0.615\n",
      "[Iteration 7136/22400] TRAIN loss: 0.724\n",
      "[Iteration 7146/22400] TRAIN loss: 0.611\n",
      "[Iteration 7156/22400] TRAIN loss: 0.682\n",
      "[Iteration 7166/22400] TRAIN loss: 0.642\n",
      "[Epoch 64/200] TRAIN acc/loss: 0.900/0.343\n",
      "[Epoch 64/200] VAL   acc/loss: 0.614/1.075\n",
      "[Iteration 7178/22400] TRAIN loss: 0.562\n",
      "[Iteration 7188/22400] TRAIN loss: 0.486\n",
      "[Iteration 7198/22400] TRAIN loss: 0.571\n",
      "[Iteration 7208/22400] TRAIN loss: 0.468\n",
      "[Iteration 7218/22400] TRAIN loss: 0.543\n",
      "[Iteration 7228/22400] TRAIN loss: 0.548\n",
      "[Iteration 7238/22400] TRAIN loss: 0.570\n",
      "[Iteration 7248/22400] TRAIN loss: 0.652\n",
      "[Iteration 7258/22400] TRAIN loss: 0.576\n",
      "[Iteration 7268/22400] TRAIN loss: 0.695\n",
      "[Iteration 7278/22400] TRAIN loss: 0.639\n",
      "[Epoch 65/200] TRAIN acc/loss: 0.900/0.320\n",
      "[Epoch 65/200] VAL   acc/loss: 0.586/1.314\n",
      "[Iteration 7290/22400] TRAIN loss: 0.571\n",
      "[Iteration 7300/22400] TRAIN loss: 0.482\n",
      "[Iteration 7310/22400] TRAIN loss: 0.611\n",
      "[Iteration 7320/22400] TRAIN loss: 0.573\n",
      "[Iteration 7330/22400] TRAIN loss: 0.626\n",
      "[Iteration 7340/22400] TRAIN loss: 0.545\n",
      "[Iteration 7350/22400] TRAIN loss: 0.592\n",
      "[Iteration 7360/22400] TRAIN loss: 0.638\n",
      "[Iteration 7370/22400] TRAIN loss: 0.546\n",
      "[Iteration 7380/22400] TRAIN loss: 0.644\n",
      "[Iteration 7390/22400] TRAIN loss: 0.739\n",
      "[Epoch 66/200] TRAIN acc/loss: 0.600/0.974\n",
      "[Epoch 66/200] VAL   acc/loss: 0.600/1.238\n",
      "[Iteration 7402/22400] TRAIN loss: 0.641\n",
      "[Iteration 7412/22400] TRAIN loss: 0.547\n",
      "[Iteration 7422/22400] TRAIN loss: 0.443\n",
      "[Iteration 7432/22400] TRAIN loss: 0.461\n",
      "[Iteration 7442/22400] TRAIN loss: 0.592\n",
      "[Iteration 7452/22400] TRAIN loss: 0.489\n",
      "[Iteration 7462/22400] TRAIN loss: 0.448\n",
      "[Iteration 7472/22400] TRAIN loss: 0.612\n",
      "[Iteration 7482/22400] TRAIN loss: 0.613\n",
      "[Iteration 7492/22400] TRAIN loss: 0.611\n",
      "[Iteration 7502/22400] TRAIN loss: 0.664\n",
      "[Epoch 67/200] TRAIN acc/loss: 0.900/0.494\n",
      "[Epoch 67/200] VAL   acc/loss: 0.607/1.177\n",
      "[Iteration 7514/22400] TRAIN loss: 0.733\n",
      "[Iteration 7524/22400] TRAIN loss: 0.680\n",
      "[Iteration 7534/22400] TRAIN loss: 0.445\n",
      "[Iteration 7544/22400] TRAIN loss: 0.478\n",
      "[Iteration 7554/22400] TRAIN loss: 0.374\n",
      "[Iteration 7564/22400] TRAIN loss: 0.505\n",
      "[Iteration 7574/22400] TRAIN loss: 0.515\n",
      "[Iteration 7584/22400] TRAIN loss: 0.514\n",
      "[Iteration 7594/22400] TRAIN loss: 0.380\n",
      "[Iteration 7604/22400] TRAIN loss: 0.595\n",
      "[Iteration 7614/22400] TRAIN loss: 0.680\n",
      "[Epoch 68/200] TRAIN acc/loss: 0.800/0.445\n",
      "[Epoch 68/200] VAL   acc/loss: 0.607/1.319\n",
      "[Iteration 7626/22400] TRAIN loss: 0.348\n",
      "[Iteration 7636/22400] TRAIN loss: 0.469\n",
      "[Iteration 7646/22400] TRAIN loss: 0.475\n",
      "[Iteration 7656/22400] TRAIN loss: 0.537\n",
      "[Iteration 7666/22400] TRAIN loss: 0.482\n",
      "[Iteration 7676/22400] TRAIN loss: 0.610\n",
      "[Iteration 7686/22400] TRAIN loss: 0.381\n",
      "[Iteration 7696/22400] TRAIN loss: 0.522\n",
      "[Iteration 7706/22400] TRAIN loss: 0.546\n",
      "[Iteration 7716/22400] TRAIN loss: 0.481\n",
      "[Iteration 7726/22400] TRAIN loss: 0.475\n",
      "[Epoch 69/200] TRAIN acc/loss: 0.900/0.361\n",
      "[Epoch 69/200] VAL   acc/loss: 0.657/1.084\n",
      "[Iteration 7738/22400] TRAIN loss: 0.460\n",
      "[Iteration 7748/22400] TRAIN loss: 0.354\n",
      "[Iteration 7758/22400] TRAIN loss: 0.608\n",
      "[Iteration 7768/22400] TRAIN loss: 0.598\n",
      "[Iteration 7778/22400] TRAIN loss: 0.519\n",
      "[Iteration 7788/22400] TRAIN loss: 0.435\n",
      "[Iteration 7798/22400] TRAIN loss: 0.604\n",
      "[Iteration 7808/22400] TRAIN loss: 0.518\n",
      "[Iteration 7818/22400] TRAIN loss: 0.467\n",
      "[Iteration 7828/22400] TRAIN loss: 0.458\n",
      "[Iteration 7838/22400] TRAIN loss: 0.419\n",
      "[Epoch 70/200] TRAIN acc/loss: 0.800/0.641\n",
      "[Epoch 70/200] VAL   acc/loss: 0.671/1.071\n",
      "[Iteration 7850/22400] TRAIN loss: 0.338\n",
      "[Iteration 7860/22400] TRAIN loss: 0.327\n",
      "[Iteration 7870/22400] TRAIN loss: 0.485\n",
      "[Iteration 7880/22400] TRAIN loss: 0.439\n",
      "[Iteration 7890/22400] TRAIN loss: 0.484\n",
      "[Iteration 7900/22400] TRAIN loss: 0.589\n",
      "[Iteration 7910/22400] TRAIN loss: 0.574\n",
      "[Iteration 7920/22400] TRAIN loss: 0.356\n",
      "[Iteration 7930/22400] TRAIN loss: 0.604\n",
      "[Iteration 7940/22400] TRAIN loss: 0.793\n",
      "[Iteration 7950/22400] TRAIN loss: 0.462\n",
      "[Epoch 71/200] TRAIN acc/loss: 0.600/0.945\n",
      "[Epoch 71/200] VAL   acc/loss: 0.600/1.177\n",
      "[Iteration 7962/22400] TRAIN loss: 0.377\n",
      "[Iteration 7972/22400] TRAIN loss: 0.442\n",
      "[Iteration 7982/22400] TRAIN loss: 0.468\n",
      "[Iteration 7992/22400] TRAIN loss: 0.341\n",
      "[Iteration 8002/22400] TRAIN loss: 0.473\n",
      "[Iteration 8012/22400] TRAIN loss: 0.465\n",
      "[Iteration 8022/22400] TRAIN loss: 0.620\n",
      "[Iteration 8032/22400] TRAIN loss: 0.627\n",
      "[Iteration 8042/22400] TRAIN loss: 0.503\n",
      "[Iteration 8052/22400] TRAIN loss: 0.464\n",
      "[Iteration 8062/22400] TRAIN loss: 0.359\n",
      "[Epoch 72/200] TRAIN acc/loss: 0.700/0.531\n",
      "[Epoch 72/200] VAL   acc/loss: 0.650/1.044\n",
      "[Iteration 8074/22400] TRAIN loss: 0.453\n",
      "[Iteration 8084/22400] TRAIN loss: 0.375\n",
      "[Iteration 8094/22400] TRAIN loss: 0.534\n",
      "[Iteration 8104/22400] TRAIN loss: 0.312\n",
      "[Iteration 8114/22400] TRAIN loss: 0.480\n",
      "[Iteration 8124/22400] TRAIN loss: 0.395\n",
      "[Iteration 8134/22400] TRAIN loss: 0.536\n",
      "[Iteration 8144/22400] TRAIN loss: 0.575\n",
      "[Iteration 8154/22400] TRAIN loss: 0.407\n",
      "[Iteration 8164/22400] TRAIN loss: 0.369\n",
      "[Iteration 8174/22400] TRAIN loss: 0.380\n",
      "[Epoch 73/200] TRAIN acc/loss: 0.900/0.278\n",
      "[Epoch 73/200] VAL   acc/loss: 0.636/1.175\n",
      "[Iteration 8186/22400] TRAIN loss: 0.418\n",
      "[Iteration 8196/22400] TRAIN loss: 0.335\n",
      "[Iteration 8206/22400] TRAIN loss: 0.328\n",
      "[Iteration 8216/22400] TRAIN loss: 0.462\n",
      "[Iteration 8226/22400] TRAIN loss: 0.355\n",
      "[Iteration 8236/22400] TRAIN loss: 0.396\n",
      "[Iteration 8246/22400] TRAIN loss: 0.322\n",
      "[Iteration 8256/22400] TRAIN loss: 0.469\n",
      "[Iteration 8266/22400] TRAIN loss: 0.427\n",
      "[Iteration 8276/22400] TRAIN loss: 0.467\n",
      "[Iteration 8286/22400] TRAIN loss: 0.477\n",
      "[Epoch 74/200] TRAIN acc/loss: 0.800/0.489\n",
      "[Epoch 74/200] VAL   acc/loss: 0.621/1.269\n",
      "[Iteration 8298/22400] TRAIN loss: 0.390\n",
      "[Iteration 8308/22400] TRAIN loss: 0.306\n",
      "[Iteration 8318/22400] TRAIN loss: 0.674\n",
      "[Iteration 8328/22400] TRAIN loss: 0.373\n",
      "[Iteration 8338/22400] TRAIN loss: 0.648\n",
      "[Iteration 8348/22400] TRAIN loss: 0.472\n",
      "[Iteration 8358/22400] TRAIN loss: 0.346\n",
      "[Iteration 8368/22400] TRAIN loss: 0.326\n",
      "[Iteration 8378/22400] TRAIN loss: 0.476\n",
      "[Iteration 8388/22400] TRAIN loss: 0.332\n",
      "[Iteration 8398/22400] TRAIN loss: 0.467\n",
      "[Epoch 75/200] TRAIN acc/loss: 0.800/0.328\n",
      "[Epoch 75/200] VAL   acc/loss: 0.643/1.218\n",
      "[Iteration 8410/22400] TRAIN loss: 0.253\n",
      "[Iteration 8420/22400] TRAIN loss: 0.322\n",
      "[Iteration 8430/22400] TRAIN loss: 0.355\n",
      "[Iteration 8440/22400] TRAIN loss: 0.294\n",
      "[Iteration 8450/22400] TRAIN loss: 0.394\n",
      "[Iteration 8460/22400] TRAIN loss: 0.521\n",
      "[Iteration 8470/22400] TRAIN loss: 0.459\n",
      "[Iteration 8480/22400] TRAIN loss: 0.301\n",
      "[Iteration 8490/22400] TRAIN loss: 0.424\n",
      "[Iteration 8500/22400] TRAIN loss: 0.507\n",
      "[Iteration 8510/22400] TRAIN loss: 0.398\n",
      "[Epoch 76/200] TRAIN acc/loss: 0.900/0.313\n",
      "[Epoch 76/200] VAL   acc/loss: 0.671/1.194\n",
      "[Iteration 8522/22400] TRAIN loss: 0.347\n",
      "[Iteration 8532/22400] TRAIN loss: 0.461\n",
      "[Iteration 8542/22400] TRAIN loss: 0.350\n",
      "[Iteration 8552/22400] TRAIN loss: 0.487\n",
      "[Iteration 8562/22400] TRAIN loss: 0.293\n",
      "[Iteration 8572/22400] TRAIN loss: 0.349\n",
      "[Iteration 8582/22400] TRAIN loss: 0.372\n",
      "[Iteration 8592/22400] TRAIN loss: 0.445\n",
      "[Iteration 8602/22400] TRAIN loss: 0.393\n",
      "[Iteration 8612/22400] TRAIN loss: 0.420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 8622/22400] TRAIN loss: 0.417\n",
      "[Epoch 77/200] TRAIN acc/loss: 0.500/1.170\n",
      "[Epoch 77/200] VAL   acc/loss: 0.686/1.189\n",
      "[Iteration 8634/22400] TRAIN loss: 0.511\n",
      "[Iteration 8644/22400] TRAIN loss: 0.408\n",
      "[Iteration 8654/22400] TRAIN loss: 0.629\n",
      "[Iteration 8664/22400] TRAIN loss: 0.327\n",
      "[Iteration 8674/22400] TRAIN loss: 0.283\n",
      "[Iteration 8684/22400] TRAIN loss: 0.537\n",
      "[Iteration 8694/22400] TRAIN loss: 0.437\n",
      "[Iteration 8704/22400] TRAIN loss: 0.359\n",
      "[Iteration 8714/22400] TRAIN loss: 0.441\n",
      "[Iteration 8724/22400] TRAIN loss: 0.381\n",
      "[Iteration 8734/22400] TRAIN loss: 0.402\n",
      "[Epoch 78/200] TRAIN acc/loss: 0.800/0.393\n",
      "[Epoch 78/200] VAL   acc/loss: 0.686/1.095\n",
      "[Iteration 8746/22400] TRAIN loss: 0.261\n",
      "[Iteration 8756/22400] TRAIN loss: 0.268\n",
      "[Iteration 8766/22400] TRAIN loss: 0.272\n",
      "[Iteration 8776/22400] TRAIN loss: 0.387\n",
      "[Iteration 8786/22400] TRAIN loss: 0.360\n",
      "[Iteration 8796/22400] TRAIN loss: 0.402\n",
      "[Iteration 8806/22400] TRAIN loss: 0.399\n",
      "[Iteration 8816/22400] TRAIN loss: 0.448\n",
      "[Iteration 8826/22400] TRAIN loss: 0.189\n",
      "[Iteration 8836/22400] TRAIN loss: 0.435\n",
      "[Iteration 8846/22400] TRAIN loss: 0.294\n",
      "[Epoch 79/200] TRAIN acc/loss: 1.000/0.179\n",
      "[Epoch 79/200] VAL   acc/loss: 0.657/1.234\n",
      "[Iteration 8858/22400] TRAIN loss: 0.356\n",
      "[Iteration 8868/22400] TRAIN loss: 0.358\n",
      "[Iteration 8878/22400] TRAIN loss: 0.309\n",
      "[Iteration 8888/22400] TRAIN loss: 0.403\n",
      "[Iteration 8898/22400] TRAIN loss: 0.329\n",
      "[Iteration 8908/22400] TRAIN loss: 0.252\n",
      "[Iteration 8918/22400] TRAIN loss: 0.338\n",
      "[Iteration 8928/22400] TRAIN loss: 0.473\n",
      "[Iteration 8938/22400] TRAIN loss: 0.477\n",
      "[Iteration 8948/22400] TRAIN loss: 0.344\n",
      "[Iteration 8958/22400] TRAIN loss: 0.250\n",
      "[Epoch 80/200] TRAIN acc/loss: 0.800/0.646\n",
      "[Epoch 80/200] VAL   acc/loss: 0.621/1.254\n",
      "[Iteration 8970/22400] TRAIN loss: 0.242\n",
      "[Iteration 8980/22400] TRAIN loss: 0.237\n",
      "[Iteration 8990/22400] TRAIN loss: 0.300\n",
      "[Iteration 9000/22400] TRAIN loss: 0.356\n",
      "[Iteration 9010/22400] TRAIN loss: 0.245\n",
      "[Iteration 9020/22400] TRAIN loss: 0.322\n",
      "[Iteration 9030/22400] TRAIN loss: 0.330\n",
      "[Iteration 9040/22400] TRAIN loss: 0.457\n",
      "[Iteration 9050/22400] TRAIN loss: 0.359\n",
      "[Iteration 9060/22400] TRAIN loss: 0.279\n",
      "[Iteration 9070/22400] TRAIN loss: 0.324\n",
      "[Epoch 81/200] TRAIN acc/loss: 0.900/0.348\n",
      "[Epoch 81/200] VAL   acc/loss: 0.707/1.070\n",
      "[Iteration 9082/22400] TRAIN loss: 0.516\n",
      "[Iteration 9092/22400] TRAIN loss: 0.352\n",
      "[Iteration 9102/22400] TRAIN loss: 0.383\n",
      "[Iteration 9112/22400] TRAIN loss: 0.351\n",
      "[Iteration 9122/22400] TRAIN loss: 0.276\n",
      "[Iteration 9132/22400] TRAIN loss: 0.342\n",
      "[Iteration 9142/22400] TRAIN loss: 0.306\n",
      "[Iteration 9152/22400] TRAIN loss: 0.487\n",
      "[Iteration 9162/22400] TRAIN loss: 0.312\n",
      "[Iteration 9172/22400] TRAIN loss: 0.333\n",
      "[Iteration 9182/22400] TRAIN loss: 0.227\n",
      "[Epoch 82/200] TRAIN acc/loss: 0.900/0.247\n",
      "[Epoch 82/200] VAL   acc/loss: 0.679/1.012\n",
      "[Iteration 9194/22400] TRAIN loss: 0.274\n",
      "[Iteration 9204/22400] TRAIN loss: 0.268\n",
      "[Iteration 9214/22400] TRAIN loss: 0.245\n",
      "[Iteration 9224/22400] TRAIN loss: 0.225\n",
      "[Iteration 9234/22400] TRAIN loss: 0.432\n",
      "[Iteration 9244/22400] TRAIN loss: 0.322\n",
      "[Iteration 9254/22400] TRAIN loss: 0.342\n",
      "[Iteration 9264/22400] TRAIN loss: 0.254\n",
      "[Iteration 9274/22400] TRAIN loss: 0.256\n",
      "[Iteration 9284/22400] TRAIN loss: 0.459\n",
      "[Iteration 9294/22400] TRAIN loss: 0.390\n",
      "[Epoch 83/200] TRAIN acc/loss: 1.000/0.085\n",
      "[Epoch 83/200] VAL   acc/loss: 0.693/1.050\n",
      "[Iteration 9306/22400] TRAIN loss: 0.241\n",
      "[Iteration 9316/22400] TRAIN loss: 0.307\n",
      "[Iteration 9326/22400] TRAIN loss: 0.268\n",
      "[Iteration 9336/22400] TRAIN loss: 0.147\n",
      "[Iteration 9346/22400] TRAIN loss: 0.250\n",
      "[Iteration 9356/22400] TRAIN loss: 0.277\n",
      "[Iteration 9366/22400] TRAIN loss: 0.356\n",
      "[Iteration 9376/22400] TRAIN loss: 0.323\n",
      "[Iteration 9386/22400] TRAIN loss: 0.314\n",
      "[Iteration 9396/22400] TRAIN loss: 0.288\n",
      "[Iteration 9406/22400] TRAIN loss: 0.382\n",
      "[Epoch 84/200] TRAIN acc/loss: 1.000/0.084\n",
      "[Epoch 84/200] VAL   acc/loss: 0.721/0.971\n",
      "[Iteration 9418/22400] TRAIN loss: 0.250\n",
      "[Iteration 9428/22400] TRAIN loss: 0.195\n",
      "[Iteration 9438/22400] TRAIN loss: 0.173\n",
      "[Iteration 9448/22400] TRAIN loss: 0.304\n",
      "[Iteration 9458/22400] TRAIN loss: 0.298\n",
      "[Iteration 9468/22400] TRAIN loss: 0.353\n",
      "[Iteration 9478/22400] TRAIN loss: 0.317\n",
      "[Iteration 9488/22400] TRAIN loss: 0.218\n",
      "[Iteration 9498/22400] TRAIN loss: 0.284\n",
      "[Iteration 9508/22400] TRAIN loss: 0.239\n",
      "[Iteration 9518/22400] TRAIN loss: 0.338\n",
      "[Epoch 85/200] TRAIN acc/loss: 0.900/0.164\n",
      "[Epoch 85/200] VAL   acc/loss: 0.679/1.071\n",
      "[Iteration 9530/22400] TRAIN loss: 0.235\n",
      "[Iteration 9540/22400] TRAIN loss: 0.241\n",
      "[Iteration 9550/22400] TRAIN loss: 0.313\n",
      "[Iteration 9560/22400] TRAIN loss: 0.234\n",
      "[Iteration 9570/22400] TRAIN loss: 0.310\n",
      "[Iteration 9580/22400] TRAIN loss: 0.326\n",
      "[Iteration 9590/22400] TRAIN loss: 0.367\n",
      "[Iteration 9600/22400] TRAIN loss: 0.329\n",
      "[Iteration 9610/22400] TRAIN loss: 0.435\n",
      "[Iteration 9620/22400] TRAIN loss: 0.333\n",
      "[Iteration 9630/22400] TRAIN loss: 0.297\n",
      "[Epoch 86/200] TRAIN acc/loss: 0.800/0.517\n",
      "[Epoch 86/200] VAL   acc/loss: 0.671/1.245\n",
      "[Iteration 9642/22400] TRAIN loss: 0.264\n",
      "[Iteration 9652/22400] TRAIN loss: 0.302\n",
      "[Iteration 9662/22400] TRAIN loss: 0.297\n",
      "[Iteration 9672/22400] TRAIN loss: 0.203\n",
      "[Iteration 9682/22400] TRAIN loss: 0.340\n",
      "[Iteration 9692/22400] TRAIN loss: 0.221\n",
      "[Iteration 9702/22400] TRAIN loss: 0.376\n",
      "[Iteration 9712/22400] TRAIN loss: 0.247\n",
      "[Iteration 9722/22400] TRAIN loss: 0.270\n",
      "[Iteration 9732/22400] TRAIN loss: 0.403\n",
      "[Iteration 9742/22400] TRAIN loss: 0.350\n",
      "[Epoch 87/200] TRAIN acc/loss: 0.900/0.206\n",
      "[Epoch 87/200] VAL   acc/loss: 0.679/1.108\n",
      "[Iteration 9754/22400] TRAIN loss: 0.285\n",
      "[Iteration 9764/22400] TRAIN loss: 0.304\n",
      "[Iteration 9774/22400] TRAIN loss: 0.327\n",
      "[Iteration 9784/22400] TRAIN loss: 0.256\n",
      "[Iteration 9794/22400] TRAIN loss: 0.485\n",
      "[Iteration 9804/22400] TRAIN loss: 0.269\n",
      "[Iteration 9814/22400] TRAIN loss: 0.320\n",
      "[Iteration 9824/22400] TRAIN loss: 0.181\n",
      "[Iteration 9834/22400] TRAIN loss: 0.402\n",
      "[Iteration 9844/22400] TRAIN loss: 0.310\n",
      "[Iteration 9854/22400] TRAIN loss: 0.354\n",
      "[Epoch 88/200] TRAIN acc/loss: 0.900/0.448\n",
      "[Epoch 88/200] VAL   acc/loss: 0.657/1.253\n",
      "[Iteration 9866/22400] TRAIN loss: 0.280\n",
      "[Iteration 9876/22400] TRAIN loss: 0.376\n",
      "[Iteration 9886/22400] TRAIN loss: 0.203\n",
      "[Iteration 9896/22400] TRAIN loss: 0.427\n",
      "[Iteration 9906/22400] TRAIN loss: 0.234\n",
      "[Iteration 9916/22400] TRAIN loss: 0.284\n",
      "[Iteration 9926/22400] TRAIN loss: 0.300\n",
      "[Iteration 9936/22400] TRAIN loss: 0.248\n",
      "[Iteration 9946/22400] TRAIN loss: 0.268\n",
      "[Iteration 9956/22400] TRAIN loss: 0.382\n",
      "[Iteration 9966/22400] TRAIN loss: 0.314\n",
      "[Epoch 89/200] TRAIN acc/loss: 0.900/0.242\n",
      "[Epoch 89/200] VAL   acc/loss: 0.736/0.968\n",
      "[Iteration 9978/22400] TRAIN loss: 0.373\n",
      "[Iteration 9988/22400] TRAIN loss: 0.240\n",
      "[Iteration 9998/22400] TRAIN loss: 0.336\n",
      "[Iteration 10008/22400] TRAIN loss: 0.316\n",
      "[Iteration 10018/22400] TRAIN loss: 0.351\n",
      "[Iteration 10028/22400] TRAIN loss: 0.182\n",
      "[Iteration 10038/22400] TRAIN loss: 0.176\n",
      "[Iteration 10048/22400] TRAIN loss: 0.337\n",
      "[Iteration 10058/22400] TRAIN loss: 0.219\n",
      "[Iteration 10068/22400] TRAIN loss: 0.240\n",
      "[Iteration 10078/22400] TRAIN loss: 0.523\n",
      "[Epoch 90/200] TRAIN acc/loss: 0.900/0.257\n",
      "[Epoch 90/200] VAL   acc/loss: 0.693/1.145\n",
      "[Iteration 10090/22400] TRAIN loss: 0.219\n",
      "[Iteration 10100/22400] TRAIN loss: 0.145\n",
      "[Iteration 10110/22400] TRAIN loss: 0.218\n",
      "[Iteration 10120/22400] TRAIN loss: 0.251\n",
      "[Iteration 10130/22400] TRAIN loss: 0.236\n",
      "[Iteration 10140/22400] TRAIN loss: 0.198\n",
      "[Iteration 10150/22400] TRAIN loss: 0.476\n",
      "[Iteration 10160/22400] TRAIN loss: 0.140\n",
      "[Iteration 10170/22400] TRAIN loss: 0.249\n",
      "[Iteration 10180/22400] TRAIN loss: 0.329\n",
      "[Iteration 10190/22400] TRAIN loss: 0.220\n",
      "[Epoch 91/200] TRAIN acc/loss: 0.900/0.472\n",
      "[Epoch 91/200] VAL   acc/loss: 0.693/1.191\n",
      "[Iteration 10202/22400] TRAIN loss: 0.210\n",
      "[Iteration 10212/22400] TRAIN loss: 0.284\n",
      "[Iteration 10222/22400] TRAIN loss: 0.406\n",
      "[Iteration 10232/22400] TRAIN loss: 0.342\n",
      "[Iteration 10242/22400] TRAIN loss: 0.312\n",
      "[Iteration 10252/22400] TRAIN loss: 0.251\n",
      "[Iteration 10262/22400] TRAIN loss: 0.255\n",
      "[Iteration 10272/22400] TRAIN loss: 0.244\n",
      "[Iteration 10282/22400] TRAIN loss: 0.255\n",
      "[Iteration 10292/22400] TRAIN loss: 0.177\n",
      "[Iteration 10302/22400] TRAIN loss: 0.354\n",
      "[Epoch 92/200] TRAIN acc/loss: 1.000/0.137\n",
      "[Epoch 92/200] VAL   acc/loss: 0.657/1.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 10314/22400] TRAIN loss: 0.155\n",
      "[Iteration 10324/22400] TRAIN loss: 0.264\n",
      "[Iteration 10334/22400] TRAIN loss: 0.337\n",
      "[Iteration 10344/22400] TRAIN loss: 0.274\n",
      "[Iteration 10354/22400] TRAIN loss: 0.324\n",
      "[Iteration 10364/22400] TRAIN loss: 0.209\n",
      "[Iteration 10374/22400] TRAIN loss: 0.375\n",
      "[Iteration 10384/22400] TRAIN loss: 0.177\n",
      "[Iteration 10394/22400] TRAIN loss: 0.307\n",
      "[Iteration 10404/22400] TRAIN loss: 0.312\n",
      "[Iteration 10414/22400] TRAIN loss: 0.152\n",
      "[Epoch 93/200] TRAIN acc/loss: 0.900/0.572\n",
      "[Epoch 93/200] VAL   acc/loss: 0.636/1.176\n",
      "[Iteration 10426/22400] TRAIN loss: 0.329\n",
      "[Iteration 10436/22400] TRAIN loss: 0.215\n",
      "[Iteration 10446/22400] TRAIN loss: 0.339\n",
      "[Iteration 10456/22400] TRAIN loss: 0.192\n",
      "[Iteration 10466/22400] TRAIN loss: 0.363\n",
      "[Iteration 10476/22400] TRAIN loss: 0.231\n",
      "[Iteration 10486/22400] TRAIN loss: 0.202\n",
      "[Iteration 10496/22400] TRAIN loss: 0.295\n",
      "[Iteration 10506/22400] TRAIN loss: 0.261\n",
      "[Iteration 10516/22400] TRAIN loss: 0.172\n",
      "[Iteration 10526/22400] TRAIN loss: 0.120\n",
      "[Epoch 94/200] TRAIN acc/loss: 0.900/0.237\n",
      "[Epoch 94/200] VAL   acc/loss: 0.714/0.900\n",
      "[Iteration 10538/22400] TRAIN loss: 0.210\n",
      "[Iteration 10548/22400] TRAIN loss: 0.134\n",
      "[Iteration 10558/22400] TRAIN loss: 0.181\n",
      "[Iteration 10568/22400] TRAIN loss: 0.147\n",
      "[Iteration 10578/22400] TRAIN loss: 0.237\n",
      "[Iteration 10588/22400] TRAIN loss: 0.165\n",
      "[Iteration 10598/22400] TRAIN loss: 0.254\n",
      "[Iteration 10608/22400] TRAIN loss: 0.385\n",
      "[Iteration 10618/22400] TRAIN loss: 0.207\n",
      "[Iteration 10628/22400] TRAIN loss: 0.267\n",
      "[Iteration 10638/22400] TRAIN loss: 0.251\n",
      "[Epoch 95/200] TRAIN acc/loss: 1.000/0.136\n",
      "[Epoch 95/200] VAL   acc/loss: 0.764/0.928\n",
      "[Iteration 10650/22400] TRAIN loss: 0.224\n",
      "[Iteration 10660/22400] TRAIN loss: 0.159\n",
      "[Iteration 10670/22400] TRAIN loss: 0.294\n",
      "[Iteration 10680/22400] TRAIN loss: 0.255\n",
      "[Iteration 10690/22400] TRAIN loss: 0.262\n",
      "[Iteration 10700/22400] TRAIN loss: 0.315\n",
      "[Iteration 10710/22400] TRAIN loss: 0.209\n",
      "[Iteration 10720/22400] TRAIN loss: 0.318\n",
      "[Iteration 10730/22400] TRAIN loss: 0.264\n",
      "[Iteration 10740/22400] TRAIN loss: 0.146\n",
      "[Iteration 10750/22400] TRAIN loss: 0.236\n",
      "[Epoch 96/200] TRAIN acc/loss: 1.000/0.035\n",
      "[Epoch 96/200] VAL   acc/loss: 0.700/1.001\n",
      "[Iteration 10762/22400] TRAIN loss: 0.224\n",
      "[Iteration 10772/22400] TRAIN loss: 0.275\n",
      "[Iteration 10782/22400] TRAIN loss: 0.148\n",
      "[Iteration 10792/22400] TRAIN loss: 0.250\n",
      "[Iteration 10802/22400] TRAIN loss: 0.195\n",
      "[Iteration 10812/22400] TRAIN loss: 0.299\n",
      "[Iteration 10822/22400] TRAIN loss: 0.236\n",
      "[Iteration 10832/22400] TRAIN loss: 0.311\n",
      "[Iteration 10842/22400] TRAIN loss: 0.206\n",
      "[Iteration 10852/22400] TRAIN loss: 0.222\n",
      "[Iteration 10862/22400] TRAIN loss: 0.408\n",
      "[Epoch 97/200] TRAIN acc/loss: 0.800/0.677\n",
      "[Epoch 97/200] VAL   acc/loss: 0.736/1.047\n",
      "[Iteration 10874/22400] TRAIN loss: 0.269\n",
      "[Iteration 10884/22400] TRAIN loss: 0.190\n",
      "[Iteration 10894/22400] TRAIN loss: 0.318\n",
      "[Iteration 10904/22400] TRAIN loss: 0.231\n",
      "[Iteration 10914/22400] TRAIN loss: 0.327\n",
      "[Iteration 10924/22400] TRAIN loss: 0.245\n",
      "[Iteration 10934/22400] TRAIN loss: 0.327\n",
      "[Iteration 10944/22400] TRAIN loss: 0.464\n",
      "[Iteration 10954/22400] TRAIN loss: 0.296\n",
      "[Iteration 10964/22400] TRAIN loss: 0.203\n",
      "[Iteration 10974/22400] TRAIN loss: 0.321\n",
      "[Epoch 98/200] TRAIN acc/loss: 1.000/0.098\n",
      "[Epoch 98/200] VAL   acc/loss: 0.736/1.077\n",
      "[Iteration 10986/22400] TRAIN loss: 0.241\n",
      "[Iteration 10996/22400] TRAIN loss: 0.247\n",
      "[Iteration 11006/22400] TRAIN loss: 0.220\n",
      "[Iteration 11016/22400] TRAIN loss: 0.203\n",
      "[Iteration 11026/22400] TRAIN loss: 0.241\n",
      "[Iteration 11036/22400] TRAIN loss: 0.244\n",
      "[Iteration 11046/22400] TRAIN loss: 0.318\n",
      "[Iteration 11056/22400] TRAIN loss: 0.128\n",
      "[Iteration 11066/22400] TRAIN loss: 0.150\n",
      "[Iteration 11076/22400] TRAIN loss: 0.153\n",
      "[Iteration 11086/22400] TRAIN loss: 0.213\n",
      "[Epoch 99/200] TRAIN acc/loss: 1.000/0.063\n",
      "[Epoch 99/200] VAL   acc/loss: 0.707/1.145\n",
      "[Iteration 11098/22400] TRAIN loss: 0.151\n",
      "[Iteration 11108/22400] TRAIN loss: 0.339\n",
      "[Iteration 11118/22400] TRAIN loss: 0.247\n",
      "[Iteration 11128/22400] TRAIN loss: 0.161\n",
      "[Iteration 11138/22400] TRAIN loss: 0.265\n",
      "[Iteration 11148/22400] TRAIN loss: 0.203\n",
      "[Iteration 11158/22400] TRAIN loss: 0.158\n",
      "[Iteration 11168/22400] TRAIN loss: 0.157\n",
      "[Iteration 11178/22400] TRAIN loss: 0.252\n",
      "[Iteration 11188/22400] TRAIN loss: 0.266\n",
      "[Iteration 11198/22400] TRAIN loss: 0.245\n",
      "[Epoch 100/200] TRAIN acc/loss: 1.000/0.027\n",
      "[Epoch 100/200] VAL   acc/loss: 0.629/1.280\n",
      "[Iteration 11210/22400] TRAIN loss: 0.453\n",
      "[Iteration 11220/22400] TRAIN loss: 0.444\n",
      "[Iteration 11230/22400] TRAIN loss: 0.258\n",
      "[Iteration 11240/22400] TRAIN loss: 0.186\n",
      "[Iteration 11250/22400] TRAIN loss: 0.282\n",
      "[Iteration 11260/22400] TRAIN loss: 0.150\n",
      "[Iteration 11270/22400] TRAIN loss: 0.271\n",
      "[Iteration 11280/22400] TRAIN loss: 0.167\n",
      "[Iteration 11290/22400] TRAIN loss: 0.209\n",
      "[Iteration 11300/22400] TRAIN loss: 0.261\n",
      "[Iteration 11310/22400] TRAIN loss: 0.223\n",
      "[Epoch 101/200] TRAIN acc/loss: 1.000/0.210\n",
      "[Epoch 101/200] VAL   acc/loss: 0.707/1.003\n",
      "[Iteration 11322/22400] TRAIN loss: 0.196\n",
      "[Iteration 11332/22400] TRAIN loss: 0.100\n",
      "[Iteration 11342/22400] TRAIN loss: 0.151\n",
      "[Iteration 11352/22400] TRAIN loss: 0.140\n",
      "[Iteration 11362/22400] TRAIN loss: 0.256\n",
      "[Iteration 11372/22400] TRAIN loss: 0.270\n",
      "[Iteration 11382/22400] TRAIN loss: 0.274\n",
      "[Iteration 11392/22400] TRAIN loss: 0.313\n",
      "[Iteration 11402/22400] TRAIN loss: 0.204\n",
      "[Iteration 11412/22400] TRAIN loss: 0.348\n",
      "[Iteration 11422/22400] TRAIN loss: 0.150\n",
      "[Epoch 102/200] TRAIN acc/loss: 1.000/0.228\n",
      "[Epoch 102/200] VAL   acc/loss: 0.714/1.045\n",
      "[Iteration 11434/22400] TRAIN loss: 0.161\n",
      "[Iteration 11444/22400] TRAIN loss: 0.129\n",
      "[Iteration 11454/22400] TRAIN loss: 0.283\n",
      "[Iteration 11464/22400] TRAIN loss: 0.313\n",
      "[Iteration 11474/22400] TRAIN loss: 0.160\n",
      "[Iteration 11484/22400] TRAIN loss: 0.338\n",
      "[Iteration 11494/22400] TRAIN loss: 0.353\n",
      "[Iteration 11504/22400] TRAIN loss: 0.172\n",
      "[Iteration 11514/22400] TRAIN loss: 0.172\n",
      "[Iteration 11524/22400] TRAIN loss: 0.389\n",
      "[Iteration 11534/22400] TRAIN loss: 0.302\n",
      "[Epoch 103/200] TRAIN acc/loss: 1.000/0.035\n",
      "[Epoch 103/200] VAL   acc/loss: 0.679/1.126\n",
      "[Iteration 11546/22400] TRAIN loss: 0.330\n",
      "[Iteration 11556/22400] TRAIN loss: 0.217\n",
      "[Iteration 11566/22400] TRAIN loss: 0.247\n",
      "[Iteration 11576/22400] TRAIN loss: 0.108\n",
      "[Iteration 11586/22400] TRAIN loss: 0.164\n",
      "[Iteration 11596/22400] TRAIN loss: 0.116\n",
      "[Iteration 11606/22400] TRAIN loss: 0.241\n",
      "[Iteration 11616/22400] TRAIN loss: 0.246\n",
      "[Iteration 11626/22400] TRAIN loss: 0.181\n",
      "[Iteration 11636/22400] TRAIN loss: 0.224\n",
      "[Iteration 11646/22400] TRAIN loss: 0.149\n",
      "[Epoch 104/200] TRAIN acc/loss: 0.900/0.241\n",
      "[Epoch 104/200] VAL   acc/loss: 0.686/1.095\n",
      "[Iteration 11658/22400] TRAIN loss: 0.191\n",
      "[Iteration 11668/22400] TRAIN loss: 0.219\n",
      "[Iteration 11678/22400] TRAIN loss: 0.151\n",
      "[Iteration 11688/22400] TRAIN loss: 0.177\n",
      "[Iteration 11698/22400] TRAIN loss: 0.138\n",
      "[Iteration 11708/22400] TRAIN loss: 0.191\n",
      "[Iteration 11718/22400] TRAIN loss: 0.150\n",
      "[Iteration 11728/22400] TRAIN loss: 0.112\n",
      "[Iteration 11738/22400] TRAIN loss: 0.327\n",
      "[Iteration 11748/22400] TRAIN loss: 0.255\n",
      "[Iteration 11758/22400] TRAIN loss: 0.232\n",
      "[Epoch 105/200] TRAIN acc/loss: 0.800/0.304\n",
      "[Epoch 105/200] VAL   acc/loss: 0.686/1.234\n",
      "[Iteration 11770/22400] TRAIN loss: 0.187\n",
      "[Iteration 11780/22400] TRAIN loss: 0.091\n",
      "[Iteration 11790/22400] TRAIN loss: 0.148\n",
      "[Iteration 11800/22400] TRAIN loss: 0.277\n",
      "[Iteration 11810/22400] TRAIN loss: 0.208\n",
      "[Iteration 11820/22400] TRAIN loss: 0.250\n",
      "[Iteration 11830/22400] TRAIN loss: 0.200\n",
      "[Iteration 11840/22400] TRAIN loss: 0.181\n",
      "[Iteration 11850/22400] TRAIN loss: 0.144\n",
      "[Iteration 11860/22400] TRAIN loss: 0.192\n",
      "[Iteration 11870/22400] TRAIN loss: 0.156\n",
      "[Epoch 106/200] TRAIN acc/loss: 1.000/0.125\n",
      "[Epoch 106/200] VAL   acc/loss: 0.750/1.081\n",
      "[Iteration 11882/22400] TRAIN loss: 0.144\n",
      "[Iteration 11892/22400] TRAIN loss: 0.194\n",
      "[Iteration 11902/22400] TRAIN loss: 0.201\n",
      "[Iteration 11912/22400] TRAIN loss: 0.266\n",
      "[Iteration 11922/22400] TRAIN loss: 0.187\n",
      "[Iteration 11932/22400] TRAIN loss: 0.133\n",
      "[Iteration 11942/22400] TRAIN loss: 0.210\n",
      "[Iteration 11952/22400] TRAIN loss: 0.111\n",
      "[Iteration 11962/22400] TRAIN loss: 0.091\n",
      "[Iteration 11972/22400] TRAIN loss: 0.155\n",
      "[Iteration 11982/22400] TRAIN loss: 0.134\n",
      "[Epoch 107/200] TRAIN acc/loss: 1.000/0.097\n",
      "[Epoch 107/200] VAL   acc/loss: 0.721/0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 11994/22400] TRAIN loss: 0.099\n",
      "[Iteration 12004/22400] TRAIN loss: 0.195\n",
      "[Iteration 12014/22400] TRAIN loss: 0.154\n",
      "[Iteration 12024/22400] TRAIN loss: 0.106\n",
      "[Iteration 12034/22400] TRAIN loss: 0.137\n",
      "[Iteration 12044/22400] TRAIN loss: 0.342\n",
      "[Iteration 12054/22400] TRAIN loss: 0.140\n",
      "[Iteration 12064/22400] TRAIN loss: 0.186\n",
      "[Iteration 12074/22400] TRAIN loss: 0.216\n",
      "[Iteration 12084/22400] TRAIN loss: 0.096\n",
      "[Iteration 12094/22400] TRAIN loss: 0.144\n",
      "[Epoch 108/200] TRAIN acc/loss: 0.900/0.468\n",
      "[Epoch 108/200] VAL   acc/loss: 0.679/1.143\n",
      "[Iteration 12106/22400] TRAIN loss: 0.103\n",
      "[Iteration 12116/22400] TRAIN loss: 0.161\n",
      "[Iteration 12126/22400] TRAIN loss: 0.125\n",
      "[Iteration 12136/22400] TRAIN loss: 0.136\n",
      "[Iteration 12146/22400] TRAIN loss: 0.171\n",
      "[Iteration 12156/22400] TRAIN loss: 0.191\n",
      "[Iteration 12166/22400] TRAIN loss: 0.155\n",
      "[Iteration 12176/22400] TRAIN loss: 0.051\n",
      "[Iteration 12186/22400] TRAIN loss: 0.242\n",
      "[Iteration 12196/22400] TRAIN loss: 0.174\n",
      "[Iteration 12206/22400] TRAIN loss: 0.068\n",
      "[Epoch 109/200] TRAIN acc/loss: 1.000/0.052\n",
      "[Epoch 109/200] VAL   acc/loss: 0.729/0.864\n",
      "[Iteration 12218/22400] TRAIN loss: 0.139\n",
      "[Iteration 12228/22400] TRAIN loss: 0.121\n",
      "[Iteration 12238/22400] TRAIN loss: 0.195\n",
      "[Iteration 12248/22400] TRAIN loss: 0.166\n",
      "[Iteration 12258/22400] TRAIN loss: 0.139\n",
      "[Iteration 12268/22400] TRAIN loss: 0.140\n",
      "[Iteration 12278/22400] TRAIN loss: 0.144\n",
      "[Iteration 12288/22400] TRAIN loss: 0.163\n",
      "[Iteration 12298/22400] TRAIN loss: 0.320\n",
      "[Iteration 12308/22400] TRAIN loss: 0.158\n",
      "[Iteration 12318/22400] TRAIN loss: 0.126\n",
      "[Epoch 110/200] TRAIN acc/loss: 1.000/0.102\n",
      "[Epoch 110/200] VAL   acc/loss: 0.743/1.076\n",
      "[Iteration 12330/22400] TRAIN loss: 0.309\n",
      "[Iteration 12340/22400] TRAIN loss: 0.206\n",
      "[Iteration 12350/22400] TRAIN loss: 0.167\n",
      "[Iteration 12360/22400] TRAIN loss: 0.198\n",
      "[Iteration 12370/22400] TRAIN loss: 0.221\n",
      "[Iteration 12380/22400] TRAIN loss: 0.155\n",
      "[Iteration 12390/22400] TRAIN loss: 0.215\n",
      "[Iteration 12400/22400] TRAIN loss: 0.258\n",
      "[Iteration 12410/22400] TRAIN loss: 0.236\n",
      "[Iteration 12420/22400] TRAIN loss: 0.154\n",
      "[Iteration 12430/22400] TRAIN loss: 0.136\n",
      "[Epoch 111/200] TRAIN acc/loss: 1.000/0.020\n",
      "[Epoch 111/200] VAL   acc/loss: 0.707/1.060\n",
      "[Iteration 12442/22400] TRAIN loss: 0.197\n",
      "[Iteration 12452/22400] TRAIN loss: 0.193\n",
      "[Iteration 12462/22400] TRAIN loss: 0.216\n",
      "[Iteration 12472/22400] TRAIN loss: 0.324\n",
      "[Iteration 12482/22400] TRAIN loss: 0.151\n",
      "[Iteration 12492/22400] TRAIN loss: 0.139\n",
      "[Iteration 12502/22400] TRAIN loss: 0.219\n",
      "[Iteration 12512/22400] TRAIN loss: 0.237\n",
      "[Iteration 12522/22400] TRAIN loss: 0.232\n",
      "[Iteration 12532/22400] TRAIN loss: 0.346\n",
      "[Iteration 12542/22400] TRAIN loss: 0.242\n",
      "[Epoch 112/200] TRAIN acc/loss: 1.000/0.073\n",
      "[Epoch 112/200] VAL   acc/loss: 0.714/1.143\n",
      "[Iteration 12554/22400] TRAIN loss: 0.171\n",
      "[Iteration 12564/22400] TRAIN loss: 0.153\n",
      "[Iteration 12574/22400] TRAIN loss: 0.278\n",
      "[Iteration 12584/22400] TRAIN loss: 0.144\n",
      "[Iteration 12594/22400] TRAIN loss: 0.216\n",
      "[Iteration 12604/22400] TRAIN loss: 0.106\n",
      "[Iteration 12614/22400] TRAIN loss: 0.234\n",
      "[Iteration 12624/22400] TRAIN loss: 0.133\n",
      "[Iteration 12634/22400] TRAIN loss: 0.316\n",
      "[Iteration 12644/22400] TRAIN loss: 0.245\n",
      "[Iteration 12654/22400] TRAIN loss: 0.300\n",
      "[Epoch 113/200] TRAIN acc/loss: 0.900/0.350\n",
      "[Epoch 113/200] VAL   acc/loss: 0.714/1.084\n",
      "[Iteration 12666/22400] TRAIN loss: 0.125\n",
      "[Iteration 12676/22400] TRAIN loss: 0.188\n",
      "[Iteration 12686/22400] TRAIN loss: 0.216\n",
      "[Iteration 12696/22400] TRAIN loss: 0.206\n",
      "[Iteration 12706/22400] TRAIN loss: 0.147\n",
      "[Iteration 12716/22400] TRAIN loss: 0.213\n",
      "[Iteration 12726/22400] TRAIN loss: 0.156\n",
      "[Iteration 12736/22400] TRAIN loss: 0.216\n",
      "[Iteration 12746/22400] TRAIN loss: 0.221\n",
      "[Iteration 12756/22400] TRAIN loss: 0.175\n",
      "[Iteration 12766/22400] TRAIN loss: 0.304\n",
      "[Epoch 114/200] TRAIN acc/loss: 1.000/0.019\n",
      "[Epoch 114/200] VAL   acc/loss: 0.736/1.104\n",
      "[Iteration 12778/22400] TRAIN loss: 0.313\n",
      "[Iteration 12788/22400] TRAIN loss: 0.196\n",
      "[Iteration 12798/22400] TRAIN loss: 0.220\n",
      "[Iteration 12808/22400] TRAIN loss: 0.143\n",
      "[Iteration 12818/22400] TRAIN loss: 0.099\n",
      "[Iteration 12828/22400] TRAIN loss: 0.331\n",
      "[Iteration 12838/22400] TRAIN loss: 0.315\n",
      "[Iteration 12848/22400] TRAIN loss: 0.230\n",
      "[Iteration 12858/22400] TRAIN loss: 0.158\n",
      "[Iteration 12868/22400] TRAIN loss: 0.167\n",
      "[Iteration 12878/22400] TRAIN loss: 0.167\n",
      "[Epoch 115/200] TRAIN acc/loss: 0.900/0.538\n",
      "[Epoch 115/200] VAL   acc/loss: 0.714/1.096\n",
      "[Iteration 12890/22400] TRAIN loss: 0.143\n",
      "[Iteration 12900/22400] TRAIN loss: 0.290\n",
      "[Iteration 12910/22400] TRAIN loss: 0.117\n",
      "[Iteration 12920/22400] TRAIN loss: 0.184\n",
      "[Iteration 12930/22400] TRAIN loss: 0.143\n",
      "[Iteration 12940/22400] TRAIN loss: 0.134\n",
      "[Iteration 12950/22400] TRAIN loss: 0.171\n",
      "[Iteration 12960/22400] TRAIN loss: 0.192\n",
      "[Iteration 12970/22400] TRAIN loss: 0.176\n",
      "[Iteration 12980/22400] TRAIN loss: 0.253\n",
      "[Iteration 12990/22400] TRAIN loss: 0.229\n",
      "[Epoch 116/200] TRAIN acc/loss: 0.900/0.498\n",
      "[Epoch 116/200] VAL   acc/loss: 0.743/0.946\n",
      "[Iteration 13002/22400] TRAIN loss: 0.225\n",
      "[Iteration 13012/22400] TRAIN loss: 0.166\n",
      "[Iteration 13022/22400] TRAIN loss: 0.193\n",
      "[Iteration 13032/22400] TRAIN loss: 0.182\n",
      "[Iteration 13042/22400] TRAIN loss: 0.246\n",
      "[Iteration 13052/22400] TRAIN loss: 0.115\n",
      "[Iteration 13062/22400] TRAIN loss: 0.111\n",
      "[Iteration 13072/22400] TRAIN loss: 0.202\n",
      "[Iteration 13082/22400] TRAIN loss: 0.121\n",
      "[Iteration 13092/22400] TRAIN loss: 0.128\n",
      "[Iteration 13102/22400] TRAIN loss: 0.125\n",
      "[Epoch 117/200] TRAIN acc/loss: 0.800/0.662\n",
      "[Epoch 117/200] VAL   acc/loss: 0.736/1.060\n",
      "[Iteration 13114/22400] TRAIN loss: 0.133\n",
      "[Iteration 13124/22400] TRAIN loss: 0.076\n",
      "[Iteration 13134/22400] TRAIN loss: 0.152\n",
      "[Iteration 13144/22400] TRAIN loss: 0.193\n",
      "[Iteration 13154/22400] TRAIN loss: 0.220\n",
      "[Iteration 13164/22400] TRAIN loss: 0.131\n",
      "[Iteration 13174/22400] TRAIN loss: 0.336\n",
      "[Iteration 13184/22400] TRAIN loss: 0.183\n",
      "[Iteration 13194/22400] TRAIN loss: 0.130\n",
      "[Iteration 13204/22400] TRAIN loss: 0.392\n",
      "[Iteration 13214/22400] TRAIN loss: 0.217\n",
      "[Epoch 118/200] TRAIN acc/loss: 0.900/0.261\n",
      "[Epoch 118/200] VAL   acc/loss: 0.714/0.979\n",
      "[Iteration 13226/22400] TRAIN loss: 0.106\n",
      "[Iteration 13236/22400] TRAIN loss: 0.256\n",
      "[Iteration 13246/22400] TRAIN loss: 0.162\n",
      "[Iteration 13256/22400] TRAIN loss: 0.268\n",
      "[Iteration 13266/22400] TRAIN loss: 0.230\n",
      "[Iteration 13276/22400] TRAIN loss: 0.189\n",
      "[Iteration 13286/22400] TRAIN loss: 0.117\n",
      "[Iteration 13296/22400] TRAIN loss: 0.224\n",
      "[Iteration 13306/22400] TRAIN loss: 0.178\n",
      "[Iteration 13316/22400] TRAIN loss: 0.172\n",
      "[Iteration 13326/22400] TRAIN loss: 0.312\n",
      "[Epoch 119/200] TRAIN acc/loss: 0.900/0.165\n",
      "[Epoch 119/200] VAL   acc/loss: 0.714/1.049\n",
      "[Iteration 13338/22400] TRAIN loss: 0.060\n",
      "[Iteration 13348/22400] TRAIN loss: 0.132\n",
      "[Iteration 13358/22400] TRAIN loss: 0.142\n",
      "[Iteration 13368/22400] TRAIN loss: 0.196\n",
      "[Iteration 13378/22400] TRAIN loss: 0.134\n",
      "[Iteration 13388/22400] TRAIN loss: 0.133\n",
      "[Iteration 13398/22400] TRAIN loss: 0.162\n",
      "[Iteration 13408/22400] TRAIN loss: 0.237\n",
      "[Iteration 13418/22400] TRAIN loss: 0.144\n",
      "[Iteration 13428/22400] TRAIN loss: 0.125\n",
      "[Iteration 13438/22400] TRAIN loss: 0.110\n",
      "[Epoch 120/200] TRAIN acc/loss: 0.900/0.185\n",
      "[Epoch 120/200] VAL   acc/loss: 0.707/1.065\n",
      "[Iteration 13450/22400] TRAIN loss: 0.104\n",
      "[Iteration 13460/22400] TRAIN loss: 0.081\n",
      "[Iteration 13470/22400] TRAIN loss: 0.139\n",
      "[Iteration 13480/22400] TRAIN loss: 0.190\n",
      "[Iteration 13490/22400] TRAIN loss: 0.137\n",
      "[Iteration 13500/22400] TRAIN loss: 0.088\n",
      "[Iteration 13510/22400] TRAIN loss: 0.095\n",
      "[Iteration 13520/22400] TRAIN loss: 0.092\n",
      "[Iteration 13530/22400] TRAIN loss: 0.173\n",
      "[Iteration 13540/22400] TRAIN loss: 0.285\n",
      "[Iteration 13550/22400] TRAIN loss: 0.147\n",
      "[Epoch 121/200] TRAIN acc/loss: 1.000/0.106\n",
      "[Epoch 121/200] VAL   acc/loss: 0.721/1.125\n",
      "[Iteration 13562/22400] TRAIN loss: 0.134\n",
      "[Iteration 13572/22400] TRAIN loss: 0.056\n",
      "[Iteration 13582/22400] TRAIN loss: 0.156\n",
      "[Iteration 13592/22400] TRAIN loss: 0.183\n",
      "[Iteration 13602/22400] TRAIN loss: 0.273\n",
      "[Iteration 13612/22400] TRAIN loss: 0.107\n",
      "[Iteration 13622/22400] TRAIN loss: 0.116\n",
      "[Iteration 13632/22400] TRAIN loss: 0.168\n",
      "[Iteration 13642/22400] TRAIN loss: 0.195\n",
      "[Iteration 13652/22400] TRAIN loss: 0.130\n",
      "[Iteration 13662/22400] TRAIN loss: 0.224\n",
      "[Epoch 122/200] TRAIN acc/loss: 1.000/0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 122/200] VAL   acc/loss: 0.693/1.124\n",
      "[Iteration 13674/22400] TRAIN loss: 0.093\n",
      "[Iteration 13684/22400] TRAIN loss: 0.152\n",
      "[Iteration 13694/22400] TRAIN loss: 0.086\n",
      "[Iteration 13704/22400] TRAIN loss: 0.208\n",
      "[Iteration 13714/22400] TRAIN loss: 0.056\n",
      "[Iteration 13724/22400] TRAIN loss: 0.073\n",
      "[Iteration 13734/22400] TRAIN loss: 0.103\n",
      "[Iteration 13744/22400] TRAIN loss: 0.074\n",
      "[Iteration 13754/22400] TRAIN loss: 0.104\n",
      "[Iteration 13764/22400] TRAIN loss: 0.161\n",
      "[Iteration 13774/22400] TRAIN loss: 0.180\n",
      "[Epoch 123/200] TRAIN acc/loss: 0.800/0.408\n",
      "[Epoch 123/200] VAL   acc/loss: 0.729/1.055\n",
      "[Iteration 13786/22400] TRAIN loss: 0.199\n",
      "[Iteration 13796/22400] TRAIN loss: 0.097\n",
      "[Iteration 13806/22400] TRAIN loss: 0.245\n",
      "[Iteration 13816/22400] TRAIN loss: 0.224\n",
      "[Iteration 13826/22400] TRAIN loss: 0.258\n",
      "[Iteration 13836/22400] TRAIN loss: 0.131\n",
      "[Iteration 13846/22400] TRAIN loss: 0.125\n",
      "[Iteration 13856/22400] TRAIN loss: 0.251\n",
      "[Iteration 13866/22400] TRAIN loss: 0.130\n",
      "[Iteration 13876/22400] TRAIN loss: 0.112\n",
      "[Iteration 13886/22400] TRAIN loss: 0.283\n",
      "[Epoch 124/200] TRAIN acc/loss: 0.900/0.204\n",
      "[Epoch 124/200] VAL   acc/loss: 0.721/1.184\n",
      "[Iteration 13898/22400] TRAIN loss: 0.214\n",
      "[Iteration 13908/22400] TRAIN loss: 0.180\n",
      "[Iteration 13918/22400] TRAIN loss: 0.207\n",
      "[Iteration 13928/22400] TRAIN loss: 0.253\n",
      "[Iteration 13938/22400] TRAIN loss: 0.189\n",
      "[Iteration 13948/22400] TRAIN loss: 0.156\n",
      "[Iteration 13958/22400] TRAIN loss: 0.157\n",
      "[Iteration 13968/22400] TRAIN loss: 0.151\n",
      "[Iteration 13978/22400] TRAIN loss: 0.116\n",
      "[Iteration 13988/22400] TRAIN loss: 0.231\n",
      "[Iteration 13998/22400] TRAIN loss: 0.217\n",
      "[Epoch 125/200] TRAIN acc/loss: 1.000/0.049\n",
      "[Epoch 125/200] VAL   acc/loss: 0.750/1.073\n",
      "[Iteration 14010/22400] TRAIN loss: 0.108\n",
      "[Iteration 14020/22400] TRAIN loss: 0.235\n",
      "[Iteration 14030/22400] TRAIN loss: 0.089\n",
      "[Iteration 14040/22400] TRAIN loss: 0.161\n",
      "[Iteration 14050/22400] TRAIN loss: 0.092\n",
      "[Iteration 14060/22400] TRAIN loss: 0.203\n",
      "[Iteration 14070/22400] TRAIN loss: 0.096\n",
      "[Iteration 14080/22400] TRAIN loss: 0.141\n",
      "[Iteration 14090/22400] TRAIN loss: 0.111\n",
      "[Iteration 14100/22400] TRAIN loss: 0.077\n",
      "[Iteration 14110/22400] TRAIN loss: 0.080\n",
      "[Epoch 126/200] TRAIN acc/loss: 0.900/0.122\n",
      "[Epoch 126/200] VAL   acc/loss: 0.700/1.144\n",
      "[Iteration 14122/22400] TRAIN loss: 0.137\n",
      "[Iteration 14132/22400] TRAIN loss: 0.168\n",
      "[Iteration 14142/22400] TRAIN loss: 0.137\n",
      "[Iteration 14152/22400] TRAIN loss: 0.116\n",
      "[Iteration 14162/22400] TRAIN loss: 0.160\n",
      "[Iteration 14172/22400] TRAIN loss: 0.181\n",
      "[Iteration 14182/22400] TRAIN loss: 0.230\n",
      "[Iteration 14192/22400] TRAIN loss: 0.086\n",
      "[Iteration 14202/22400] TRAIN loss: 0.099\n",
      "[Iteration 14212/22400] TRAIN loss: 0.234\n",
      "[Iteration 14222/22400] TRAIN loss: 0.159\n",
      "[Epoch 127/200] TRAIN acc/loss: 1.000/0.022\n",
      "[Epoch 127/200] VAL   acc/loss: 0.714/1.078\n",
      "[Iteration 14234/22400] TRAIN loss: 0.087\n",
      "[Iteration 14244/22400] TRAIN loss: 0.142\n",
      "[Iteration 14254/22400] TRAIN loss: 0.061\n",
      "[Iteration 14264/22400] TRAIN loss: 0.088\n",
      "[Iteration 14274/22400] TRAIN loss: 0.090\n",
      "[Iteration 14284/22400] TRAIN loss: 0.157\n",
      "[Iteration 14294/22400] TRAIN loss: 0.118\n",
      "[Iteration 14304/22400] TRAIN loss: 0.057\n",
      "[Iteration 14314/22400] TRAIN loss: 0.068\n",
      "[Iteration 14324/22400] TRAIN loss: 0.158\n",
      "[Iteration 14334/22400] TRAIN loss: 0.184\n",
      "[Epoch 128/200] TRAIN acc/loss: 1.000/0.017\n",
      "[Epoch 128/200] VAL   acc/loss: 0.714/1.074\n",
      "[Iteration 14346/22400] TRAIN loss: 0.105\n",
      "[Iteration 14356/22400] TRAIN loss: 0.153\n",
      "[Iteration 14366/22400] TRAIN loss: 0.107\n",
      "[Iteration 14376/22400] TRAIN loss: 0.170\n",
      "[Iteration 14386/22400] TRAIN loss: 0.155\n",
      "[Iteration 14396/22400] TRAIN loss: 0.148\n",
      "[Iteration 14406/22400] TRAIN loss: 0.190\n",
      "[Iteration 14416/22400] TRAIN loss: 0.085\n",
      "[Iteration 14426/22400] TRAIN loss: 0.111\n",
      "[Iteration 14436/22400] TRAIN loss: 0.068\n",
      "[Iteration 14446/22400] TRAIN loss: 0.179\n",
      "[Epoch 129/200] TRAIN acc/loss: 1.000/0.024\n",
      "[Epoch 129/200] VAL   acc/loss: 0.743/1.038\n",
      "[Iteration 14458/22400] TRAIN loss: 0.079\n",
      "[Iteration 14468/22400] TRAIN loss: 0.091\n",
      "[Iteration 14478/22400] TRAIN loss: 0.191\n",
      "[Iteration 14488/22400] TRAIN loss: 0.096\n",
      "[Iteration 14498/22400] TRAIN loss: 0.172\n",
      "[Iteration 14508/22400] TRAIN loss: 0.123\n",
      "[Iteration 14518/22400] TRAIN loss: 0.127\n",
      "[Iteration 14528/22400] TRAIN loss: 0.279\n",
      "[Iteration 14538/22400] TRAIN loss: 0.117\n",
      "[Iteration 14548/22400] TRAIN loss: 0.144\n",
      "[Iteration 14558/22400] TRAIN loss: 0.070\n",
      "[Epoch 130/200] TRAIN acc/loss: 1.000/0.097\n",
      "[Epoch 130/200] VAL   acc/loss: 0.743/1.078\n",
      "[Iteration 14570/22400] TRAIN loss: 0.079\n",
      "[Iteration 14580/22400] TRAIN loss: 0.063\n",
      "[Iteration 14590/22400] TRAIN loss: 0.162\n",
      "[Iteration 14600/22400] TRAIN loss: 0.089\n",
      "[Iteration 14610/22400] TRAIN loss: 0.152\n",
      "[Iteration 14620/22400] TRAIN loss: 0.070\n",
      "[Iteration 14630/22400] TRAIN loss: 0.204\n",
      "[Iteration 14640/22400] TRAIN loss: 0.171\n",
      "[Iteration 14650/22400] TRAIN loss: 0.109\n",
      "[Iteration 14660/22400] TRAIN loss: 0.135\n",
      "[Iteration 14670/22400] TRAIN loss: 0.052\n",
      "[Epoch 131/200] TRAIN acc/loss: 1.000/0.138\n",
      "[Epoch 131/200] VAL   acc/loss: 0.700/1.067\n",
      "[Iteration 14682/22400] TRAIN loss: 0.111\n",
      "[Iteration 14692/22400] TRAIN loss: 0.130\n",
      "[Iteration 14702/22400] TRAIN loss: 0.116\n",
      "[Iteration 14712/22400] TRAIN loss: 0.117\n",
      "[Iteration 14722/22400] TRAIN loss: 0.141\n",
      "[Iteration 14732/22400] TRAIN loss: 0.157\n",
      "[Iteration 14742/22400] TRAIN loss: 0.092\n",
      "[Iteration 14752/22400] TRAIN loss: 0.070\n",
      "[Iteration 14762/22400] TRAIN loss: 0.183\n",
      "[Iteration 14772/22400] TRAIN loss: 0.081\n",
      "[Iteration 14782/22400] TRAIN loss: 0.146\n",
      "[Epoch 132/200] TRAIN acc/loss: 0.700/0.625\n",
      "[Epoch 132/200] VAL   acc/loss: 0.714/1.164\n",
      "[Iteration 14794/22400] TRAIN loss: 0.062\n",
      "[Iteration 14804/22400] TRAIN loss: 0.258\n",
      "[Iteration 14814/22400] TRAIN loss: 0.088\n",
      "[Iteration 14824/22400] TRAIN loss: 0.086\n",
      "[Iteration 14834/22400] TRAIN loss: 0.174\n",
      "[Iteration 14844/22400] TRAIN loss: 0.139\n",
      "[Iteration 14854/22400] TRAIN loss: 0.295\n",
      "[Iteration 14864/22400] TRAIN loss: 0.159\n",
      "[Iteration 14874/22400] TRAIN loss: 0.065\n",
      "[Iteration 14884/22400] TRAIN loss: 0.200\n",
      "[Iteration 14894/22400] TRAIN loss: 0.081\n",
      "[Epoch 133/200] TRAIN acc/loss: 1.000/0.088\n",
      "[Epoch 133/200] VAL   acc/loss: 0.700/1.191\n",
      "[Iteration 14906/22400] TRAIN loss: 0.149\n",
      "[Iteration 14916/22400] TRAIN loss: 0.081\n",
      "[Iteration 14926/22400] TRAIN loss: 0.091\n",
      "[Iteration 14936/22400] TRAIN loss: 0.196\n",
      "[Iteration 14946/22400] TRAIN loss: 0.060\n",
      "[Iteration 14956/22400] TRAIN loss: 0.105\n",
      "[Iteration 14966/22400] TRAIN loss: 0.130\n",
      "[Iteration 14976/22400] TRAIN loss: 0.075\n",
      "[Iteration 14986/22400] TRAIN loss: 0.119\n",
      "[Iteration 14996/22400] TRAIN loss: 0.143\n",
      "[Iteration 15006/22400] TRAIN loss: 0.109\n",
      "[Epoch 134/200] TRAIN acc/loss: 1.000/0.051\n",
      "[Epoch 134/200] VAL   acc/loss: 0.700/1.164\n",
      "[Iteration 15018/22400] TRAIN loss: 0.036\n",
      "[Iteration 15028/22400] TRAIN loss: 0.040\n",
      "[Iteration 15038/22400] TRAIN loss: 0.045\n",
      "[Iteration 15048/22400] TRAIN loss: 0.097\n",
      "[Iteration 15058/22400] TRAIN loss: 0.204\n",
      "[Iteration 15068/22400] TRAIN loss: 0.253\n",
      "[Iteration 15078/22400] TRAIN loss: 0.092\n",
      "[Iteration 15088/22400] TRAIN loss: 0.091\n",
      "[Iteration 15098/22400] TRAIN loss: 0.204\n",
      "[Iteration 15108/22400] TRAIN loss: 0.060\n",
      "[Iteration 15118/22400] TRAIN loss: 0.081\n",
      "[Epoch 135/200] TRAIN acc/loss: 1.000/0.008\n",
      "[Epoch 135/200] VAL   acc/loss: 0.779/0.867\n",
      "[Iteration 15130/22400] TRAIN loss: 0.176\n",
      "[Iteration 15140/22400] TRAIN loss: 0.055\n",
      "[Iteration 15150/22400] TRAIN loss: 0.101\n",
      "[Iteration 15160/22400] TRAIN loss: 0.082\n",
      "[Iteration 15170/22400] TRAIN loss: 0.085\n",
      "[Iteration 15180/22400] TRAIN loss: 0.064\n",
      "[Iteration 15190/22400] TRAIN loss: 0.051\n",
      "[Iteration 15200/22400] TRAIN loss: 0.036\n",
      "[Iteration 15210/22400] TRAIN loss: 0.085\n",
      "[Iteration 15220/22400] TRAIN loss: 0.195\n",
      "[Iteration 15230/22400] TRAIN loss: 0.089\n",
      "[Epoch 136/200] TRAIN acc/loss: 1.000/0.031\n",
      "[Epoch 136/200] VAL   acc/loss: 0.729/1.075\n",
      "[Iteration 15242/22400] TRAIN loss: 0.106\n",
      "[Iteration 15252/22400] TRAIN loss: 0.086\n",
      "[Iteration 15262/22400] TRAIN loss: 0.057\n",
      "[Iteration 15272/22400] TRAIN loss: 0.122\n",
      "[Iteration 15282/22400] TRAIN loss: 0.347\n",
      "[Iteration 15292/22400] TRAIN loss: 0.128\n",
      "[Iteration 15302/22400] TRAIN loss: 0.088\n",
      "[Iteration 15312/22400] TRAIN loss: 0.076\n",
      "[Iteration 15322/22400] TRAIN loss: 0.066\n",
      "[Iteration 15332/22400] TRAIN loss: 0.077\n",
      "[Iteration 15342/22400] TRAIN loss: 0.089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 137/200] TRAIN acc/loss: 1.000/0.038\n",
      "[Epoch 137/200] VAL   acc/loss: 0.686/1.033\n",
      "[Iteration 15354/22400] TRAIN loss: 0.094\n",
      "[Iteration 15364/22400] TRAIN loss: 0.070\n",
      "[Iteration 15374/22400] TRAIN loss: 0.075\n",
      "[Iteration 15384/22400] TRAIN loss: 0.103\n",
      "[Iteration 15394/22400] TRAIN loss: 0.177\n",
      "[Iteration 15404/22400] TRAIN loss: 0.128\n",
      "[Iteration 15414/22400] TRAIN loss: 0.156\n",
      "[Iteration 15424/22400] TRAIN loss: 0.111\n",
      "[Iteration 15434/22400] TRAIN loss: 0.121\n",
      "[Iteration 15444/22400] TRAIN loss: 0.122\n",
      "[Iteration 15454/22400] TRAIN loss: 0.126\n",
      "[Epoch 138/200] TRAIN acc/loss: 0.900/0.210\n",
      "[Epoch 138/200] VAL   acc/loss: 0.721/1.059\n",
      "[Iteration 15466/22400] TRAIN loss: 0.101\n",
      "[Iteration 15476/22400] TRAIN loss: 0.075\n",
      "[Iteration 15486/22400] TRAIN loss: 0.181\n",
      "[Iteration 15496/22400] TRAIN loss: 0.129\n",
      "[Iteration 15506/22400] TRAIN loss: 0.094\n",
      "[Iteration 15516/22400] TRAIN loss: 0.119\n",
      "[Iteration 15526/22400] TRAIN loss: 0.100\n",
      "[Iteration 15536/22400] TRAIN loss: 0.168\n",
      "[Iteration 15546/22400] TRAIN loss: 0.148\n",
      "[Iteration 15556/22400] TRAIN loss: 0.060\n",
      "[Iteration 15566/22400] TRAIN loss: 0.149\n",
      "[Epoch 139/200] TRAIN acc/loss: 1.000/0.004\n",
      "[Epoch 139/200] VAL   acc/loss: 0.729/1.134\n",
      "[Iteration 15578/22400] TRAIN loss: 0.140\n",
      "[Iteration 15588/22400] TRAIN loss: 0.193\n",
      "[Iteration 15598/22400] TRAIN loss: 0.107\n",
      "[Iteration 15608/22400] TRAIN loss: 0.125\n",
      "[Iteration 15618/22400] TRAIN loss: 0.132\n",
      "[Iteration 15628/22400] TRAIN loss: 0.197\n",
      "[Iteration 15638/22400] TRAIN loss: 0.042\n",
      "[Iteration 15648/22400] TRAIN loss: 0.157\n",
      "[Iteration 15658/22400] TRAIN loss: 0.082\n",
      "[Iteration 15668/22400] TRAIN loss: 0.058\n",
      "[Iteration 15678/22400] TRAIN loss: 0.096\n",
      "[Epoch 140/200] TRAIN acc/loss: 0.900/0.391\n",
      "[Epoch 140/200] VAL   acc/loss: 0.736/1.044\n",
      "[Iteration 15690/22400] TRAIN loss: 0.182\n",
      "[Iteration 15700/22400] TRAIN loss: 0.095\n",
      "[Iteration 15710/22400] TRAIN loss: 0.091\n",
      "[Iteration 15720/22400] TRAIN loss: 0.156\n",
      "[Iteration 15730/22400] TRAIN loss: 0.144\n",
      "[Iteration 15740/22400] TRAIN loss: 0.162\n",
      "[Iteration 15750/22400] TRAIN loss: 0.056\n",
      "[Iteration 15760/22400] TRAIN loss: 0.057\n",
      "[Iteration 15770/22400] TRAIN loss: 0.070\n",
      "[Iteration 15780/22400] TRAIN loss: 0.080\n",
      "[Iteration 15790/22400] TRAIN loss: 0.167\n",
      "[Epoch 141/200] TRAIN acc/loss: 1.000/0.014\n",
      "[Epoch 141/200] VAL   acc/loss: 0.779/0.879\n",
      "[Iteration 15802/22400] TRAIN loss: 0.115\n",
      "[Iteration 15812/22400] TRAIN loss: 0.057\n",
      "[Iteration 15822/22400] TRAIN loss: 0.185\n",
      "[Iteration 15832/22400] TRAIN loss: 0.123\n",
      "[Iteration 15842/22400] TRAIN loss: 0.118\n",
      "[Iteration 15852/22400] TRAIN loss: 0.051\n",
      "[Iteration 15862/22400] TRAIN loss: 0.046\n",
      "[Iteration 15872/22400] TRAIN loss: 0.100\n",
      "[Iteration 15882/22400] TRAIN loss: 0.202\n",
      "[Iteration 15892/22400] TRAIN loss: 0.087\n",
      "[Iteration 15902/22400] TRAIN loss: 0.148\n",
      "[Epoch 142/200] TRAIN acc/loss: 0.800/0.329\n",
      "[Epoch 142/200] VAL   acc/loss: 0.757/0.939\n",
      "[Iteration 15914/22400] TRAIN loss: 0.148\n",
      "[Iteration 15924/22400] TRAIN loss: 0.298\n",
      "[Iteration 15934/22400] TRAIN loss: 0.146\n",
      "[Iteration 15944/22400] TRAIN loss: 0.149\n",
      "[Iteration 15954/22400] TRAIN loss: 0.103\n",
      "[Iteration 15964/22400] TRAIN loss: 0.167\n",
      "[Iteration 15974/22400] TRAIN loss: 0.117\n",
      "[Iteration 15984/22400] TRAIN loss: 0.153\n",
      "[Iteration 15994/22400] TRAIN loss: 0.122\n",
      "[Iteration 16004/22400] TRAIN loss: 0.130\n",
      "[Iteration 16014/22400] TRAIN loss: 0.191\n",
      "[Epoch 143/200] TRAIN acc/loss: 1.000/0.086\n",
      "[Epoch 143/200] VAL   acc/loss: 0.736/1.042\n",
      "[Iteration 16026/22400] TRAIN loss: 0.066\n",
      "[Iteration 16036/22400] TRAIN loss: 0.037\n",
      "[Iteration 16046/22400] TRAIN loss: 0.121\n",
      "[Iteration 16056/22400] TRAIN loss: 0.095\n",
      "[Iteration 16066/22400] TRAIN loss: 0.059\n",
      "[Iteration 16076/22400] TRAIN loss: 0.069\n",
      "[Iteration 16086/22400] TRAIN loss: 0.179\n",
      "[Iteration 16096/22400] TRAIN loss: 0.073\n",
      "[Iteration 16106/22400] TRAIN loss: 0.111\n",
      "[Iteration 16116/22400] TRAIN loss: 0.210\n",
      "[Iteration 16126/22400] TRAIN loss: 0.175\n",
      "[Epoch 144/200] TRAIN acc/loss: 1.000/0.005\n",
      "[Epoch 144/200] VAL   acc/loss: 0.750/1.047\n",
      "[Iteration 16138/22400] TRAIN loss: 0.275\n",
      "[Iteration 16148/22400] TRAIN loss: 0.105\n",
      "[Iteration 16158/22400] TRAIN loss: 0.121\n",
      "[Iteration 16168/22400] TRAIN loss: 0.085\n",
      "[Iteration 16178/22400] TRAIN loss: 0.079\n",
      "[Iteration 16188/22400] TRAIN loss: 0.205\n",
      "[Iteration 16198/22400] TRAIN loss: 0.077\n",
      "[Iteration 16208/22400] TRAIN loss: 0.118\n",
      "[Iteration 16218/22400] TRAIN loss: 0.099\n",
      "[Iteration 16228/22400] TRAIN loss: 0.120\n",
      "[Iteration 16238/22400] TRAIN loss: 0.091\n",
      "[Epoch 145/200] TRAIN acc/loss: 1.000/0.018\n",
      "[Epoch 145/200] VAL   acc/loss: 0.757/1.087\n",
      "[Iteration 16250/22400] TRAIN loss: 0.136\n",
      "[Iteration 16260/22400] TRAIN loss: 0.121\n",
      "[Iteration 16270/22400] TRAIN loss: 0.147\n",
      "[Iteration 16280/22400] TRAIN loss: 0.142\n",
      "[Iteration 16290/22400] TRAIN loss: 0.085\n",
      "[Iteration 16300/22400] TRAIN loss: 0.044\n",
      "[Iteration 16310/22400] TRAIN loss: 0.069\n",
      "[Iteration 16320/22400] TRAIN loss: 0.117\n",
      "[Iteration 16330/22400] TRAIN loss: 0.078\n",
      "[Iteration 16340/22400] TRAIN loss: 0.053\n",
      "[Iteration 16350/22400] TRAIN loss: 0.056\n",
      "[Epoch 146/200] TRAIN acc/loss: 0.900/0.189\n",
      "[Epoch 146/200] VAL   acc/loss: 0.779/0.990\n",
      "[Iteration 16362/22400] TRAIN loss: 0.115\n",
      "[Iteration 16372/22400] TRAIN loss: 0.060\n",
      "[Iteration 16382/22400] TRAIN loss: 0.108\n",
      "[Iteration 16392/22400] TRAIN loss: 0.131\n",
      "[Iteration 16402/22400] TRAIN loss: 0.076\n",
      "[Iteration 16412/22400] TRAIN loss: 0.060\n",
      "[Iteration 16422/22400] TRAIN loss: 0.109\n",
      "[Iteration 16432/22400] TRAIN loss: 0.097\n",
      "[Iteration 16442/22400] TRAIN loss: 0.053\n",
      "[Iteration 16452/22400] TRAIN loss: 0.145\n",
      "[Iteration 16462/22400] TRAIN loss: 0.082\n",
      "[Epoch 147/200] TRAIN acc/loss: 1.000/0.147\n",
      "[Epoch 147/200] VAL   acc/loss: 0.757/1.047\n",
      "[Iteration 16474/22400] TRAIN loss: 0.033\n",
      "[Iteration 16484/22400] TRAIN loss: 0.052\n",
      "[Iteration 16494/22400] TRAIN loss: 0.055\n",
      "[Iteration 16504/22400] TRAIN loss: 0.063\n",
      "[Iteration 16514/22400] TRAIN loss: 0.042\n",
      "[Iteration 16524/22400] TRAIN loss: 0.070\n",
      "[Iteration 16534/22400] TRAIN loss: 0.055\n",
      "[Iteration 16544/22400] TRAIN loss: 0.094\n",
      "[Iteration 16554/22400] TRAIN loss: 0.134\n",
      "[Iteration 16564/22400] TRAIN loss: 0.075\n",
      "[Iteration 16574/22400] TRAIN loss: 0.039\n",
      "[Epoch 148/200] TRAIN acc/loss: 1.000/0.102\n",
      "[Epoch 148/200] VAL   acc/loss: 0.729/1.146\n",
      "[Iteration 16586/22400] TRAIN loss: 0.053\n",
      "[Iteration 16596/22400] TRAIN loss: 0.064\n",
      "[Iteration 16606/22400] TRAIN loss: 0.074\n",
      "[Iteration 16616/22400] TRAIN loss: 0.125\n",
      "[Iteration 16626/22400] TRAIN loss: 0.119\n",
      "[Iteration 16636/22400] TRAIN loss: 0.114\n",
      "[Iteration 16646/22400] TRAIN loss: 0.121\n",
      "[Iteration 16656/22400] TRAIN loss: 0.115\n",
      "[Iteration 16666/22400] TRAIN loss: 0.104\n",
      "[Iteration 16676/22400] TRAIN loss: 0.128\n",
      "[Iteration 16686/22400] TRAIN loss: 0.037\n",
      "[Epoch 149/200] TRAIN acc/loss: 0.900/0.223\n",
      "[Epoch 149/200] VAL   acc/loss: 0.750/1.072\n",
      "[Iteration 16698/22400] TRAIN loss: 0.086\n",
      "[Iteration 16708/22400] TRAIN loss: 0.137\n",
      "[Iteration 16718/22400] TRAIN loss: 0.068\n",
      "[Iteration 16728/22400] TRAIN loss: 0.134\n",
      "[Iteration 16738/22400] TRAIN loss: 0.069\n",
      "[Iteration 16748/22400] TRAIN loss: 0.077\n",
      "[Iteration 16758/22400] TRAIN loss: 0.165\n",
      "[Iteration 16768/22400] TRAIN loss: 0.143\n",
      "[Iteration 16778/22400] TRAIN loss: 0.053\n",
      "[Iteration 16788/22400] TRAIN loss: 0.098\n",
      "[Iteration 16798/22400] TRAIN loss: 0.051\n",
      "[Epoch 150/200] TRAIN acc/loss: 1.000/0.013\n",
      "[Epoch 150/200] VAL   acc/loss: 0.729/1.136\n",
      "[Iteration 16810/22400] TRAIN loss: 0.109\n",
      "[Iteration 16820/22400] TRAIN loss: 0.119\n",
      "[Iteration 16830/22400] TRAIN loss: 0.133\n",
      "[Iteration 16840/22400] TRAIN loss: 0.116\n",
      "[Iteration 16850/22400] TRAIN loss: 0.060\n",
      "[Iteration 16860/22400] TRAIN loss: 0.025\n",
      "[Iteration 16870/22400] TRAIN loss: 0.267\n",
      "[Iteration 16880/22400] TRAIN loss: 0.062\n",
      "[Iteration 16890/22400] TRAIN loss: 0.062\n",
      "[Iteration 16900/22400] TRAIN loss: 0.082\n",
      "[Iteration 16910/22400] TRAIN loss: 0.036\n",
      "[Epoch 151/200] TRAIN acc/loss: 1.000/0.039\n",
      "[Epoch 151/200] VAL   acc/loss: 0.750/0.985\n",
      "[Iteration 16922/22400] TRAIN loss: 0.042\n",
      "[Iteration 16932/22400] TRAIN loss: 0.038\n",
      "[Iteration 16942/22400] TRAIN loss: 0.056\n",
      "[Iteration 16952/22400] TRAIN loss: 0.062\n",
      "[Iteration 16962/22400] TRAIN loss: 0.105\n",
      "[Iteration 16972/22400] TRAIN loss: 0.116\n",
      "[Iteration 16982/22400] TRAIN loss: 0.064\n",
      "[Iteration 16992/22400] TRAIN loss: 0.056\n",
      "[Iteration 17002/22400] TRAIN loss: 0.112\n",
      "[Iteration 17012/22400] TRAIN loss: 0.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 17022/22400] TRAIN loss: 0.068\n",
      "[Epoch 152/200] TRAIN acc/loss: 0.900/0.271\n",
      "[Epoch 152/200] VAL   acc/loss: 0.764/1.069\n",
      "[Iteration 17034/22400] TRAIN loss: 0.018\n",
      "[Iteration 17044/22400] TRAIN loss: 0.146\n",
      "[Iteration 17054/22400] TRAIN loss: 0.230\n",
      "[Iteration 17064/22400] TRAIN loss: 0.114\n",
      "[Iteration 17074/22400] TRAIN loss: 0.091\n",
      "[Iteration 17084/22400] TRAIN loss: 0.116\n",
      "[Iteration 17094/22400] TRAIN loss: 0.126\n",
      "[Iteration 17104/22400] TRAIN loss: 0.126\n",
      "[Iteration 17114/22400] TRAIN loss: 0.047\n",
      "[Iteration 17124/22400] TRAIN loss: 0.129\n",
      "[Iteration 17134/22400] TRAIN loss: 0.068\n",
      "[Epoch 153/200] TRAIN acc/loss: 1.000/0.023\n",
      "[Epoch 153/200] VAL   acc/loss: 0.736/1.170\n",
      "[Iteration 17146/22400] TRAIN loss: 0.224\n",
      "[Iteration 17156/22400] TRAIN loss: 0.147\n",
      "[Iteration 17166/22400] TRAIN loss: 0.093\n",
      "[Iteration 17176/22400] TRAIN loss: 0.036\n",
      "[Iteration 17186/22400] TRAIN loss: 0.148\n",
      "[Iteration 17196/22400] TRAIN loss: 0.023\n",
      "[Iteration 17206/22400] TRAIN loss: 0.056\n",
      "[Iteration 17216/22400] TRAIN loss: 0.168\n",
      "[Iteration 17226/22400] TRAIN loss: 0.081\n",
      "[Iteration 17236/22400] TRAIN loss: 0.156\n",
      "[Iteration 17246/22400] TRAIN loss: 0.085\n",
      "[Epoch 154/200] TRAIN acc/loss: 1.000/0.036\n",
      "[Epoch 154/200] VAL   acc/loss: 0.786/0.991\n",
      "[Iteration 17258/22400] TRAIN loss: 0.047\n",
      "[Iteration 17268/22400] TRAIN loss: 0.101\n",
      "[Iteration 17278/22400] TRAIN loss: 0.090\n",
      "[Iteration 17288/22400] TRAIN loss: 0.038\n",
      "[Iteration 17298/22400] TRAIN loss: 0.132\n",
      "[Iteration 17308/22400] TRAIN loss: 0.111\n",
      "[Iteration 17318/22400] TRAIN loss: 0.049\n",
      "[Iteration 17328/22400] TRAIN loss: 0.055\n",
      "[Iteration 17338/22400] TRAIN loss: 0.174\n",
      "[Iteration 17348/22400] TRAIN loss: 0.111\n",
      "[Iteration 17358/22400] TRAIN loss: 0.124\n",
      "[Epoch 155/200] TRAIN acc/loss: 1.000/0.003\n",
      "[Epoch 155/200] VAL   acc/loss: 0.707/1.428\n",
      "[Iteration 17370/22400] TRAIN loss: 0.040\n",
      "[Iteration 17380/22400] TRAIN loss: 0.243\n",
      "[Iteration 17390/22400] TRAIN loss: 0.087\n",
      "[Iteration 17400/22400] TRAIN loss: 0.058\n",
      "[Iteration 17410/22400] TRAIN loss: 0.073\n",
      "[Iteration 17420/22400] TRAIN loss: 0.106\n",
      "[Iteration 17430/22400] TRAIN loss: 0.123\n",
      "[Iteration 17440/22400] TRAIN loss: 0.089\n",
      "[Iteration 17450/22400] TRAIN loss: 0.115\n",
      "[Iteration 17460/22400] TRAIN loss: 0.072\n",
      "[Iteration 17470/22400] TRAIN loss: 0.094\n",
      "[Epoch 156/200] TRAIN acc/loss: 1.000/0.071\n",
      "[Epoch 156/200] VAL   acc/loss: 0.757/1.217\n",
      "[Iteration 17482/22400] TRAIN loss: 0.039\n",
      "[Iteration 17492/22400] TRAIN loss: 0.074\n",
      "[Iteration 17502/22400] TRAIN loss: 0.076\n",
      "[Iteration 17512/22400] TRAIN loss: 0.203\n",
      "[Iteration 17522/22400] TRAIN loss: 0.135\n",
      "[Iteration 17532/22400] TRAIN loss: 0.077\n",
      "[Iteration 17542/22400] TRAIN loss: 0.173\n",
      "[Iteration 17552/22400] TRAIN loss: 0.067\n",
      "[Iteration 17562/22400] TRAIN loss: 0.334\n",
      "[Iteration 17572/22400] TRAIN loss: 0.122\n",
      "[Iteration 17582/22400] TRAIN loss: 0.159\n",
      "[Epoch 157/200] TRAIN acc/loss: 1.000/0.077\n",
      "[Epoch 157/200] VAL   acc/loss: 0.743/1.181\n",
      "[Iteration 17594/22400] TRAIN loss: 0.153\n",
      "[Iteration 17604/22400] TRAIN loss: 0.135\n",
      "[Iteration 17614/22400] TRAIN loss: 0.113\n",
      "[Iteration 17624/22400] TRAIN loss: 0.208\n",
      "[Iteration 17634/22400] TRAIN loss: 0.104\n",
      "[Iteration 17644/22400] TRAIN loss: 0.141\n",
      "[Iteration 17654/22400] TRAIN loss: 0.172\n",
      "[Iteration 17664/22400] TRAIN loss: 0.183\n",
      "[Iteration 17674/22400] TRAIN loss: 0.170\n",
      "[Iteration 17684/22400] TRAIN loss: 0.153\n",
      "[Iteration 17694/22400] TRAIN loss: 0.144\n",
      "[Epoch 158/200] TRAIN acc/loss: 1.000/0.013\n",
      "[Epoch 158/200] VAL   acc/loss: 0.714/1.195\n",
      "[Iteration 17706/22400] TRAIN loss: 0.146\n",
      "[Iteration 17716/22400] TRAIN loss: 0.247\n",
      "[Iteration 17726/22400] TRAIN loss: 0.260\n",
      "[Iteration 17736/22400] TRAIN loss: 0.134\n",
      "[Iteration 17746/22400] TRAIN loss: 0.074\n",
      "[Iteration 17756/22400] TRAIN loss: 0.146\n",
      "[Iteration 17766/22400] TRAIN loss: 0.081\n",
      "[Iteration 17776/22400] TRAIN loss: 0.099\n",
      "[Iteration 17786/22400] TRAIN loss: 0.141\n",
      "[Iteration 17796/22400] TRAIN loss: 0.073\n",
      "[Iteration 17806/22400] TRAIN loss: 0.146\n",
      "[Epoch 159/200] TRAIN acc/loss: 0.900/0.601\n",
      "[Epoch 159/200] VAL   acc/loss: 0.771/1.166\n",
      "[Iteration 17818/22400] TRAIN loss: 0.234\n",
      "[Iteration 17828/22400] TRAIN loss: 0.118\n",
      "[Iteration 17838/22400] TRAIN loss: 0.104\n",
      "[Iteration 17848/22400] TRAIN loss: 0.119\n",
      "[Iteration 17858/22400] TRAIN loss: 0.150\n",
      "[Iteration 17868/22400] TRAIN loss: 0.090\n",
      "[Iteration 17878/22400] TRAIN loss: 0.152\n",
      "[Iteration 17888/22400] TRAIN loss: 0.051\n",
      "[Iteration 17898/22400] TRAIN loss: 0.180\n",
      "[Iteration 17908/22400] TRAIN loss: 0.118\n",
      "[Iteration 17918/22400] TRAIN loss: 0.155\n",
      "[Epoch 160/200] TRAIN acc/loss: 1.000/0.022\n",
      "[Epoch 160/200] VAL   acc/loss: 0.743/1.141\n",
      "[Iteration 17930/22400] TRAIN loss: 0.239\n",
      "[Iteration 17940/22400] TRAIN loss: 0.176\n",
      "[Iteration 17950/22400] TRAIN loss: 0.050\n",
      "[Iteration 17960/22400] TRAIN loss: 0.112\n",
      "[Iteration 17970/22400] TRAIN loss: 0.072\n",
      "[Iteration 17980/22400] TRAIN loss: 0.148\n",
      "[Iteration 17990/22400] TRAIN loss: 0.036\n",
      "[Iteration 18000/22400] TRAIN loss: 0.185\n",
      "[Iteration 18010/22400] TRAIN loss: 0.128\n",
      "[Iteration 18020/22400] TRAIN loss: 0.119\n",
      "[Iteration 18030/22400] TRAIN loss: 0.144\n",
      "[Epoch 161/200] TRAIN acc/loss: 1.000/0.006\n",
      "[Epoch 161/200] VAL   acc/loss: 0.686/1.378\n",
      "[Iteration 18042/22400] TRAIN loss: 0.136\n",
      "[Iteration 18052/22400] TRAIN loss: 0.222\n",
      "[Iteration 18062/22400] TRAIN loss: 0.122\n",
      "[Iteration 18072/22400] TRAIN loss: 0.077\n",
      "[Iteration 18082/22400] TRAIN loss: 0.040\n",
      "[Iteration 18092/22400] TRAIN loss: 0.115\n",
      "[Iteration 18102/22400] TRAIN loss: 0.114\n",
      "[Iteration 18112/22400] TRAIN loss: 0.091\n",
      "[Iteration 18122/22400] TRAIN loss: 0.098\n",
      "[Iteration 18132/22400] TRAIN loss: 0.106\n",
      "[Iteration 18142/22400] TRAIN loss: 0.121\n",
      "[Epoch 162/200] TRAIN acc/loss: 0.800/0.261\n",
      "[Epoch 162/200] VAL   acc/loss: 0.693/1.347\n",
      "[Iteration 18154/22400] TRAIN loss: 0.105\n",
      "[Iteration 18164/22400] TRAIN loss: 0.079\n",
      "[Iteration 18174/22400] TRAIN loss: 0.091\n",
      "[Iteration 18184/22400] TRAIN loss: 0.031\n",
      "[Iteration 18194/22400] TRAIN loss: 0.070\n",
      "[Iteration 18204/22400] TRAIN loss: 0.197\n",
      "[Iteration 18214/22400] TRAIN loss: 0.186\n",
      "[Iteration 18224/22400] TRAIN loss: 0.090\n",
      "[Iteration 18234/22400] TRAIN loss: 0.123\n",
      "[Iteration 18244/22400] TRAIN loss: 0.083\n",
      "[Iteration 18254/22400] TRAIN loss: 0.091\n",
      "[Epoch 163/200] TRAIN acc/loss: 1.000/0.008\n",
      "[Epoch 163/200] VAL   acc/loss: 0.729/1.283\n",
      "[Iteration 18266/22400] TRAIN loss: 0.128\n",
      "[Iteration 18276/22400] TRAIN loss: 0.037\n",
      "[Iteration 18286/22400] TRAIN loss: 0.084\n",
      "[Iteration 18296/22400] TRAIN loss: 0.033\n",
      "[Iteration 18306/22400] TRAIN loss: 0.088\n",
      "[Iteration 18316/22400] TRAIN loss: 0.021\n",
      "[Iteration 18326/22400] TRAIN loss: 0.104\n",
      "[Iteration 18336/22400] TRAIN loss: 0.084\n",
      "[Iteration 18346/22400] TRAIN loss: 0.044\n",
      "[Iteration 18356/22400] TRAIN loss: 0.139\n",
      "[Iteration 18366/22400] TRAIN loss: 0.216\n",
      "[Epoch 164/200] TRAIN acc/loss: 1.000/0.126\n",
      "[Epoch 164/200] VAL   acc/loss: 0.743/1.105\n",
      "[Iteration 18378/22400] TRAIN loss: 0.056\n",
      "[Iteration 18388/22400] TRAIN loss: 0.117\n",
      "[Iteration 18398/22400] TRAIN loss: 0.161\n",
      "[Iteration 18408/22400] TRAIN loss: 0.149\n",
      "[Iteration 18418/22400] TRAIN loss: 0.103\n",
      "[Iteration 18428/22400] TRAIN loss: 0.333\n",
      "[Iteration 18438/22400] TRAIN loss: 0.118\n",
      "[Iteration 18448/22400] TRAIN loss: 0.087\n",
      "[Iteration 18458/22400] TRAIN loss: 0.087\n",
      "[Iteration 18468/22400] TRAIN loss: 0.131\n",
      "[Iteration 18478/22400] TRAIN loss: 0.110\n",
      "[Epoch 165/200] TRAIN acc/loss: 1.000/0.112\n",
      "[Epoch 165/200] VAL   acc/loss: 0.707/1.140\n",
      "[Iteration 18490/22400] TRAIN loss: 0.138\n",
      "[Iteration 18500/22400] TRAIN loss: 0.066\n",
      "[Iteration 18510/22400] TRAIN loss: 0.137\n",
      "[Iteration 18520/22400] TRAIN loss: 0.061\n",
      "[Iteration 18530/22400] TRAIN loss: 0.089\n",
      "[Iteration 18540/22400] TRAIN loss: 0.066\n",
      "[Iteration 18550/22400] TRAIN loss: 0.110\n",
      "[Iteration 18560/22400] TRAIN loss: 0.089\n",
      "[Iteration 18570/22400] TRAIN loss: 0.131\n",
      "[Iteration 18580/22400] TRAIN loss: 0.092\n",
      "[Iteration 18590/22400] TRAIN loss: 0.076\n",
      "[Epoch 166/200] TRAIN acc/loss: 1.000/0.004\n",
      "[Epoch 166/200] VAL   acc/loss: 0.779/0.969\n",
      "[Iteration 18602/22400] TRAIN loss: 0.033\n",
      "[Iteration 18612/22400] TRAIN loss: 0.121\n",
      "[Iteration 18622/22400] TRAIN loss: 0.134\n",
      "[Iteration 18632/22400] TRAIN loss: 0.155\n",
      "[Iteration 18642/22400] TRAIN loss: 0.046\n",
      "[Iteration 18652/22400] TRAIN loss: 0.104\n",
      "[Iteration 18662/22400] TRAIN loss: 0.063\n",
      "[Iteration 18672/22400] TRAIN loss: 0.035\n",
      "[Iteration 18682/22400] TRAIN loss: 0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 18692/22400] TRAIN loss: 0.145\n",
      "[Iteration 18702/22400] TRAIN loss: 0.044\n",
      "[Epoch 167/200] TRAIN acc/loss: 1.000/0.094\n",
      "[Epoch 167/200] VAL   acc/loss: 0.793/0.964\n",
      "[Iteration 18714/22400] TRAIN loss: 0.099\n",
      "[Iteration 18724/22400] TRAIN loss: 0.133\n",
      "[Iteration 18734/22400] TRAIN loss: 0.031\n",
      "[Iteration 18744/22400] TRAIN loss: 0.103\n",
      "[Iteration 18754/22400] TRAIN loss: 0.025\n",
      "[Iteration 18764/22400] TRAIN loss: 0.039\n",
      "[Iteration 18774/22400] TRAIN loss: 0.076\n",
      "[Iteration 18784/22400] TRAIN loss: 0.082\n",
      "[Iteration 18794/22400] TRAIN loss: 0.037\n",
      "[Iteration 18804/22400] TRAIN loss: 0.042\n",
      "[Iteration 18814/22400] TRAIN loss: 0.074\n",
      "[Epoch 168/200] TRAIN acc/loss: 1.000/0.015\n",
      "[Epoch 168/200] VAL   acc/loss: 0.757/1.014\n",
      "[Iteration 18826/22400] TRAIN loss: 0.078\n",
      "[Iteration 18836/22400] TRAIN loss: 0.049\n",
      "[Iteration 18846/22400] TRAIN loss: 0.076\n",
      "[Iteration 18856/22400] TRAIN loss: 0.067\n",
      "[Iteration 18866/22400] TRAIN loss: 0.068\n",
      "[Iteration 18876/22400] TRAIN loss: 0.035\n",
      "[Iteration 18886/22400] TRAIN loss: 0.052\n",
      "[Iteration 18896/22400] TRAIN loss: 0.078\n",
      "[Iteration 18906/22400] TRAIN loss: 0.032\n",
      "[Iteration 18916/22400] TRAIN loss: 0.148\n",
      "[Iteration 18926/22400] TRAIN loss: 0.033\n",
      "[Epoch 169/200] TRAIN acc/loss: 1.000/0.064\n",
      "[Epoch 169/200] VAL   acc/loss: 0.764/1.063\n",
      "[Iteration 18938/22400] TRAIN loss: 0.030\n",
      "[Iteration 18948/22400] TRAIN loss: 0.038\n",
      "[Iteration 18958/22400] TRAIN loss: 0.033\n",
      "[Iteration 18968/22400] TRAIN loss: 0.029\n",
      "[Iteration 18978/22400] TRAIN loss: 0.015\n",
      "[Iteration 18988/22400] TRAIN loss: 0.092\n",
      "[Iteration 18998/22400] TRAIN loss: 0.051\n",
      "[Iteration 19008/22400] TRAIN loss: 0.021\n",
      "[Iteration 19018/22400] TRAIN loss: 0.155\n",
      "[Iteration 19028/22400] TRAIN loss: 0.074\n",
      "[Iteration 19038/22400] TRAIN loss: 0.017\n",
      "[Epoch 170/200] TRAIN acc/loss: 1.000/0.027\n",
      "[Epoch 170/200] VAL   acc/loss: 0.743/1.129\n",
      "[Iteration 19050/22400] TRAIN loss: 0.059\n",
      "[Iteration 19060/22400] TRAIN loss: 0.077\n",
      "[Iteration 19070/22400] TRAIN loss: 0.081\n",
      "[Iteration 19080/22400] TRAIN loss: 0.061\n",
      "[Iteration 19090/22400] TRAIN loss: 0.060\n",
      "[Iteration 19100/22400] TRAIN loss: 0.033\n",
      "[Iteration 19110/22400] TRAIN loss: 0.084\n",
      "[Iteration 19120/22400] TRAIN loss: 0.021\n",
      "[Iteration 19130/22400] TRAIN loss: 0.119\n",
      "[Iteration 19140/22400] TRAIN loss: 0.143\n",
      "[Iteration 19150/22400] TRAIN loss: 0.091\n",
      "[Epoch 171/200] TRAIN acc/loss: 0.900/0.577\n",
      "[Epoch 171/200] VAL   acc/loss: 0.771/1.053\n",
      "[Iteration 19162/22400] TRAIN loss: 0.033\n",
      "[Iteration 19172/22400] TRAIN loss: 0.047\n",
      "[Iteration 19182/22400] TRAIN loss: 0.053\n",
      "[Iteration 19192/22400] TRAIN loss: 0.034\n",
      "[Iteration 19202/22400] TRAIN loss: 0.090\n",
      "[Iteration 19212/22400] TRAIN loss: 0.046\n",
      "[Iteration 19222/22400] TRAIN loss: 0.056\n",
      "[Iteration 19232/22400] TRAIN loss: 0.023\n",
      "[Iteration 19242/22400] TRAIN loss: 0.047\n",
      "[Iteration 19252/22400] TRAIN loss: 0.026\n",
      "[Iteration 19262/22400] TRAIN loss: 0.105\n",
      "[Epoch 172/200] TRAIN acc/loss: 1.000/0.029\n",
      "[Epoch 172/200] VAL   acc/loss: 0.729/1.211\n",
      "[Iteration 19274/22400] TRAIN loss: 0.016\n",
      "[Iteration 19284/22400] TRAIN loss: 0.198\n",
      "[Iteration 19294/22400] TRAIN loss: 0.018\n",
      "[Iteration 19304/22400] TRAIN loss: 0.227\n",
      "[Iteration 19314/22400] TRAIN loss: 0.040\n",
      "[Iteration 19324/22400] TRAIN loss: 0.101\n",
      "[Iteration 19334/22400] TRAIN loss: 0.078\n",
      "[Iteration 19344/22400] TRAIN loss: 0.043\n",
      "[Iteration 19354/22400] TRAIN loss: 0.026\n",
      "[Iteration 19364/22400] TRAIN loss: 0.061\n",
      "[Iteration 19374/22400] TRAIN loss: 0.025\n",
      "[Epoch 173/200] TRAIN acc/loss: 1.000/0.018\n",
      "[Epoch 173/200] VAL   acc/loss: 0.793/1.083\n",
      "[Iteration 19386/22400] TRAIN loss: 0.069\n",
      "[Iteration 19396/22400] TRAIN loss: 0.035\n",
      "[Iteration 19406/22400] TRAIN loss: 0.037\n",
      "[Iteration 19416/22400] TRAIN loss: 0.062\n",
      "[Iteration 19426/22400] TRAIN loss: 0.071\n",
      "[Iteration 19436/22400] TRAIN loss: 0.118\n",
      "[Iteration 19446/22400] TRAIN loss: 0.062\n",
      "[Iteration 19456/22400] TRAIN loss: 0.117\n",
      "[Iteration 19466/22400] TRAIN loss: 0.040\n",
      "[Iteration 19476/22400] TRAIN loss: 0.096\n",
      "[Iteration 19486/22400] TRAIN loss: 0.147\n",
      "[Epoch 174/200] TRAIN acc/loss: 1.000/0.050\n",
      "[Epoch 174/200] VAL   acc/loss: 0.786/1.027\n",
      "[Iteration 19498/22400] TRAIN loss: 0.070\n",
      "[Iteration 19508/22400] TRAIN loss: 0.073\n",
      "[Iteration 19518/22400] TRAIN loss: 0.074\n",
      "[Iteration 19528/22400] TRAIN loss: 0.100\n",
      "[Iteration 19538/22400] TRAIN loss: 0.074\n",
      "[Iteration 19548/22400] TRAIN loss: 0.076\n",
      "[Iteration 19558/22400] TRAIN loss: 0.077\n",
      "[Iteration 19568/22400] TRAIN loss: 0.092\n",
      "[Iteration 19578/22400] TRAIN loss: 0.093\n",
      "[Iteration 19588/22400] TRAIN loss: 0.025\n",
      "[Iteration 19598/22400] TRAIN loss: 0.191\n",
      "[Epoch 175/200] TRAIN acc/loss: 1.000/0.016\n",
      "[Epoch 175/200] VAL   acc/loss: 0.729/1.130\n",
      "[Iteration 19610/22400] TRAIN loss: 0.081\n",
      "[Iteration 19620/22400] TRAIN loss: 0.089\n",
      "[Iteration 19630/22400] TRAIN loss: 0.123\n",
      "[Iteration 19640/22400] TRAIN loss: 0.073\n",
      "[Iteration 19650/22400] TRAIN loss: 0.158\n",
      "[Iteration 19660/22400] TRAIN loss: 0.106\n",
      "[Iteration 19670/22400] TRAIN loss: 0.049\n",
      "[Iteration 19680/22400] TRAIN loss: 0.071\n",
      "[Iteration 19690/22400] TRAIN loss: 0.060\n",
      "[Iteration 19700/22400] TRAIN loss: 0.148\n",
      "[Iteration 19710/22400] TRAIN loss: 0.212\n",
      "[Epoch 176/200] TRAIN acc/loss: 1.000/0.018\n",
      "[Epoch 176/200] VAL   acc/loss: 0.750/0.972\n",
      "[Iteration 19722/22400] TRAIN loss: 0.048\n",
      "[Iteration 19732/22400] TRAIN loss: 0.078\n",
      "[Iteration 19742/22400] TRAIN loss: 0.103\n",
      "[Iteration 19752/22400] TRAIN loss: 0.100\n",
      "[Iteration 19762/22400] TRAIN loss: 0.039\n",
      "[Iteration 19772/22400] TRAIN loss: 0.078\n",
      "[Iteration 19782/22400] TRAIN loss: 0.052\n",
      "[Iteration 19792/22400] TRAIN loss: 0.081\n",
      "[Iteration 19802/22400] TRAIN loss: 0.093\n",
      "[Iteration 19812/22400] TRAIN loss: 0.027\n",
      "[Iteration 19822/22400] TRAIN loss: 0.196\n",
      "[Epoch 177/200] TRAIN acc/loss: 1.000/0.014\n",
      "[Epoch 177/200] VAL   acc/loss: 0.750/0.938\n",
      "[Iteration 19834/22400] TRAIN loss: 0.031\n",
      "[Iteration 19844/22400] TRAIN loss: 0.030\n",
      "[Iteration 19854/22400] TRAIN loss: 0.108\n",
      "[Iteration 19864/22400] TRAIN loss: 0.047\n",
      "[Iteration 19874/22400] TRAIN loss: 0.106\n",
      "[Iteration 19884/22400] TRAIN loss: 0.084\n",
      "[Iteration 19894/22400] TRAIN loss: 0.071\n",
      "[Iteration 19904/22400] TRAIN loss: 0.073\n",
      "[Iteration 19914/22400] TRAIN loss: 0.109\n",
      "[Iteration 19924/22400] TRAIN loss: 0.098\n",
      "[Iteration 19934/22400] TRAIN loss: 0.056\n",
      "[Epoch 178/200] TRAIN acc/loss: 1.000/0.009\n",
      "[Epoch 178/200] VAL   acc/loss: 0.736/1.079\n",
      "[Iteration 19946/22400] TRAIN loss: 0.134\n",
      "[Iteration 19956/22400] TRAIN loss: 0.145\n",
      "[Iteration 19966/22400] TRAIN loss: 0.063\n",
      "[Iteration 19976/22400] TRAIN loss: 0.039\n",
      "[Iteration 19986/22400] TRAIN loss: 0.031\n",
      "[Iteration 19996/22400] TRAIN loss: 0.091\n",
      "[Iteration 20006/22400] TRAIN loss: 0.041\n",
      "[Iteration 20016/22400] TRAIN loss: 0.025\n",
      "[Iteration 20026/22400] TRAIN loss: 0.101\n",
      "[Iteration 20036/22400] TRAIN loss: 0.067\n",
      "[Iteration 20046/22400] TRAIN loss: 0.049\n",
      "[Epoch 179/200] TRAIN acc/loss: 1.000/0.109\n",
      "[Epoch 179/200] VAL   acc/loss: 0.764/1.047\n",
      "[Iteration 20058/22400] TRAIN loss: 0.054\n",
      "[Iteration 20068/22400] TRAIN loss: 0.030\n",
      "[Iteration 20078/22400] TRAIN loss: 0.042\n",
      "[Iteration 20088/22400] TRAIN loss: 0.017\n",
      "[Iteration 20098/22400] TRAIN loss: 0.052\n",
      "[Iteration 20108/22400] TRAIN loss: 0.027\n",
      "[Iteration 20118/22400] TRAIN loss: 0.060\n",
      "[Iteration 20128/22400] TRAIN loss: 0.148\n",
      "[Iteration 20138/22400] TRAIN loss: 0.117\n",
      "[Iteration 20148/22400] TRAIN loss: 0.059\n",
      "[Iteration 20158/22400] TRAIN loss: 0.055\n",
      "[Epoch 180/200] TRAIN acc/loss: 0.900/0.157\n",
      "[Epoch 180/200] VAL   acc/loss: 0.757/1.060\n",
      "[Iteration 20170/22400] TRAIN loss: 0.055\n",
      "[Iteration 20180/22400] TRAIN loss: 0.147\n",
      "[Iteration 20190/22400] TRAIN loss: 0.022\n",
      "[Iteration 20200/22400] TRAIN loss: 0.041\n",
      "[Iteration 20210/22400] TRAIN loss: 0.074\n",
      "[Iteration 20220/22400] TRAIN loss: 0.072\n",
      "[Iteration 20230/22400] TRAIN loss: 0.055\n",
      "[Iteration 20240/22400] TRAIN loss: 0.079\n",
      "[Iteration 20250/22400] TRAIN loss: 0.037\n",
      "[Iteration 20260/22400] TRAIN loss: 0.067\n",
      "[Iteration 20270/22400] TRAIN loss: 0.056\n",
      "[Epoch 181/200] TRAIN acc/loss: 1.000/0.010\n",
      "[Epoch 181/200] VAL   acc/loss: 0.814/0.764\n",
      "[Iteration 20282/22400] TRAIN loss: 0.017\n",
      "[Iteration 20292/22400] TRAIN loss: 0.096\n",
      "[Iteration 20302/22400] TRAIN loss: 0.059\n",
      "[Iteration 20312/22400] TRAIN loss: 0.103\n",
      "[Iteration 20322/22400] TRAIN loss: 0.049\n",
      "[Iteration 20332/22400] TRAIN loss: 0.152\n",
      "[Iteration 20342/22400] TRAIN loss: 0.105\n",
      "[Iteration 20352/22400] TRAIN loss: 0.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 20362/22400] TRAIN loss: 0.059\n",
      "[Iteration 20372/22400] TRAIN loss: 0.070\n",
      "[Iteration 20382/22400] TRAIN loss: 0.108\n",
      "[Epoch 182/200] TRAIN acc/loss: 0.900/0.125\n",
      "[Epoch 182/200] VAL   acc/loss: 0.793/0.922\n",
      "[Iteration 20394/22400] TRAIN loss: 0.062\n",
      "[Iteration 20404/22400] TRAIN loss: 0.104\n",
      "[Iteration 20414/22400] TRAIN loss: 0.022\n",
      "[Iteration 20424/22400] TRAIN loss: 0.121\n",
      "[Iteration 20434/22400] TRAIN loss: 0.076\n",
      "[Iteration 20444/22400] TRAIN loss: 0.067\n",
      "[Iteration 20454/22400] TRAIN loss: 0.055\n",
      "[Iteration 20464/22400] TRAIN loss: 0.069\n",
      "[Iteration 20474/22400] TRAIN loss: 0.135\n",
      "[Iteration 20484/22400] TRAIN loss: 0.066\n",
      "[Iteration 20494/22400] TRAIN loss: 0.108\n",
      "[Epoch 183/200] TRAIN acc/loss: 0.900/0.162\n",
      "[Epoch 183/200] VAL   acc/loss: 0.829/0.850\n",
      "[Iteration 20506/22400] TRAIN loss: 0.037\n",
      "[Iteration 20516/22400] TRAIN loss: 0.036\n",
      "[Iteration 20526/22400] TRAIN loss: 0.089\n",
      "[Iteration 20536/22400] TRAIN loss: 0.073\n",
      "[Iteration 20546/22400] TRAIN loss: 0.043\n",
      "[Iteration 20556/22400] TRAIN loss: 0.016\n",
      "[Iteration 20566/22400] TRAIN loss: 0.138\n",
      "[Iteration 20576/22400] TRAIN loss: 0.033\n",
      "[Iteration 20586/22400] TRAIN loss: 0.107\n",
      "[Iteration 20596/22400] TRAIN loss: 0.092\n",
      "[Iteration 20606/22400] TRAIN loss: 0.060\n",
      "[Epoch 184/200] TRAIN acc/loss: 1.000/0.013\n",
      "[Epoch 184/200] VAL   acc/loss: 0.800/0.845\n",
      "[Iteration 20618/22400] TRAIN loss: 0.032\n",
      "[Iteration 20628/22400] TRAIN loss: 0.072\n",
      "[Iteration 20638/22400] TRAIN loss: 0.053\n",
      "[Iteration 20648/22400] TRAIN loss: 0.035\n",
      "[Iteration 20658/22400] TRAIN loss: 0.101\n",
      "[Iteration 20668/22400] TRAIN loss: 0.058\n",
      "[Iteration 20678/22400] TRAIN loss: 0.052\n",
      "[Iteration 20688/22400] TRAIN loss: 0.064\n",
      "[Iteration 20698/22400] TRAIN loss: 0.119\n",
      "[Iteration 20708/22400] TRAIN loss: 0.137\n",
      "[Iteration 20718/22400] TRAIN loss: 0.092\n",
      "[Epoch 185/200] TRAIN acc/loss: 0.900/0.120\n",
      "[Epoch 185/200] VAL   acc/loss: 0.743/0.992\n",
      "[Iteration 20730/22400] TRAIN loss: 0.058\n",
      "[Iteration 20740/22400] TRAIN loss: 0.038\n",
      "[Iteration 20750/22400] TRAIN loss: 0.051\n",
      "[Iteration 20760/22400] TRAIN loss: 0.053\n",
      "[Iteration 20770/22400] TRAIN loss: 0.078\n",
      "[Iteration 20780/22400] TRAIN loss: 0.087\n",
      "[Iteration 20790/22400] TRAIN loss: 0.136\n",
      "[Iteration 20800/22400] TRAIN loss: 0.105\n",
      "[Iteration 20810/22400] TRAIN loss: 0.036\n",
      "[Iteration 20820/22400] TRAIN loss: 0.096\n",
      "[Iteration 20830/22400] TRAIN loss: 0.133\n",
      "[Epoch 186/200] TRAIN acc/loss: 1.000/0.041\n",
      "[Epoch 186/200] VAL   acc/loss: 0.764/0.928\n",
      "[Iteration 20842/22400] TRAIN loss: 0.046\n",
      "[Iteration 20852/22400] TRAIN loss: 0.070\n",
      "[Iteration 20862/22400] TRAIN loss: 0.037\n",
      "[Iteration 20872/22400] TRAIN loss: 0.025\n",
      "[Iteration 20882/22400] TRAIN loss: 0.050\n",
      "[Iteration 20892/22400] TRAIN loss: 0.045\n",
      "[Iteration 20902/22400] TRAIN loss: 0.009\n",
      "[Iteration 20912/22400] TRAIN loss: 0.083\n",
      "[Iteration 20922/22400] TRAIN loss: 0.118\n",
      "[Iteration 20932/22400] TRAIN loss: 0.106\n",
      "[Iteration 20942/22400] TRAIN loss: 0.074\n",
      "[Epoch 187/200] TRAIN acc/loss: 1.000/0.065\n",
      "[Epoch 187/200] VAL   acc/loss: 0.800/0.857\n",
      "[Iteration 20954/22400] TRAIN loss: 0.029\n",
      "[Iteration 20964/22400] TRAIN loss: 0.029\n",
      "[Iteration 20974/22400] TRAIN loss: 0.068\n",
      "[Iteration 20984/22400] TRAIN loss: 0.039\n",
      "[Iteration 20994/22400] TRAIN loss: 0.060\n",
      "[Iteration 21004/22400] TRAIN loss: 0.048\n",
      "[Iteration 21014/22400] TRAIN loss: 0.075\n",
      "[Iteration 21024/22400] TRAIN loss: 0.287\n",
      "[Iteration 21034/22400] TRAIN loss: 0.091\n",
      "[Iteration 21044/22400] TRAIN loss: 0.085\n",
      "[Iteration 21054/22400] TRAIN loss: 0.068\n",
      "[Epoch 188/200] TRAIN acc/loss: 1.000/0.024\n",
      "[Epoch 188/200] VAL   acc/loss: 0.750/1.141\n",
      "[Iteration 21066/22400] TRAIN loss: 0.052\n",
      "[Iteration 21076/22400] TRAIN loss: 0.068\n",
      "[Iteration 21086/22400] TRAIN loss: 0.042\n",
      "[Iteration 21096/22400] TRAIN loss: 0.022\n",
      "[Iteration 21106/22400] TRAIN loss: 0.028\n",
      "[Iteration 21116/22400] TRAIN loss: 0.057\n",
      "[Iteration 21126/22400] TRAIN loss: 0.037\n",
      "[Iteration 21136/22400] TRAIN loss: 0.047\n",
      "[Iteration 21146/22400] TRAIN loss: 0.028\n",
      "[Iteration 21156/22400] TRAIN loss: 0.043\n",
      "[Iteration 21166/22400] TRAIN loss: 0.098\n",
      "[Epoch 189/200] TRAIN acc/loss: 0.900/0.156\n",
      "[Epoch 189/200] VAL   acc/loss: 0.793/0.979\n",
      "[Iteration 21178/22400] TRAIN loss: 0.072\n",
      "[Iteration 21188/22400] TRAIN loss: 0.104\n",
      "[Iteration 21198/22400] TRAIN loss: 0.064\n",
      "[Iteration 21208/22400] TRAIN loss: 0.027\n",
      "[Iteration 21218/22400] TRAIN loss: 0.074\n",
      "[Iteration 21228/22400] TRAIN loss: 0.035\n",
      "[Iteration 21238/22400] TRAIN loss: 0.032\n",
      "[Iteration 21248/22400] TRAIN loss: 0.042\n",
      "[Iteration 21258/22400] TRAIN loss: 0.019\n",
      "[Iteration 21268/22400] TRAIN loss: 0.019\n",
      "[Iteration 21278/22400] TRAIN loss: 0.097\n",
      "[Epoch 190/200] TRAIN acc/loss: 1.000/0.013\n",
      "[Epoch 190/200] VAL   acc/loss: 0.771/0.938\n",
      "[Iteration 21290/22400] TRAIN loss: 0.035\n",
      "[Iteration 21300/22400] TRAIN loss: 0.079\n",
      "[Iteration 21310/22400] TRAIN loss: 0.016\n",
      "[Iteration 21320/22400] TRAIN loss: 0.066\n",
      "[Iteration 21330/22400] TRAIN loss: 0.070\n",
      "[Iteration 21340/22400] TRAIN loss: 0.124\n",
      "[Iteration 21350/22400] TRAIN loss: 0.113\n",
      "[Iteration 21360/22400] TRAIN loss: 0.082\n",
      "[Iteration 21370/22400] TRAIN loss: 0.060\n",
      "[Iteration 21380/22400] TRAIN loss: 0.135\n",
      "[Iteration 21390/22400] TRAIN loss: 0.041\n",
      "[Epoch 191/200] TRAIN acc/loss: 1.000/0.013\n",
      "[Epoch 191/200] VAL   acc/loss: 0.793/1.010\n",
      "[Iteration 21402/22400] TRAIN loss: 0.172\n",
      "[Iteration 21412/22400] TRAIN loss: 0.083\n",
      "[Iteration 21422/22400] TRAIN loss: 0.058\n",
      "[Iteration 21432/22400] TRAIN loss: 0.168\n",
      "[Iteration 21442/22400] TRAIN loss: 0.112\n",
      "[Iteration 21452/22400] TRAIN loss: 0.060\n",
      "[Iteration 21462/22400] TRAIN loss: 0.088\n",
      "[Iteration 21472/22400] TRAIN loss: 0.175\n",
      "[Iteration 21482/22400] TRAIN loss: 0.167\n",
      "[Iteration 21492/22400] TRAIN loss: 0.063\n",
      "[Iteration 21502/22400] TRAIN loss: 0.087\n",
      "[Epoch 192/200] TRAIN acc/loss: 1.000/0.079\n",
      "[Epoch 192/200] VAL   acc/loss: 0.793/0.870\n",
      "[Iteration 21514/22400] TRAIN loss: 0.169\n",
      "[Iteration 21524/22400] TRAIN loss: 0.113\n",
      "[Iteration 21534/22400] TRAIN loss: 0.146\n",
      "[Iteration 21544/22400] TRAIN loss: 0.076\n",
      "[Iteration 21554/22400] TRAIN loss: 0.087\n",
      "[Iteration 21564/22400] TRAIN loss: 0.092\n",
      "[Iteration 21574/22400] TRAIN loss: 0.069\n",
      "[Iteration 21584/22400] TRAIN loss: 0.035\n",
      "[Iteration 21594/22400] TRAIN loss: 0.028\n",
      "[Iteration 21604/22400] TRAIN loss: 0.177\n",
      "[Iteration 21614/22400] TRAIN loss: 0.089\n",
      "[Epoch 193/200] TRAIN acc/loss: 0.900/0.372\n",
      "[Epoch 193/200] VAL   acc/loss: 0.757/1.091\n",
      "[Iteration 21626/22400] TRAIN loss: 0.085\n",
      "[Iteration 21636/22400] TRAIN loss: 0.049\n",
      "[Iteration 21646/22400] TRAIN loss: 0.037\n",
      "[Iteration 21656/22400] TRAIN loss: 0.093\n",
      "[Iteration 21666/22400] TRAIN loss: 0.015\n",
      "[Iteration 21676/22400] TRAIN loss: 0.040\n",
      "[Iteration 21686/22400] TRAIN loss: 0.033\n",
      "[Iteration 21696/22400] TRAIN loss: 0.071\n",
      "[Iteration 21706/22400] TRAIN loss: 0.042\n",
      "[Iteration 21716/22400] TRAIN loss: 0.032\n",
      "[Iteration 21726/22400] TRAIN loss: 0.060\n",
      "[Epoch 194/200] TRAIN acc/loss: 0.900/0.204\n",
      "[Epoch 194/200] VAL   acc/loss: 0.750/0.920\n",
      "[Iteration 21738/22400] TRAIN loss: 0.076\n",
      "[Iteration 21748/22400] TRAIN loss: 0.038\n",
      "[Iteration 21758/22400] TRAIN loss: 0.064\n",
      "[Iteration 21768/22400] TRAIN loss: 0.027\n",
      "[Iteration 21778/22400] TRAIN loss: 0.085\n",
      "[Iteration 21788/22400] TRAIN loss: 0.086\n",
      "[Iteration 21798/22400] TRAIN loss: 0.083\n",
      "[Iteration 21808/22400] TRAIN loss: 0.027\n",
      "[Iteration 21818/22400] TRAIN loss: 0.017\n",
      "[Iteration 21828/22400] TRAIN loss: 0.056\n",
      "[Iteration 21838/22400] TRAIN loss: 0.047\n",
      "[Epoch 195/200] TRAIN acc/loss: 1.000/0.032\n",
      "[Epoch 195/200] VAL   acc/loss: 0.764/1.015\n",
      "[Iteration 21850/22400] TRAIN loss: 0.075\n",
      "[Iteration 21860/22400] TRAIN loss: 0.024\n",
      "[Iteration 21870/22400] TRAIN loss: 0.086\n",
      "[Iteration 21880/22400] TRAIN loss: 0.079\n",
      "[Iteration 21890/22400] TRAIN loss: 0.068\n",
      "[Iteration 21900/22400] TRAIN loss: 0.092\n",
      "[Iteration 21910/22400] TRAIN loss: 0.020\n",
      "[Iteration 21920/22400] TRAIN loss: 0.083\n",
      "[Iteration 21930/22400] TRAIN loss: 0.146\n",
      "[Iteration 21940/22400] TRAIN loss: 0.076\n",
      "[Iteration 21950/22400] TRAIN loss: 0.165\n",
      "[Epoch 196/200] TRAIN acc/loss: 1.000/0.008\n",
      "[Epoch 196/200] VAL   acc/loss: 0.779/0.896\n",
      "[Iteration 21962/22400] TRAIN loss: 0.036\n",
      "[Iteration 21972/22400] TRAIN loss: 0.060\n",
      "[Iteration 21982/22400] TRAIN loss: 0.025\n",
      "[Iteration 21992/22400] TRAIN loss: 0.028\n",
      "[Iteration 22002/22400] TRAIN loss: 0.028\n",
      "[Iteration 22012/22400] TRAIN loss: 0.086\n",
      "[Iteration 22022/22400] TRAIN loss: 0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 22032/22400] TRAIN loss: 0.087\n",
      "[Iteration 22042/22400] TRAIN loss: 0.048\n",
      "[Iteration 22052/22400] TRAIN loss: 0.107\n",
      "[Iteration 22062/22400] TRAIN loss: 0.058\n",
      "[Epoch 197/200] TRAIN acc/loss: 1.000/0.002\n",
      "[Epoch 197/200] VAL   acc/loss: 0.800/0.843\n",
      "[Iteration 22074/22400] TRAIN loss: 0.069\n",
      "[Iteration 22084/22400] TRAIN loss: 0.069\n",
      "[Iteration 22094/22400] TRAIN loss: 0.033\n",
      "[Iteration 22104/22400] TRAIN loss: 0.037\n",
      "[Iteration 22114/22400] TRAIN loss: 0.087\n",
      "[Iteration 22124/22400] TRAIN loss: 0.028\n",
      "[Iteration 22134/22400] TRAIN loss: 0.070\n",
      "[Iteration 22144/22400] TRAIN loss: 0.057\n",
      "[Iteration 22154/22400] TRAIN loss: 0.043\n",
      "[Iteration 22164/22400] TRAIN loss: 0.031\n",
      "[Iteration 22174/22400] TRAIN loss: 0.027\n",
      "[Epoch 198/200] TRAIN acc/loss: 1.000/0.006\n",
      "[Epoch 198/200] VAL   acc/loss: 0.807/0.928\n",
      "[Iteration 22186/22400] TRAIN loss: 0.052\n",
      "[Iteration 22196/22400] TRAIN loss: 0.036\n",
      "[Iteration 22206/22400] TRAIN loss: 0.082\n",
      "[Iteration 22216/22400] TRAIN loss: 0.038\n",
      "[Iteration 22226/22400] TRAIN loss: 0.066\n",
      "[Iteration 22236/22400] TRAIN loss: 0.041\n",
      "[Iteration 22246/22400] TRAIN loss: 0.052\n",
      "[Iteration 22256/22400] TRAIN loss: 0.014\n",
      "[Iteration 22266/22400] TRAIN loss: 0.122\n",
      "[Iteration 22276/22400] TRAIN loss: 0.078\n",
      "[Iteration 22286/22400] TRAIN loss: 0.084\n",
      "[Epoch 199/200] TRAIN acc/loss: 1.000/0.012\n",
      "[Epoch 199/200] VAL   acc/loss: 0.771/0.931\n",
      "[Iteration 22298/22400] TRAIN loss: 0.040\n",
      "[Iteration 22308/22400] TRAIN loss: 0.034\n",
      "[Iteration 22318/22400] TRAIN loss: 0.063\n",
      "[Iteration 22328/22400] TRAIN loss: 0.045\n",
      "[Iteration 22338/22400] TRAIN loss: 0.106\n",
      "[Iteration 22348/22400] TRAIN loss: 0.083\n",
      "[Iteration 22358/22400] TRAIN loss: 0.030\n",
      "[Iteration 22368/22400] TRAIN loss: 0.106\n",
      "[Iteration 22378/22400] TRAIN loss: 0.051\n",
      "[Iteration 22388/22400] TRAIN loss: 0.058\n",
      "[Iteration 22398/22400] TRAIN loss: 0.045\n",
      "[Epoch 200/200] TRAIN acc/loss: 1.000/0.020\n",
      "[Epoch 200/200] VAL   acc/loss: 0.757/0.994\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "num_epochs = 200\n",
    "iter_per_epoch = len(train_loader)\n",
    "#224 lines -> 112 val output for log_nth=10000\n",
    "#1120000 iter\n",
    "log_nth = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):\n",
    "        #inputs, targets = Variable(inputs.float()), Variable(targets.float())\n",
    "        #for CrossEntropyLoss\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #loss = criterion(outputs, targets)\n",
    "        #for CrossEntropyLoss\n",
    "        loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss_history.append(loss.data.cpu().numpy())\n",
    "        if log_nth and i % log_nth == 0:\n",
    "            last_log_nth_losses = train_loss_history[-log_nth:]\n",
    "            train_loss = np.mean(last_log_nth_losses)\n",
    "            print('[Iteration %d/%d] TRAIN loss: %.3f' % \\\n",
    "                        (i + epoch * iter_per_epoch,\n",
    "                         iter_per_epoch * num_epochs,\n",
    "                         train_loss))\n",
    "            \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "\n",
    "    train_acc = np.mean((preds == target_indices).data.cpu().numpy())\n",
    "    train_acc_history.append(train_acc)\n",
    "    if log_nth:\n",
    "        print('[Epoch %d/%d] TRAIN acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                            num_epochs,\n",
    "                                                            train_acc,\n",
    "                                                            loss))\n",
    "        '''_, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Only allow images/pixels with label >= 0 e.g. for segmentation\n",
    "        targets_mask = labels >= 0\n",
    "        train_acc = np.mean((preds == targets)[targets_mask].data.cpu().numpy())\n",
    "        train_acc_history.append(train_acc)\n",
    "        if log_nth:\n",
    "            print('[Epoch %d/%d] TRAIN acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                                   num_epochs,\n",
    "                                                                   train_acc,\n",
    "                                                                   train_loss))'''\n",
    "        \n",
    "    # VALIDATION\n",
    "    val_losses = []\n",
    "    val_scores = []\n",
    "    model.eval()\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        \n",
    "        #loss = criterion(outputs, targets)\n",
    "        \n",
    "        #for CrossEntropyLoss\n",
    "        loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "        \n",
    "        val_losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        _, target_indices = torch.max(targets, 1)\n",
    "\n",
    "        scores = np.mean((preds == target_indices).data.cpu().numpy())\n",
    "        val_scores.append(scores)\n",
    "\n",
    "    model.train()\n",
    "    val_acc, val_loss = np.mean(val_scores), np.mean(val_losses)\n",
    "    val_acc_history.append(val_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    if log_nth:\n",
    "        print('[Epoch %d/%d] VAL   acc/loss: %.3f/%.3f' % (epoch + 1,\n",
    "                                                            num_epochs,\n",
    "                                                            val_acc,\n",
    "                                                            val_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models\n",
    "currentDT = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"saved_models\", \"resnet_rgb_\" + str(num_epochs) + \"_\" + currentDT + \".model\")\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "#Loading\n",
    "#model = VGG('VGG16')\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = ResNet152()\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "\n",
    "#path = \"C:/Users/Bilal/git/gesture_recog_leap/saved_models/resnet_rgb_100_2018-07-04_06-59-19.model\"\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-483328a1b67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\gesture_recog_leap\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\gesture_recog_leap\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     47\u001b[0m         return F.batch_norm(\n\u001b[0;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m     )\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    scores.extend((preds == target_indices).data.cpu().numpy())\n",
    "    \n",
    "print('Test set accuracy: %f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFIiIyO3/AYqOTcLk0FpeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db97991b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERS1CiIiEjK/wPnd1WfXyjqZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1db979d9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history, '-')\n",
    "#plt.plot(val_loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_history, '-o')\n",
    "plt.plot(val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-33409bd0339c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\gesture_recog_leap\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    scores.extend((preds == target_indices).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9268e7648ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\gesture_recog_leap\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 301\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524543037166\\work\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "for inputs, target in test_loader:\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, target_indices = torch.max(targets, 1)\n",
    "    \n",
    "    numpy_inputs = inputs.data.cpu().numpy()\n",
    "    numpy_outputs = outputs.data.cpu().numpy()\n",
    "    numpy_targets = targets.data.cpu().numpy()\n",
    "    \n",
    "    img = numpy_inputs[0]\n",
    "    img = img[0, :, :]\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Predicted: \"+str(preds[0].item()) + \" Target: \"+str(target_indices[0].item()))\n",
    "    plt.show()\n",
    "    #currentDT = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    #path = \"images/img_\" + currentDT + \".png\"\n",
    "    #plt.imsave(path, img.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
